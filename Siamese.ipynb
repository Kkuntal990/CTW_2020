{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Siamese",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fesmFnEKGskw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "33f5e542-d731-4f80-b573-2c96c3a41a93"
      },
      "source": [
        "import h5py\n",
        "f = h5py.File(\"/content/drive/My Drive/CTW2020/Processed Data/udata2020.hdf5\",\"r\")\n",
        "import numpy as np\n",
        "X = np.zeros((31848,56,924),dtype=np.dtype(\"<f2\"))\n",
        "for i in range(31848):\n",
        "  temp = f[\"H_Est\"][i:i+1]\n",
        "  if(i%2000)==0:\n",
        "    print(i)\n",
        "  X[i] = temp[:,:,:,0]**2 + temp[:,:,:,1]**2\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "2000\n",
            "4000\n",
            "6000\n",
            "8000\n",
            "10000\n",
            "12000\n",
            "14000\n",
            "16000\n",
            "18000\n",
            "20000\n",
            "22000\n",
            "24000\n",
            "26000\n",
            "28000\n",
            "30000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hos9jEq9cW6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "15ae63f9-e79a-4e8b-8467-e008922cf56c"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31848, 56, 924)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYGpSuWAPoil",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import h5py\n",
        "f = h5py.File(\"/content/drive/My Drive/CTW2020/Processed Data/Labelled_1.hdf5\",\"r\")\n",
        "H = f[\"H_Est\"][:]\n",
        "Pos = f[\"Pos\"][:]\n",
        "SNR = f[\"SNR\"][:]\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOk-lnxv9MRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "H = np.sqrt(H[:,:,:,0]**2 + H[:,:,:,1]**2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0U4rJbdh9nKb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "H_Train, H_Test , Pos_Train , Pos_Test = train_test_split(H,Pos,test_size=0.05, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmZKU7tvakmQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import os\n",
        "from torchvision.utils import save_image\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCu8RSztap10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "H_Train, H_Val , Pos_Train , Pos_Val = train_test_split(H_Train,Pos_Train,test_size=0.1, random_state=99)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6el12NxbqAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "H_Train = torch.tensor(H_Train,dtype=torch.float)\n",
        "Pos_Train = torch.tensor(Pos_Train,dtype=torch.float)\n",
        "H_Val = torch.tensor(H_Val,dtype=torch.float)\n",
        "Pos_Val = torch.tensor(Pos_Val,dtype=torch.float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12yL8P_Bmzrc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3ue3vK55J8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sammon(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Sammon, self).__init__()\n",
        "        \n",
        "        \n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(1, 3, (4,2), stride=(4,1)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(3, 6, (4,2), stride=(4,1)),\n",
        "            nn.ReLU())\n",
        "        \n",
        "        self.Project = nn.Sequential(nn.Linear(16596,7000),\n",
        "                                     nn.Tanh(),\n",
        "                                     nn.Linear(7000,5000),\n",
        "                                     nn.Softplus(),\n",
        "                                     nn.Linear(5000,4500),\n",
        "                                     nn.Tanh(),)\n",
        "                                     \n",
        "          \n",
        "            \n",
        "    def forward(self, input):\n",
        "        return self.Project(self.main(input).view(-1,16596))\n",
        "\n",
        "\n",
        "class Regressor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Regressor, self).__init__()        \n",
        "        self.Project = nn.Sequential(nn.Linear(4500,2048),\n",
        "                                     nn.Tanh(),\n",
        "                                     nn.Linear(2048,1024),\n",
        "                                     nn.Softplus(),\n",
        "                                     nn.Linear(1024,512),\n",
        "                                     nn.Tanh(),\n",
        "                                     nn.Linear(512,256),\n",
        "                                     nn.Tanh(),\n",
        "                                     nn.Linear(256,128),\n",
        "                                     nn.Tanh(),\n",
        "                                     nn.Linear(128,64),\n",
        "                                     nn.Tanh(),\n",
        "                                     nn.Linear(64,32),\n",
        "                                     nn.Softplus(),\n",
        "                                     nn.Linear(32,16),\n",
        "                                     nn.Tanh(),\n",
        "                                     nn.Linear(16,8),\n",
        "                                     nn.Tanh(),\n",
        "                                     nn.Linear(8,3))\n",
        "          \n",
        "            \n",
        "    def forward(self, input):\n",
        "        return self.Project(input).view(-1,3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7yfKTDJA-O7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
        "sammon = Sammon().to(device)\n",
        "reg = Regressor().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkGd-RUyBxdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt1 = optim.Adam(sammon.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
        "opt2 = optim.Adam(reg.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
        "MSE = loss = nn.MSELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpK2iU41cZEL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "A = list(range(len(X)))*4\n",
        "pairs = []\n",
        "import random\n",
        "while len(A)>1:\n",
        "  t = random.randint(1,len(A)-1)\n",
        "  if(A[t]!=A[0]):\n",
        "    pairs.append((A[0],A[t]))\n",
        "    del A[0]\n",
        "    del A[t-1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnxcS8sie2Ix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pairs = np.array(pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JNeMIPNcPVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_b2 = len(H_Train)\n",
        "N_b = len(pairs)\n",
        "N_val = len(H_Val)\n",
        "bs1= 64\n",
        "bs2 = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UKR9fGq5HOW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bdd652e4-1fe7-4761-e601-78f676570b08"
      },
      "source": [
        "for i in range(100):\n",
        "  batch1=0\n",
        "  batch2=0\n",
        "  for j in range(0,N_b,bs1):\n",
        "    sammon.zero_grad()\n",
        "    end = min(j+bs1,N_b)\n",
        "    Xn,Xm = torch.tensor(X[pairs[j:end,0]],dtype=torch.float),  torch.tensor(X[pairs[j:end,1]],dtype=torch.float)\n",
        "    Xn , Xm = Variable(Xn, requires_grad=True) , Variable(Xm, requires_grad=True)\n",
        "    Xn = Xn.to(device)\n",
        "    Xm = Xm.to(device)\n",
        "    Yn = sammon(Xn.view(-1,1,56,924)).view(-1)\n",
        "    Ym = sammon(Xm.view(-1,1,56,924)).view(-1)\n",
        "    loss1 = MSE(Xn, Xm)\n",
        "    loss2 = MSE(Yn,Ym)\n",
        "    loss = MSE(loss1,loss2)\n",
        "    loss.backward()\n",
        "    opt1.step()\n",
        "    batch1+=1\n",
        "    if(batch1%10 == 0):\n",
        "      print(\"Batch\",batch1,\"/\",(N_b//bs1)+1,\"Sammon Loss :\" , loss.item())\n",
        "  \n",
        "  for k in range(0,N_b2,bs2):\n",
        "    end = min(k+bs2,N_b2)\n",
        "    H1 = H_Train[k:end]\n",
        "    sammon.zero_grad()\n",
        "    reg.zero_grad()\n",
        "    H1 = Variable(H1,requires_grad=True)\n",
        "    H1 = H1.to(device)\n",
        "    Y1 = reg(sammon(H1.view(-1,1,56,924)))\n",
        "    loss_1 = MSE(Y1,Pos_Train[k:end].to(device)).view(-1)\n",
        "    loss_1.backward()\n",
        "    opt1.step()\n",
        "    opt2.step()\n",
        "    batch2+=1\n",
        "    if(batch2%10 == 0):\n",
        "      print(\"Batch\",batch2,\"/\",(N_b2//bs2)+1,\"Regressor Loss :\" , loss_1.item())\n",
        "  print(\"Epoch \",i,\"/\",100,\" Sammon Loss :\",loss.item() ,\" Regressor Loss :\" , loss_1.item())\n",
        "  for l in range(N_val):\n",
        "    H_val = torch.tensor(H_Val[l],dtype=torch.float)\n",
        "    H_val = H_val.to(device)\n",
        "    with torch.no_grad():\n",
        "      Y_Val = reg(sammon(H_val.view(-1,1,56,924)))\n",
        "      loss_Val = MSE(Y_Val,Pos_Val[l].to(device)).view(-1)\n",
        "  print(\"Val Loss :\" , loss_Val.item())\n",
        "\n",
        "    \n",
        "\n",
        "      \n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch 10 / 996 Sammon Loss : 0.0083991177380085\n",
            "Batch 20 / 996 Sammon Loss : 0.0010822005569934845\n",
            "Batch 30 / 996 Sammon Loss : 0.006653514225035906\n",
            "Batch 40 / 996 Sammon Loss : 0.032325953245162964\n",
            "Batch 50 / 996 Sammon Loss : 0.021435728296637535\n",
            "Batch 60 / 996 Sammon Loss : 0.02428772859275341\n",
            "Batch 70 / 996 Sammon Loss : 0.010107041336596012\n",
            "Batch 80 / 996 Sammon Loss : 0.016238121315836906\n",
            "Batch 90 / 996 Sammon Loss : 0.001901067211292684\n",
            "Batch 100 / 996 Sammon Loss : 0.012429174967110157\n",
            "Batch 110 / 996 Sammon Loss : 0.00011216865823371336\n",
            "Batch 120 / 996 Sammon Loss : 0.009747683070600033\n",
            "Batch 130 / 996 Sammon Loss : 0.0007925923564471304\n",
            "Batch 140 / 996 Sammon Loss : 0.004083746112883091\n",
            "Batch 150 / 996 Sammon Loss : 0.00011599066783674061\n",
            "Batch 160 / 996 Sammon Loss : 0.0017142882570624352\n",
            "Batch 170 / 996 Sammon Loss : 0.005111721809953451\n",
            "Batch 180 / 996 Sammon Loss : 0.028658824041485786\n",
            "Batch 190 / 996 Sammon Loss : 0.03912125900387764\n",
            "Batch 200 / 996 Sammon Loss : 0.002301952801644802\n",
            "Batch 210 / 996 Sammon Loss : 0.0045103468000888824\n",
            "Batch 220 / 996 Sammon Loss : 0.00252935872413218\n",
            "Batch 230 / 996 Sammon Loss : 0.015376509167253971\n",
            "Batch 240 / 996 Sammon Loss : 0.052355412393808365\n",
            "Batch 250 / 996 Sammon Loss : 0.008869783021509647\n",
            "Batch 260 / 996 Sammon Loss : 0.0002890974283218384\n",
            "Batch 270 / 996 Sammon Loss : 0.0022632356267422438\n",
            "Batch 280 / 996 Sammon Loss : 0.0009263170650228858\n",
            "Batch 290 / 996 Sammon Loss : 0.014366982504725456\n",
            "Batch 300 / 996 Sammon Loss : 0.0014078436652198434\n",
            "Batch 310 / 996 Sammon Loss : 0.012482290156185627\n",
            "Batch 320 / 996 Sammon Loss : 7.150028977775946e-06\n",
            "Batch 330 / 996 Sammon Loss : 0.00020326711819507182\n",
            "Batch 340 / 996 Sammon Loss : 0.006987395230680704\n",
            "Batch 350 / 996 Sammon Loss : 0.00025524484226480126\n",
            "Batch 360 / 996 Sammon Loss : 3.2375686714658514e-05\n",
            "Batch 370 / 996 Sammon Loss : 0.0024115692358464003\n",
            "Batch 380 / 996 Sammon Loss : 1.8290356820216402e-05\n",
            "Batch 390 / 996 Sammon Loss : 0.0011601019650697708\n",
            "Batch 400 / 996 Sammon Loss : 0.027977673336863518\n",
            "Batch 410 / 996 Sammon Loss : 0.09248968213796616\n",
            "Batch 420 / 996 Sammon Loss : 3.747478695004247e-05\n",
            "Batch 430 / 996 Sammon Loss : 0.030018532648682594\n",
            "Batch 440 / 996 Sammon Loss : 0.04118047654628754\n",
            "Batch 450 / 996 Sammon Loss : 0.004166923463344574\n",
            "Batch 460 / 996 Sammon Loss : 0.004102710168808699\n",
            "Batch 470 / 996 Sammon Loss : 0.012861043214797974\n",
            "Batch 480 / 996 Sammon Loss : 0.028620906174182892\n",
            "Batch 490 / 996 Sammon Loss : 0.0006163956131786108\n",
            "Batch 500 / 996 Sammon Loss : 0.00985399167984724\n",
            "Batch 510 / 996 Sammon Loss : 0.002220376394689083\n",
            "Batch 520 / 996 Sammon Loss : 0.022647423669695854\n",
            "Batch 530 / 996 Sammon Loss : 0.0017251059180125594\n",
            "Batch 540 / 996 Sammon Loss : 0.0013007732341066003\n",
            "Batch 550 / 996 Sammon Loss : 0.0013133492320775986\n",
            "Batch 560 / 996 Sammon Loss : 0.02744460664689541\n",
            "Batch 570 / 996 Sammon Loss : 1.765286583577108e-07\n",
            "Batch 580 / 996 Sammon Loss : 0.03152281418442726\n",
            "Batch 590 / 996 Sammon Loss : 0.0009764432907104492\n",
            "Batch 600 / 996 Sammon Loss : 2.539519118727185e-05\n",
            "Batch 610 / 996 Sammon Loss : 0.0015089382650330663\n",
            "Batch 620 / 996 Sammon Loss : 0.00178051448892802\n",
            "Batch 630 / 996 Sammon Loss : 0.011811014264822006\n",
            "Batch 640 / 996 Sammon Loss : 0.0036153336986899376\n",
            "Batch 650 / 996 Sammon Loss : 0.0004486102261580527\n",
            "Batch 660 / 996 Sammon Loss : 0.0031954562291502953\n",
            "Batch 670 / 996 Sammon Loss : 0.003840841818600893\n",
            "Batch 680 / 996 Sammon Loss : 0.0006913340766914189\n",
            "Batch 690 / 996 Sammon Loss : 0.0008684280328452587\n",
            "Batch 700 / 996 Sammon Loss : 0.20208285748958588\n",
            "Batch 710 / 996 Sammon Loss : 0.0002835809427779168\n",
            "Batch 720 / 996 Sammon Loss : 0.00599319115281105\n",
            "Batch 730 / 996 Sammon Loss : 0.017811043187975883\n",
            "Batch 740 / 996 Sammon Loss : 2.4257572022179374e-06\n",
            "Batch 750 / 996 Sammon Loss : 0.00940250325948\n",
            "Batch 760 / 996 Sammon Loss : 0.18483573198318481\n",
            "Batch 770 / 996 Sammon Loss : 0.06198229268193245\n",
            "Batch 780 / 996 Sammon Loss : 0.008292191661894321\n",
            "Batch 790 / 996 Sammon Loss : 0.002803630894050002\n",
            "Batch 800 / 996 Sammon Loss : 0.0009498465224169195\n",
            "Batch 810 / 996 Sammon Loss : 0.0018441396532580256\n",
            "Batch 820 / 996 Sammon Loss : 0.0005267938831821084\n",
            "Batch 830 / 996 Sammon Loss : 0.004534649662673473\n",
            "Batch 840 / 996 Sammon Loss : 0.0014017202192917466\n",
            "Batch 850 / 996 Sammon Loss : 0.014236808754503727\n",
            "Batch 860 / 996 Sammon Loss : 0.0018985854694619775\n",
            "Batch 870 / 996 Sammon Loss : 0.0031357856933027506\n",
            "Batch 880 / 996 Sammon Loss : 1.7722930351737887e-05\n",
            "Batch 890 / 996 Sammon Loss : 0.0009809208568185568\n",
            "Batch 900 / 996 Sammon Loss : 0.001444517751224339\n",
            "Batch 910 / 996 Sammon Loss : 0.16341929137706757\n",
            "Batch 920 / 996 Sammon Loss : 1.7948854292626493e-05\n",
            "Batch 930 / 996 Sammon Loss : 1.4248604202293791e-05\n",
            "Batch 940 / 996 Sammon Loss : 0.00829678401350975\n",
            "Batch 950 / 996 Sammon Loss : 0.022452641278505325\n",
            "Batch 960 / 996 Sammon Loss : 0.03635638207197189\n",
            "Batch 970 / 996 Sammon Loss : 0.003101418260484934\n",
            "Batch 980 / 996 Sammon Loss : 0.007126044016331434\n",
            "Batch 990 / 996 Sammon Loss : 0.00038505642442032695\n",
            "Batch 10 / 533 Regressor Loss : 147791.71875\n",
            "Batch 20 / 533 Regressor Loss : 120575.2734375\n",
            "Batch 30 / 533 Regressor Loss : 142966.9375\n",
            "Batch 40 / 533 Regressor Loss : 150359.046875\n",
            "Batch 50 / 533 Regressor Loss : 137751.359375\n",
            "Batch 60 / 533 Regressor Loss : 129350.4609375\n",
            "Batch 70 / 533 Regressor Loss : 135768.984375\n",
            "Batch 80 / 533 Regressor Loss : 123797.2109375\n",
            "Batch 90 / 533 Regressor Loss : 134629.765625\n",
            "Batch 100 / 533 Regressor Loss : 149564.265625\n",
            "Batch 110 / 533 Regressor Loss : 127468.8125\n",
            "Batch 120 / 533 Regressor Loss : 115675.5\n",
            "Batch 130 / 533 Regressor Loss : 145623.75\n",
            "Batch 140 / 533 Regressor Loss : 117120.2109375\n",
            "Batch 150 / 533 Regressor Loss : 117370.125\n",
            "Batch 160 / 533 Regressor Loss : 141434.171875\n",
            "Batch 170 / 533 Regressor Loss : 148800.734375\n",
            "Batch 180 / 533 Regressor Loss : 130018.9375\n",
            "Batch 190 / 533 Regressor Loss : 151882.09375\n",
            "Batch 200 / 533 Regressor Loss : 146749.96875\n",
            "Batch 210 / 533 Regressor Loss : 116587.0234375\n",
            "Batch 220 / 533 Regressor Loss : 113121.1796875\n",
            "Batch 230 / 533 Regressor Loss : 137743.28125\n",
            "Batch 240 / 533 Regressor Loss : 115934.8671875\n",
            "Batch 250 / 533 Regressor Loss : 150245.59375\n",
            "Batch 260 / 533 Regressor Loss : 131185.859375\n",
            "Batch 270 / 533 Regressor Loss : 115673.40625\n",
            "Batch 280 / 533 Regressor Loss : 140308.953125\n",
            "Batch 290 / 533 Regressor Loss : 145289.40625\n",
            "Batch 300 / 533 Regressor Loss : 147187.4375\n",
            "Batch 310 / 533 Regressor Loss : 143940.140625\n",
            "Batch 320 / 533 Regressor Loss : 143963.84375\n",
            "Batch 330 / 533 Regressor Loss : 181000.234375\n",
            "Batch 340 / 533 Regressor Loss : 114262.7109375\n",
            "Batch 350 / 533 Regressor Loss : 119476.8125\n",
            "Batch 360 / 533 Regressor Loss : 138616.140625\n",
            "Batch 370 / 533 Regressor Loss : 132060.671875\n",
            "Batch 380 / 533 Regressor Loss : 129898.9609375\n",
            "Batch 390 / 533 Regressor Loss : 135950.734375\n",
            "Batch 400 / 533 Regressor Loss : 118648.4921875\n",
            "Batch 410 / 533 Regressor Loss : 119271.109375\n",
            "Batch 420 / 533 Regressor Loss : 159655.3125\n",
            "Batch 430 / 533 Regressor Loss : 113487.7734375\n",
            "Batch 440 / 533 Regressor Loss : 143946.265625\n",
            "Batch 450 / 533 Regressor Loss : 130605.5859375\n",
            "Batch 460 / 533 Regressor Loss : 126181.0234375\n",
            "Batch 470 / 533 Regressor Loss : 135867.34375\n",
            "Batch 480 / 533 Regressor Loss : 137787.40625\n",
            "Batch 490 / 533 Regressor Loss : 132648.046875\n",
            "Batch 500 / 533 Regressor Loss : 141297.34375\n",
            "Batch 510 / 533 Regressor Loss : 125858.25\n",
            "Batch 520 / 533 Regressor Loss : 146520.8125\n",
            "Batch 530 / 533 Regressor Loss : 135122.8125\n",
            "Epoch  0 / 100  Sammon Loss : 0.007702513597905636  Regressor Loss : 181309.484375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([1, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Loss : 108700.53125\n",
            "Batch 10 / 996 Sammon Loss : 0.20362131297588348\n",
            "Batch 20 / 996 Sammon Loss : 0.2075655460357666\n",
            "Batch 30 / 996 Sammon Loss : 0.3207903206348419\n",
            "Batch 40 / 996 Sammon Loss : 0.943100094795227\n",
            "Batch 50 / 996 Sammon Loss : 0.6516808867454529\n",
            "Batch 60 / 996 Sammon Loss : 0.05872718617320061\n",
            "Batch 70 / 996 Sammon Loss : 0.06931544840335846\n",
            "Batch 80 / 996 Sammon Loss : 0.050553422421216965\n",
            "Batch 90 / 996 Sammon Loss : 0.020151996985077858\n",
            "Batch 100 / 996 Sammon Loss : 0.016281787306070328\n",
            "Batch 110 / 996 Sammon Loss : 0.008399019949138165\n",
            "Batch 120 / 996 Sammon Loss : 0.046326540410518646\n",
            "Batch 130 / 996 Sammon Loss : 0.001967438729479909\n",
            "Batch 140 / 996 Sammon Loss : 0.02190415933728218\n",
            "Batch 150 / 996 Sammon Loss : 0.00015732993779238313\n",
            "Batch 160 / 996 Sammon Loss : 0.006906980648636818\n",
            "Batch 170 / 996 Sammon Loss : 0.21873237192630768\n",
            "Batch 180 / 996 Sammon Loss : 0.07931903749704361\n",
            "Batch 190 / 996 Sammon Loss : 0.21199102699756622\n",
            "Batch 200 / 996 Sammon Loss : 0.09687324613332748\n",
            "Batch 210 / 996 Sammon Loss : 0.14622235298156738\n",
            "Batch 220 / 996 Sammon Loss : 0.03796093538403511\n",
            "Batch 230 / 996 Sammon Loss : 0.01608247309923172\n",
            "Batch 240 / 996 Sammon Loss : 0.05731778219342232\n",
            "Batch 250 / 996 Sammon Loss : 0.022573115304112434\n",
            "Batch 260 / 996 Sammon Loss : 0.005025167483836412\n",
            "Batch 270 / 996 Sammon Loss : 0.003929466009140015\n",
            "Batch 280 / 996 Sammon Loss : 0.009139472618699074\n",
            "Batch 290 / 996 Sammon Loss : 0.04858928173780441\n",
            "Batch 300 / 996 Sammon Loss : 0.00023727085499558598\n",
            "Batch 310 / 996 Sammon Loss : 0.01725376397371292\n",
            "Batch 320 / 996 Sammon Loss : 0.0026420264039188623\n",
            "Batch 330 / 996 Sammon Loss : 0.060699138790369034\n",
            "Batch 340 / 996 Sammon Loss : 0.04467003047466278\n",
            "Batch 350 / 996 Sammon Loss : 0.006206333637237549\n",
            "Batch 360 / 996 Sammon Loss : 0.017689760774374008\n",
            "Batch 370 / 996 Sammon Loss : 0.11192523688077927\n",
            "Batch 380 / 996 Sammon Loss : 0.01639375276863575\n",
            "Batch 390 / 996 Sammon Loss : 0.08339320123195648\n",
            "Batch 400 / 996 Sammon Loss : 0.006532761268317699\n",
            "Batch 410 / 996 Sammon Loss : 0.33847665786743164\n",
            "Batch 420 / 996 Sammon Loss : 0.02955694869160652\n",
            "Batch 430 / 996 Sammon Loss : 0.11641518026590347\n",
            "Batch 440 / 996 Sammon Loss : 1.732412338256836\n",
            "Batch 450 / 996 Sammon Loss : 1.366226077079773\n",
            "Batch 460 / 996 Sammon Loss : 0.6464201807975769\n",
            "Batch 470 / 996 Sammon Loss : 0.9891689419746399\n",
            "Batch 480 / 996 Sammon Loss : 0.017491377890110016\n",
            "Batch 490 / 996 Sammon Loss : 0.06301890313625336\n",
            "Batch 500 / 996 Sammon Loss : 0.07894836366176605\n",
            "Batch 510 / 996 Sammon Loss : 0.08429750800132751\n",
            "Batch 520 / 996 Sammon Loss : 0.23455935716629028\n",
            "Batch 530 / 996 Sammon Loss : 0.051024384796619415\n",
            "Batch 540 / 996 Sammon Loss : 0.021320577710866928\n",
            "Batch 550 / 996 Sammon Loss : 0.049902357161045074\n",
            "Batch 560 / 996 Sammon Loss : 0.48174381256103516\n",
            "Batch 570 / 996 Sammon Loss : 0.1954030990600586\n",
            "Batch 580 / 996 Sammon Loss : 0.12789122760295868\n",
            "Batch 590 / 996 Sammon Loss : 0.11116663366556168\n",
            "Batch 600 / 996 Sammon Loss : 0.009754810482263565\n",
            "Batch 610 / 996 Sammon Loss : 0.010687187314033508\n",
            "Batch 620 / 996 Sammon Loss : 0.0025958213955163956\n",
            "Batch 630 / 996 Sammon Loss : 0.047812141478061676\n",
            "Batch 640 / 996 Sammon Loss : 0.03517058864235878\n",
            "Batch 650 / 996 Sammon Loss : 0.03947313129901886\n",
            "Batch 660 / 996 Sammon Loss : 0.01210426539182663\n",
            "Batch 670 / 996 Sammon Loss : 0.00849437527358532\n",
            "Batch 680 / 996 Sammon Loss : 0.03560055047273636\n",
            "Batch 690 / 996 Sammon Loss : 0.12902575731277466\n",
            "Batch 700 / 996 Sammon Loss : 0.458233118057251\n",
            "Batch 710 / 996 Sammon Loss : 0.05700164660811424\n",
            "Batch 720 / 996 Sammon Loss : 0.03543038293719292\n",
            "Batch 730 / 996 Sammon Loss : 0.012521475553512573\n",
            "Batch 740 / 996 Sammon Loss : 0.04349934682250023\n",
            "Batch 750 / 996 Sammon Loss : 0.26289451122283936\n",
            "Batch 760 / 996 Sammon Loss : 0.6047709584236145\n",
            "Batch 770 / 996 Sammon Loss : 0.5221672058105469\n",
            "Batch 780 / 996 Sammon Loss : 0.08769587427377701\n",
            "Batch 790 / 996 Sammon Loss : 0.06553487479686737\n",
            "Batch 800 / 996 Sammon Loss : 0.040273718535900116\n",
            "Batch 810 / 996 Sammon Loss : 0.09504111111164093\n",
            "Batch 820 / 996 Sammon Loss : 0.031192537397146225\n",
            "Batch 830 / 996 Sammon Loss : 0.07903064787387848\n",
            "Batch 840 / 996 Sammon Loss : 0.08584553003311157\n",
            "Batch 850 / 996 Sammon Loss : 0.07204573601484299\n",
            "Batch 860 / 996 Sammon Loss : 0.0018391271587461233\n",
            "Batch 870 / 996 Sammon Loss : 0.05599132925271988\n",
            "Batch 880 / 996 Sammon Loss : 0.01316784042865038\n",
            "Batch 890 / 996 Sammon Loss : 0.01574392430484295\n",
            "Batch 900 / 996 Sammon Loss : 0.01939241960644722\n",
            "Batch 910 / 996 Sammon Loss : 0.5281426310539246\n",
            "Batch 920 / 996 Sammon Loss : 0.08417125791311264\n",
            "Batch 930 / 996 Sammon Loss : 0.11735627055168152\n",
            "Batch 940 / 996 Sammon Loss : 0.4551873803138733\n",
            "Batch 950 / 996 Sammon Loss : 0.030603541061282158\n",
            "Batch 960 / 996 Sammon Loss : 0.011095905676484108\n",
            "Batch 970 / 996 Sammon Loss : 0.16783744096755981\n",
            "Batch 980 / 996 Sammon Loss : 0.051580093801021576\n",
            "Batch 990 / 996 Sammon Loss : 0.009286273270845413\n",
            "Batch 10 / 533 Regressor Loss : 147101.65625\n",
            "Batch 20 / 533 Regressor Loss : 119917.7734375\n",
            "Batch 30 / 533 Regressor Loss : 142306.28125\n",
            "Batch 40 / 533 Regressor Loss : 149746.125\n",
            "Batch 50 / 533 Regressor Loss : 137164.65625\n",
            "Batch 60 / 533 Regressor Loss : 128714.8125\n",
            "Batch 70 / 533 Regressor Loss : 135153.609375\n",
            "Batch 80 / 533 Regressor Loss : 123228.5859375\n",
            "Batch 90 / 533 Regressor Loss : 134059.75\n",
            "Batch 100 / 533 Regressor Loss : 148968.375\n",
            "Batch 110 / 533 Regressor Loss : 126933.328125\n",
            "Batch 120 / 533 Regressor Loss : 115150.0859375\n",
            "Batch 130 / 533 Regressor Loss : 145051.28125\n",
            "Batch 140 / 533 Regressor Loss : 116591.8984375\n",
            "Batch 150 / 533 Regressor Loss : 116844.734375\n",
            "Batch 160 / 533 Regressor Loss : 140918.03125\n",
            "Batch 170 / 533 Regressor Loss : 148268.78125\n",
            "Batch 180 / 533 Regressor Loss : 129499.765625\n",
            "Batch 190 / 533 Regressor Loss : 151369.8125\n",
            "Batch 200 / 533 Regressor Loss : 146239.8125\n",
            "Batch 210 / 533 Regressor Loss : 116104.109375\n",
            "Batch 220 / 533 Regressor Loss : 112647.328125\n",
            "Batch 230 / 533 Regressor Loss : 137273.9375\n",
            "Batch 240 / 533 Regressor Loss : 115463.453125\n",
            "Batch 250 / 533 Regressor Loss : 149736.671875\n",
            "Batch 260 / 533 Regressor Loss : 130710.765625\n",
            "Batch 270 / 533 Regressor Loss : 115216.421875\n",
            "Batch 280 / 533 Regressor Loss : 139833.9375\n",
            "Batch 290 / 533 Regressor Loss : 144809.8125\n",
            "Batch 300 / 533 Regressor Loss : 146749.9375\n",
            "Batch 310 / 533 Regressor Loss : 143485.953125\n",
            "Batch 320 / 533 Regressor Loss : 143508.875\n",
            "Batch 330 / 533 Regressor Loss : 180546.84375\n",
            "Batch 340 / 533 Regressor Loss : 113846.734375\n",
            "Batch 350 / 533 Regressor Loss : 119055.1171875\n",
            "Batch 360 / 533 Regressor Loss : 138187.1875\n",
            "Batch 370 / 533 Regressor Loss : 131651.84375\n",
            "Batch 380 / 533 Regressor Loss : 129492.1875\n",
            "Batch 390 / 533 Regressor Loss : 135538.359375\n",
            "Batch 400 / 533 Regressor Loss : 118256.0234375\n",
            "Batch 410 / 533 Regressor Loss : 118882.8125\n",
            "Batch 420 / 533 Regressor Loss : 159233.796875\n",
            "Batch 430 / 533 Regressor Loss : 113116.0859375\n",
            "Batch 440 / 533 Regressor Loss : 143534.234375\n",
            "Batch 450 / 533 Regressor Loss : 130209.453125\n",
            "Batch 460 / 533 Regressor Loss : 125821.75\n",
            "Batch 470 / 533 Regressor Loss : 135474.609375\n",
            "Batch 480 / 533 Regressor Loss : 137376.5\n",
            "Batch 490 / 533 Regressor Loss : 132263.875\n",
            "Batch 500 / 533 Regressor Loss : 140918.671875\n",
            "Batch 510 / 533 Regressor Loss : 125501.5625\n",
            "Batch 520 / 533 Regressor Loss : 146113.203125\n",
            "Batch 530 / 533 Regressor Loss : 134742.53125\n",
            "Epoch  1 / 100  Sammon Loss : 0.3028241991996765  Regressor Loss : 180858.125\n",
            "Val Loss : 108327.90625\n",
            "Batch 10 / 996 Sammon Loss : 0.3467167019844055\n",
            "Batch 20 / 996 Sammon Loss : 0.3927364945411682\n",
            "Batch 30 / 996 Sammon Loss : 0.5494597554206848\n",
            "Batch 40 / 996 Sammon Loss : 1.2236802577972412\n",
            "Batch 50 / 996 Sammon Loss : 0.9131371378898621\n",
            "Batch 60 / 996 Sammon Loss : 0.07805559784173965\n",
            "Batch 70 / 996 Sammon Loss : 0.12138907611370087\n",
            "Batch 80 / 996 Sammon Loss : 0.1404729187488556\n",
            "Batch 90 / 996 Sammon Loss : 0.04468977078795433\n",
            "Batch 100 / 996 Sammon Loss : 0.039722997695207596\n",
            "Batch 110 / 996 Sammon Loss : 0.01800369843840599\n",
            "Batch 120 / 996 Sammon Loss : 0.10082484781742096\n",
            "Batch 130 / 996 Sammon Loss : 0.003910406492650509\n",
            "Batch 140 / 996 Sammon Loss : 0.05679690092802048\n",
            "Batch 150 / 996 Sammon Loss : 0.00343458098359406\n",
            "Batch 160 / 996 Sammon Loss : 0.01585349254310131\n",
            "Batch 170 / 996 Sammon Loss : 0.4820282459259033\n",
            "Batch 180 / 996 Sammon Loss : 0.25036489963531494\n",
            "Batch 190 / 996 Sammon Loss : 0.411702960729599\n",
            "Batch 200 / 996 Sammon Loss : 0.21055163443088531\n",
            "Batch 210 / 996 Sammon Loss : 0.30570825934410095\n",
            "Batch 220 / 996 Sammon Loss : 0.08888041973114014\n",
            "Batch 230 / 996 Sammon Loss : 0.060632482171058655\n",
            "Batch 240 / 996 Sammon Loss : 0.11098308116197586\n",
            "Batch 250 / 996 Sammon Loss : 0.0776534378528595\n",
            "Batch 260 / 996 Sammon Loss : 0.011658510193228722\n",
            "Batch 270 / 996 Sammon Loss : 0.030159305781126022\n",
            "Batch 280 / 996 Sammon Loss : 0.023270992562174797\n",
            "Batch 290 / 996 Sammon Loss : 0.13487885892391205\n",
            "Batch 300 / 996 Sammon Loss : 0.02591635473072529\n",
            "Batch 310 / 996 Sammon Loss : 0.0551760196685791\n",
            "Batch 320 / 996 Sammon Loss : 0.003997132182121277\n",
            "Batch 330 / 996 Sammon Loss : 0.0591123066842556\n",
            "Batch 340 / 996 Sammon Loss : 0.04396739974617958\n",
            "Batch 350 / 996 Sammon Loss : 0.00597396632656455\n",
            "Batch 360 / 996 Sammon Loss : 0.017221249639987946\n",
            "Batch 370 / 996 Sammon Loss : 0.11032095551490784\n",
            "Batch 380 / 996 Sammon Loss : 0.015880979597568512\n",
            "Batch 390 / 996 Sammon Loss : 0.08192302286624908\n",
            "Batch 400 / 996 Sammon Loss : 0.006238327361643314\n",
            "Batch 410 / 996 Sammon Loss : 0.33624133467674255\n",
            "Batch 420 / 996 Sammon Loss : 0.028938451781868935\n",
            "Batch 430 / 996 Sammon Loss : 0.11560091376304626\n",
            "Batch 440 / 996 Sammon Loss : 1.7261189222335815\n",
            "Batch 450 / 996 Sammon Loss : 1.3588066101074219\n",
            "Batch 460 / 996 Sammon Loss : 0.6421127319335938\n",
            "Batch 470 / 996 Sammon Loss : 0.5148105025291443\n",
            "Batch 480 / 996 Sammon Loss : 0.00028636030037887394\n",
            "Batch 490 / 996 Sammon Loss : 0.0036487497854977846\n",
            "Batch 500 / 996 Sammon Loss : 0.014945833012461662\n",
            "Batch 510 / 996 Sammon Loss : 0.008882957510650158\n",
            "Batch 520 / 996 Sammon Loss : 0.08680044114589691\n",
            "Batch 530 / 996 Sammon Loss : 0.012798684649169445\n",
            "Batch 540 / 996 Sammon Loss : 0.0019984247628599405\n",
            "Batch 550 / 996 Sammon Loss : 0.0030885240994393826\n",
            "Batch 560 / 996 Sammon Loss : 0.24239401519298553\n",
            "Batch 570 / 996 Sammon Loss : 0.04291985183954239\n",
            "Batch 580 / 996 Sammon Loss : 0.024680374190211296\n",
            "Batch 590 / 996 Sammon Loss : 0.017443930730223656\n",
            "Batch 600 / 996 Sammon Loss : 0.00014627777272835374\n",
            "Batch 610 / 996 Sammon Loss : 0.0003544146311469376\n",
            "Batch 620 / 996 Sammon Loss : 0.0024477439001202583\n",
            "Batch 630 / 996 Sammon Loss : 0.011396695859730244\n",
            "Batch 640 / 996 Sammon Loss : 0.004148846957832575\n",
            "Batch 650 / 996 Sammon Loss : 0.0023289502132683992\n",
            "Batch 660 / 996 Sammon Loss : 0.0027971025556325912\n",
            "Batch 670 / 996 Sammon Loss : 5.8525278291199356e-05\n",
            "Batch 680 / 996 Sammon Loss : 0.00601181760430336\n",
            "Batch 690 / 996 Sammon Loss : 0.04060373827815056\n",
            "Batch 700 / 996 Sammon Loss : 0.18377748131752014\n",
            "Batch 710 / 996 Sammon Loss : 0.003082342678681016\n",
            "Batch 720 / 996 Sammon Loss : 0.0023438858333975077\n",
            "Batch 730 / 996 Sammon Loss : 0.009514311328530312\n",
            "Batch 740 / 996 Sammon Loss : 0.0006759105599485338\n",
            "Batch 750 / 996 Sammon Loss : 0.06483780592679977\n",
            "Batch 760 / 996 Sammon Loss : 0.22950479388237\n",
            "Batch 770 / 996 Sammon Loss : 0.27754780650138855\n",
            "Batch 780 / 996 Sammon Loss : 0.028731556609272957\n",
            "Batch 790 / 996 Sammon Loss : 0.0002243759372504428\n",
            "Batch 800 / 996 Sammon Loss : 0.0028421489987522364\n",
            "Batch 810 / 996 Sammon Loss : 0.028858348727226257\n",
            "Batch 820 / 996 Sammon Loss : 0.0014416659250855446\n",
            "Batch 830 / 996 Sammon Loss : 0.0014706745278090239\n",
            "Batch 840 / 996 Sammon Loss : 0.0020656376145780087\n",
            "Batch 850 / 996 Sammon Loss : 0.0036128684878349304\n",
            "Batch 860 / 996 Sammon Loss : 0.0037051639519631863\n",
            "Batch 870 / 996 Sammon Loss : 0.007518984377384186\n",
            "Batch 880 / 996 Sammon Loss : 0.006591364275664091\n",
            "Batch 890 / 996 Sammon Loss : 0.0005510724149644375\n",
            "Batch 900 / 996 Sammon Loss : 0.00015560626343358308\n",
            "Batch 910 / 996 Sammon Loss : 0.2131669670343399\n",
            "Batch 920 / 996 Sammon Loss : 0.011218810454010963\n",
            "Batch 930 / 996 Sammon Loss : 0.004533221013844013\n",
            "Batch 940 / 996 Sammon Loss : 0.16786572337150574\n",
            "Batch 950 / 996 Sammon Loss : 0.0035986164584755898\n",
            "Batch 960 / 996 Sammon Loss : 0.019806088879704475\n",
            "Batch 970 / 996 Sammon Loss : 0.011635084636509418\n",
            "Batch 980 / 996 Sammon Loss : 0.0009157837484963238\n",
            "Batch 990 / 996 Sammon Loss : 0.0029409185517579317\n",
            "Batch 10 / 533 Regressor Loss : 146730.359375\n",
            "Batch 20 / 533 Regressor Loss : 119559.234375\n",
            "Batch 30 / 533 Regressor Loss : 141925.78125\n",
            "Batch 40 / 533 Regressor Loss : 149371.59375\n",
            "Batch 50 / 533 Regressor Loss : 136817.65625\n",
            "Batch 60 / 533 Regressor Loss : 128348.625\n",
            "Batch 70 / 533 Regressor Loss : 134773.03125\n",
            "Batch 80 / 533 Regressor Loss : 122878.9609375\n",
            "Batch 90 / 533 Regressor Loss : 133705.171875\n",
            "Batch 100 / 533 Regressor Loss : 148575.8125\n",
            "Batch 110 / 533 Regressor Loss : 126604.6171875\n",
            "Batch 120 / 533 Regressor Loss : 114834.5234375\n",
            "Batch 130 / 533 Regressor Loss : 144674.484375\n",
            "Batch 140 / 533 Regressor Loss : 116264.75\n",
            "Batch 150 / 533 Regressor Loss : 116510.3046875\n",
            "Batch 160 / 533 Regressor Loss : 140590.65625\n",
            "Batch 170 / 533 Regressor Loss : 147913.5625\n",
            "Batch 180 / 533 Regressor Loss : 129153.5859375\n",
            "Batch 190 / 533 Regressor Loss : 151026.921875\n",
            "Batch 200 / 533 Regressor Loss : 145893.25\n",
            "Batch 210 / 533 Regressor Loss : 115784.59375\n",
            "Batch 220 / 533 Regressor Loss : 112333.2734375\n",
            "Batch 230 / 533 Regressor Loss : 136960.125\n",
            "Batch 240 / 533 Regressor Loss : 115139.875\n",
            "Batch 250 / 533 Regressor Loss : 149363.03125\n",
            "Batch 260 / 533 Regressor Loss : 130370.3125\n",
            "Batch 270 / 533 Regressor Loss : 114891.890625\n",
            "Batch 280 / 533 Regressor Loss : 139483.25\n",
            "Batch 290 / 533 Regressor Loss : 144450.796875\n",
            "Batch 300 / 533 Regressor Loss : 146436.3125\n",
            "Batch 310 / 533 Regressor Loss : 143145.9375\n",
            "Batch 320 / 533 Regressor Loss : 143164.859375\n",
            "Batch 330 / 533 Regressor Loss : 180204.09375\n",
            "Batch 340 / 533 Regressor Loss : 113534.78125\n",
            "Batch 350 / 533 Regressor Loss : 118734.2734375\n",
            "Batch 360 / 533 Regressor Loss : 137855.796875\n",
            "Batch 370 / 533 Regressor Loss : 131338.5625\n",
            "Batch 380 / 533 Regressor Loss : 129176.4296875\n",
            "Batch 390 / 533 Regressor Loss : 135214.8125\n",
            "Batch 400 / 533 Regressor Loss : 117950.203125\n",
            "Batch 410 / 533 Regressor Loss : 118578.9609375\n",
            "Batch 420 / 533 Regressor Loss : 158896.171875\n",
            "Batch 430 / 533 Regressor Loss : 112823.53125\n",
            "Batch 440 / 533 Regressor Loss : 143200.3125\n",
            "Batch 450 / 533 Regressor Loss : 129888.65625\n",
            "Batch 460 / 533 Regressor Loss : 125535.1875\n",
            "Batch 470 / 533 Regressor Loss : 135153.375\n",
            "Batch 480 / 533 Regressor Loss : 137036.5625\n",
            "Batch 490 / 533 Regressor Loss : 131947.5625\n",
            "Batch 500 / 533 Regressor Loss : 140606.21875\n",
            "Batch 510 / 533 Regressor Loss : 125208.171875\n",
            "Batch 520 / 533 Regressor Loss : 145770.40625\n",
            "Batch 530 / 533 Regressor Loss : 134424.0625\n",
            "Epoch  2 / 100  Sammon Loss : 0.10167528688907623  Regressor Loss : 180471.53125\n",
            "Val Loss : 108015.953125\n",
            "Batch 10 / 996 Sammon Loss : 0.08157699555158615\n",
            "Batch 20 / 996 Sammon Loss : 0.07874353975057602\n",
            "Batch 30 / 996 Sammon Loss : 0.2001265287399292\n",
            "Batch 40 / 996 Sammon Loss : 0.6388979554176331\n",
            "Batch 50 / 996 Sammon Loss : 0.4997353255748749\n",
            "Batch 60 / 996 Sammon Loss : 0.015568501316010952\n",
            "Batch 70 / 996 Sammon Loss : 0.020589252933859825\n",
            "Batch 80 / 996 Sammon Loss : 0.01325412280857563\n",
            "Batch 90 / 996 Sammon Loss : 0.004658508114516735\n",
            "Batch 100 / 996 Sammon Loss : 1.2159729521954432e-05\n",
            "Batch 110 / 996 Sammon Loss : 0.0051923650316894054\n",
            "Batch 120 / 996 Sammon Loss : 0.004679187200963497\n",
            "Batch 130 / 996 Sammon Loss : 0.003148517105728388\n",
            "Batch 140 / 996 Sammon Loss : 0.011224309913814068\n",
            "Batch 150 / 996 Sammon Loss : 0.011687898077070713\n",
            "Batch 160 / 996 Sammon Loss : 3.835948882624507e-05\n",
            "Batch 170 / 996 Sammon Loss : 0.1331396996974945\n",
            "Batch 180 / 996 Sammon Loss : 0.014216262847185135\n",
            "Batch 190 / 996 Sammon Loss : 0.1351727545261383\n",
            "Batch 200 / 996 Sammon Loss : 0.026412708684802055\n",
            "Batch 210 / 996 Sammon Loss : 0.07068538665771484\n",
            "Batch 220 / 996 Sammon Loss : 0.002145718317478895\n",
            "Batch 230 / 996 Sammon Loss : 0.004437984898686409\n",
            "Batch 240 / 996 Sammon Loss : 0.03597616031765938\n",
            "Batch 250 / 996 Sammon Loss : 0.012286871671676636\n",
            "Batch 260 / 996 Sammon Loss : 0.006824851967394352\n",
            "Batch 270 / 996 Sammon Loss : 0.0017456189962103963\n",
            "Batch 280 / 996 Sammon Loss : 7.707896656938829e-06\n",
            "Batch 290 / 996 Sammon Loss : 0.01642686128616333\n",
            "Batch 300 / 996 Sammon Loss : 0.004375015385448933\n",
            "Batch 310 / 996 Sammon Loss : 0.013279675506055355\n",
            "Batch 320 / 996 Sammon Loss : 0.008480402640998363\n",
            "Batch 330 / 996 Sammon Loss : 0.0008406629203818738\n",
            "Batch 340 / 996 Sammon Loss : 0.0017843593377619982\n",
            "Batch 350 / 996 Sammon Loss : 0.0029190245550125837\n",
            "Batch 360 / 996 Sammon Loss : 0.003719308180734515\n",
            "Batch 370 / 996 Sammon Loss : 0.0018192067509517074\n",
            "Batch 380 / 996 Sammon Loss : 0.0028835716657340527\n",
            "Batch 390 / 996 Sammon Loss : 0.00048626528587192297\n",
            "Batch 400 / 996 Sammon Loss : 0.012783688493072987\n",
            "Batch 410 / 996 Sammon Loss : 0.10745854675769806\n",
            "Batch 420 / 996 Sammon Loss : 0.00019088717817794532\n",
            "Batch 430 / 996 Sammon Loss : 0.017760660499334335\n",
            "Batch 440 / 996 Sammon Loss : 0.9333309531211853\n",
            "Batch 450 / 996 Sammon Loss : 0.6056694984436035\n",
            "Batch 460 / 996 Sammon Loss : 0.2832539975643158\n",
            "Batch 470 / 996 Sammon Loss : 0.45209193229675293\n",
            "Batch 480 / 996 Sammon Loss : 0.002995423274114728\n",
            "Batch 490 / 996 Sammon Loss : 0.0015906468033790588\n",
            "Batch 500 / 996 Sammon Loss : 0.0011138106929138303\n",
            "Batch 510 / 996 Sammon Loss : 0.006475139409303665\n",
            "Batch 520 / 996 Sammon Loss : 0.07542223483324051\n",
            "Batch 530 / 996 Sammon Loss : 0.004877852275967598\n",
            "Batch 540 / 996 Sammon Loss : 0.00580395944416523\n",
            "Batch 550 / 996 Sammon Loss : 0.0006764080608263612\n",
            "Batch 560 / 996 Sammon Loss : 0.09773557633161545\n",
            "Batch 570 / 996 Sammon Loss : 0.005607006140053272\n",
            "Batch 580 / 996 Sammon Loss : 0.0005556208197958767\n",
            "Batch 590 / 996 Sammon Loss : 0.0010398252634331584\n",
            "Batch 600 / 996 Sammon Loss : 0.004017121158540249\n",
            "Batch 610 / 996 Sammon Loss : 0.016782378777861595\n",
            "Batch 620 / 996 Sammon Loss : 0.014550866559147835\n",
            "Batch 630 / 996 Sammon Loss : 0.0005367064150050282\n",
            "Batch 640 / 996 Sammon Loss : 0.00043372277286835015\n",
            "Batch 650 / 996 Sammon Loss : 0.003730217693373561\n",
            "Batch 660 / 996 Sammon Loss : 0.01826813630759716\n",
            "Batch 670 / 996 Sammon Loss : 0.0003253868198953569\n",
            "Batch 680 / 996 Sammon Loss : 0.0008296758169308305\n",
            "Batch 690 / 996 Sammon Loss : 0.022783378139138222\n",
            "Batch 700 / 996 Sammon Loss : 0.11024453490972519\n",
            "Batch 710 / 996 Sammon Loss : 0.000536370906047523\n",
            "Batch 720 / 996 Sammon Loss : 0.009732747450470924\n",
            "Batch 730 / 996 Sammon Loss : 0.009295541793107986\n",
            "Batch 740 / 996 Sammon Loss : 0.00014817205374129117\n",
            "Batch 750 / 996 Sammon Loss : 0.05621573328971863\n",
            "Batch 760 / 996 Sammon Loss : 0.20933867990970612\n",
            "Batch 770 / 996 Sammon Loss : 0.1631067842245102\n",
            "Batch 780 / 996 Sammon Loss : 0.009771456941962242\n",
            "Batch 790 / 996 Sammon Loss : 0.002278807107359171\n",
            "Batch 800 / 996 Sammon Loss : 0.0059986119158566\n",
            "Batch 810 / 996 Sammon Loss : 0.017237598076462746\n",
            "Batch 820 / 996 Sammon Loss : 3.954652856918983e-05\n",
            "Batch 830 / 996 Sammon Loss : 0.0023568561300635338\n",
            "Batch 840 / 996 Sammon Loss : 0.007355937734246254\n",
            "Batch 850 / 996 Sammon Loss : 0.001690315897576511\n",
            "Batch 860 / 996 Sammon Loss : 0.004987191874533892\n",
            "Batch 870 / 996 Sammon Loss : 0.0038882088847458363\n",
            "Batch 880 / 996 Sammon Loss : 0.009920617565512657\n",
            "Batch 890 / 996 Sammon Loss : 0.0006732910987921059\n",
            "Batch 900 / 996 Sammon Loss : 0.0006641563377343118\n",
            "Batch 910 / 996 Sammon Loss : 0.2249966412782669\n",
            "Batch 920 / 996 Sammon Loss : 0.012795130722224712\n",
            "Batch 930 / 996 Sammon Loss : 0.018683677539229393\n",
            "Batch 940 / 996 Sammon Loss : 0.17804653942584991\n",
            "Batch 950 / 996 Sammon Loss : 0.0008131636423058808\n",
            "Batch 960 / 996 Sammon Loss : 0.012117677368223667\n",
            "Batch 970 / 996 Sammon Loss : 0.01563197374343872\n",
            "Batch 980 / 996 Sammon Loss : 0.00772522296756506\n",
            "Batch 990 / 996 Sammon Loss : 0.0009125439100898802\n",
            "Batch 10 / 533 Regressor Loss : 146419.0\n",
            "Batch 20 / 533 Regressor Loss : 119258.359375\n",
            "Batch 30 / 533 Regressor Loss : 141602.921875\n",
            "Batch 40 / 533 Regressor Loss : 149053.21875\n",
            "Batch 50 / 533 Regressor Loss : 136523.890625\n",
            "Batch 60 / 533 Regressor Loss : 128035.5859375\n",
            "Batch 70 / 533 Regressor Loss : 134445.21875\n",
            "Batch 80 / 533 Regressor Loss : 122579.140625\n",
            "Batch 90 / 533 Regressor Loss : 133399.609375\n",
            "Batch 100 / 533 Regressor Loss : 148233.71875\n",
            "Batch 110 / 533 Regressor Loss : 126321.359375\n",
            "Batch 120 / 533 Regressor Loss : 114562.640625\n",
            "Batch 130 / 533 Regressor Loss : 144344.265625\n",
            "Batch 140 / 533 Regressor Loss : 115980.1171875\n",
            "Batch 150 / 533 Regressor Loss : 116217.921875\n",
            "Batch 160 / 533 Regressor Loss : 140304.25\n",
            "Batch 170 / 533 Regressor Loss : 147600.171875\n",
            "Batch 180 / 533 Regressor Loss : 128847.78125\n",
            "Batch 190 / 533 Regressor Loss : 150723.3125\n",
            "Batch 200 / 533 Regressor Loss : 145585.5\n",
            "Batch 210 / 533 Regressor Loss : 115501.484375\n",
            "Batch 220 / 533 Regressor Loss : 112054.6171875\n",
            "Batch 230 / 533 Regressor Loss : 136680.9375\n",
            "Batch 240 / 533 Regressor Loss : 114851.0234375\n",
            "Batch 250 / 533 Regressor Loss : 149026.375\n",
            "Batch 260 / 533 Regressor Loss : 130064.046875\n",
            "Batch 270 / 533 Regressor Loss : 114599.9609375\n",
            "Batch 280 / 533 Regressor Loss : 139166.234375\n",
            "Batch 290 / 533 Regressor Loss : 144125.3125\n",
            "Batch 300 / 533 Regressor Loss : 146152.734375\n",
            "Batch 310 / 533 Regressor Loss : 142837.046875\n",
            "Batch 320 / 533 Regressor Loss : 142851.6875\n",
            "Batch 330 / 533 Regressor Loss : 179891.234375\n",
            "Batch 340 / 533 Regressor Loss : 113250.9921875\n",
            "Batch 350 / 533 Regressor Loss : 118441.53125\n",
            "Batch 360 / 533 Regressor Loss : 137552.515625\n",
            "Batch 370 / 533 Regressor Loss : 131051.859375\n",
            "Batch 380 / 533 Regressor Loss : 128887.125\n",
            "Batch 390 / 533 Regressor Loss : 134917.671875\n",
            "Batch 400 / 533 Regressor Loss : 117669.359375\n",
            "Batch 410 / 533 Regressor Loss : 118299.5234375\n",
            "Batch 420 / 533 Regressor Loss : 158584.15625\n",
            "Batch 430 / 533 Regressor Loss : 112553.9921875\n",
            "Batch 440 / 533 Regressor Loss : 142891.125\n",
            "Batch 450 / 533 Regressor Loss : 129591.7109375\n",
            "Batch 460 / 533 Regressor Loss : 125270.2109375\n",
            "Batch 470 / 533 Regressor Loss : 134855.234375\n",
            "Batch 480 / 533 Regressor Loss : 136720.09375\n",
            "Batch 490 / 533 Regressor Loss : 131653.375\n",
            "Batch 500 / 533 Regressor Loss : 140315.21875\n",
            "Batch 510 / 533 Regressor Loss : 124935.1796875\n",
            "Batch 520 / 533 Regressor Loss : 145449.875\n",
            "Batch 530 / 533 Regressor Loss : 134126.375\n",
            "Epoch  3 / 100  Sammon Loss : 0.08799730986356735  Regressor Loss : 180108.90625\n",
            "Val Loss : 107724.40625\n",
            "Batch 10 / 996 Sammon Loss : 0.10394291579723358\n",
            "Batch 20 / 996 Sammon Loss : 0.07298097759485245\n",
            "Batch 30 / 996 Sammon Loss : 0.17779645323753357\n",
            "Batch 40 / 996 Sammon Loss : 1.2294000387191772\n",
            "Batch 50 / 996 Sammon Loss : 0.9172654747962952\n",
            "Batch 60 / 996 Sammon Loss : 0.07883239537477493\n",
            "Batch 70 / 996 Sammon Loss : 0.12291055917739868\n",
            "Batch 80 / 996 Sammon Loss : 0.1425311416387558\n",
            "Batch 90 / 996 Sammon Loss : 0.04519304260611534\n",
            "Batch 100 / 996 Sammon Loss : 0.04039505496621132\n",
            "Batch 110 / 996 Sammon Loss : 0.018384914845228195\n",
            "Batch 120 / 996 Sammon Loss : 0.10209954530000687\n",
            "Batch 130 / 996 Sammon Loss : 0.004028654657304287\n",
            "Batch 140 / 996 Sammon Loss : 0.05743428319692612\n",
            "Batch 150 / 996 Sammon Loss : 0.0036336607299745083\n",
            "Batch 160 / 996 Sammon Loss : 0.016144920140504837\n",
            "Batch 170 / 996 Sammon Loss : 0.4859280288219452\n",
            "Batch 180 / 996 Sammon Loss : 0.25369012355804443\n",
            "Batch 190 / 996 Sammon Loss : 0.41471028327941895\n",
            "Batch 200 / 996 Sammon Loss : 0.21295586228370667\n",
            "Batch 210 / 996 Sammon Loss : 0.3082476854324341\n",
            "Batch 220 / 996 Sammon Loss : 0.09034281224012375\n",
            "Batch 230 / 996 Sammon Loss : 0.06170253828167915\n",
            "Batch 240 / 996 Sammon Loss : 0.11200668662786484\n",
            "Batch 250 / 996 Sammon Loss : 0.07851611822843552\n",
            "Batch 260 / 996 Sammon Loss : 0.012308330275118351\n",
            "Batch 270 / 996 Sammon Loss : 0.030966365709900856\n",
            "Batch 280 / 996 Sammon Loss : 0.02386641316115856\n",
            "Batch 290 / 996 Sammon Loss : 0.136982724070549\n",
            "Batch 300 / 996 Sammon Loss : 0.026798885315656662\n",
            "Batch 310 / 996 Sammon Loss : 0.055974334478378296\n",
            "Batch 320 / 996 Sammon Loss : 0.004273951053619385\n",
            "Batch 330 / 996 Sammon Loss : 0.06088273599743843\n",
            "Batch 340 / 996 Sammon Loss : 0.044775743037462234\n",
            "Batch 350 / 996 Sammon Loss : 0.006247549783438444\n",
            "Batch 360 / 996 Sammon Loss : 0.017748525366187096\n",
            "Batch 370 / 996 Sammon Loss : 0.11245974898338318\n",
            "Batch 380 / 996 Sammon Loss : 0.016434192657470703\n",
            "Batch 390 / 996 Sammon Loss : 0.08358185738325119\n",
            "Batch 400 / 996 Sammon Loss : 0.006586446426808834\n",
            "Batch 410 / 996 Sammon Loss : 0.3395689129829407\n",
            "Batch 420 / 996 Sammon Loss : 0.029742982238531113\n",
            "Batch 430 / 996 Sammon Loss : 0.11692529916763306\n",
            "Batch 440 / 996 Sammon Loss : 1.735817551612854\n",
            "Batch 450 / 996 Sammon Loss : 1.3682215213775635\n",
            "Batch 460 / 996 Sammon Loss : 0.6467912197113037\n",
            "Batch 470 / 996 Sammon Loss : 0.9904451966285706\n",
            "Batch 480 / 996 Sammon Loss : 0.017613964155316353\n",
            "Batch 490 / 996 Sammon Loss : 0.06334782391786575\n",
            "Batch 500 / 996 Sammon Loss : 0.07935891300439835\n",
            "Batch 510 / 996 Sammon Loss : 0.08470287919044495\n",
            "Batch 520 / 996 Sammon Loss : 0.23523087799549103\n",
            "Batch 530 / 996 Sammon Loss : 0.05122263729572296\n",
            "Batch 540 / 996 Sammon Loss : 0.021481750532984734\n",
            "Batch 550 / 996 Sammon Loss : 0.05017951503396034\n",
            "Batch 560 / 996 Sammon Loss : 0.48329320549964905\n",
            "Batch 570 / 996 Sammon Loss : 0.1966220587491989\n",
            "Batch 580 / 996 Sammon Loss : 0.12861011922359467\n",
            "Batch 590 / 996 Sammon Loss : 0.11202364414930344\n",
            "Batch 600 / 996 Sammon Loss : 0.009871350601315498\n",
            "Batch 610 / 996 Sammon Loss : 0.010869883000850677\n",
            "Batch 620 / 996 Sammon Loss : 0.0026678089052438736\n",
            "Batch 630 / 996 Sammon Loss : 0.048140350729227066\n",
            "Batch 640 / 996 Sammon Loss : 0.03548266738653183\n",
            "Batch 650 / 996 Sammon Loss : 0.039931777864694595\n",
            "Batch 660 / 996 Sammon Loss : 0.012310219928622246\n",
            "Batch 670 / 996 Sammon Loss : 0.008603288792073727\n",
            "Batch 680 / 996 Sammon Loss : 0.0358506441116333\n",
            "Batch 690 / 996 Sammon Loss : 0.1296980232000351\n",
            "Batch 700 / 996 Sammon Loss : 0.46019411087036133\n",
            "Batch 710 / 996 Sammon Loss : 0.057425301522016525\n",
            "Batch 720 / 996 Sammon Loss : 0.03581381216645241\n",
            "Batch 730 / 996 Sammon Loss : 0.012726493179798126\n",
            "Batch 740 / 996 Sammon Loss : 0.04381942003965378\n",
            "Batch 750 / 996 Sammon Loss : 0.2641753554344177\n",
            "Batch 760 / 996 Sammon Loss : 0.6067297458648682\n",
            "Batch 770 / 996 Sammon Loss : 0.5238189101219177\n",
            "Batch 780 / 996 Sammon Loss : 0.08815564960241318\n",
            "Batch 790 / 996 Sammon Loss : 0.0663556158542633\n",
            "Batch 800 / 996 Sammon Loss : 0.040857598185539246\n",
            "Batch 810 / 996 Sammon Loss : 0.09558994323015213\n",
            "Batch 820 / 996 Sammon Loss : 0.03156011551618576\n",
            "Batch 830 / 996 Sammon Loss : 0.07995663583278656\n",
            "Batch 840 / 996 Sammon Loss : 0.08731354773044586\n",
            "Batch 850 / 996 Sammon Loss : 0.07282628864049911\n",
            "Batch 860 / 996 Sammon Loss : 0.0019145015394315124\n",
            "Batch 870 / 996 Sammon Loss : 0.0565665028989315\n",
            "Batch 880 / 996 Sammon Loss : 0.01355814840644598\n",
            "Batch 890 / 996 Sammon Loss : 0.01601545698940754\n",
            "Batch 900 / 996 Sammon Loss : 0.01964319497346878\n",
            "Batch 910 / 996 Sammon Loss : 0.5307192206382751\n",
            "Batch 920 / 996 Sammon Loss : 0.08494897186756134\n",
            "Batch 930 / 996 Sammon Loss : 0.11853787302970886\n",
            "Batch 940 / 996 Sammon Loss : 0.4571523368358612\n",
            "Batch 950 / 996 Sammon Loss : 0.031093256548047066\n",
            "Batch 960 / 996 Sammon Loss : 0.01143194641917944\n",
            "Batch 970 / 996 Sammon Loss : 0.16954004764556885\n",
            "Batch 980 / 996 Sammon Loss : 0.05211907625198364\n",
            "Batch 990 / 996 Sammon Loss : 0.00949271209537983\n",
            "Batch 10 / 533 Regressor Loss : 146127.65625\n",
            "Batch 20 / 533 Regressor Loss : 118976.765625\n",
            "Batch 30 / 533 Regressor Loss : 141299.96875\n",
            "Batch 40 / 533 Regressor Loss : 148754.15625\n",
            "Batch 50 / 533 Regressor Loss : 136248.21875\n",
            "Batch 60 / 533 Regressor Loss : 127741.359375\n",
            "Batch 70 / 533 Regressor Loss : 134136.46875\n",
            "Batch 80 / 533 Regressor Loss : 122296.828125\n",
            "Batch 90 / 533 Regressor Loss : 133111.609375\n",
            "Batch 100 / 533 Regressor Loss : 147910.390625\n",
            "Batch 110 / 533 Regressor Loss : 126054.265625\n",
            "Batch 120 / 533 Regressor Loss : 114306.3125\n",
            "Batch 130 / 533 Regressor Loss : 144031.65625\n",
            "Batch 140 / 533 Regressor Loss : 115711.140625\n",
            "Batch 150 / 533 Regressor Loss : 115941.234375\n",
            "Batch 160 / 533 Regressor Loss : 140033.09375\n",
            "Batch 170 / 533 Regressor Loss : 147302.8125\n",
            "Batch 180 / 533 Regressor Loss : 128557.484375\n",
            "Batch 190 / 533 Regressor Loss : 150434.859375\n",
            "Batch 200 / 533 Regressor Loss : 145292.921875\n",
            "Batch 210 / 533 Regressor Loss : 115232.484375\n",
            "Batch 220 / 533 Regressor Loss : 111789.7421875\n",
            "Batch 230 / 533 Regressor Loss : 136415.328125\n",
            "Batch 240 / 533 Regressor Loss : 114575.9921875\n",
            "Batch 250 / 533 Regressor Loss : 148704.90625\n",
            "Batch 260 / 533 Regressor Loss : 129771.7109375\n",
            "Batch 270 / 533 Regressor Loss : 114321.296875\n",
            "Batch 280 / 533 Regressor Loss : 138863.21875\n",
            "Batch 290 / 533 Regressor Loss : 143813.84375\n",
            "Batch 300 / 533 Regressor Loss : 145881.5625\n",
            "Batch 310 / 533 Regressor Loss : 142541.28125\n",
            "Batch 320 / 533 Regressor Loss : 142551.609375\n",
            "Batch 330 / 533 Regressor Loss : 179591.171875\n",
            "Batch 340 / 533 Regressor Loss : 112979.1484375\n",
            "Batch 350 / 533 Regressor Loss : 118160.8125\n",
            "Batch 360 / 533 Regressor Loss : 137261.40625\n",
            "Batch 370 / 533 Regressor Loss : 130776.671875\n",
            "Batch 380 / 533 Regressor Loss : 128609.3359375\n",
            "Batch 390 / 533 Regressor Loss : 134632.140625\n",
            "Batch 400 / 533 Regressor Loss : 117399.453125\n",
            "Batch 410 / 533 Regressor Loss : 118030.84375\n",
            "Batch 420 / 533 Regressor Loss : 158283.625\n",
            "Batch 430 / 533 Regressor Loss : 112294.671875\n",
            "Batch 440 / 533 Regressor Loss : 142593.15625\n",
            "Batch 450 / 533 Regressor Loss : 129305.5625\n",
            "Batch 460 / 533 Regressor Loss : 125014.921875\n",
            "Batch 470 / 533 Regressor Loss : 134567.671875\n",
            "Batch 480 / 533 Regressor Loss : 136414.5\n",
            "Batch 490 / 533 Regressor Loss : 131369.40625\n",
            "Batch 500 / 533 Regressor Loss : 140034.15625\n",
            "Batch 510 / 533 Regressor Loss : 124671.59375\n",
            "Batch 520 / 533 Regressor Loss : 145139.8125\n",
            "Batch 530 / 533 Regressor Loss : 133838.4375\n",
            "Epoch  4 / 100  Sammon Loss : 0.3046169877052307  Regressor Loss : 179757.75\n",
            "Val Loss : 107442.484375\n",
            "Batch 10 / 996 Sammon Loss : 0.348817378282547\n",
            "Batch 20 / 996 Sammon Loss : 0.39586299657821655\n",
            "Batch 30 / 996 Sammon Loss : 0.5528454184532166\n",
            "Batch 40 / 996 Sammon Loss : 1.2294000387191772\n",
            "Batch 50 / 996 Sammon Loss : 0.9172654747962952\n",
            "Batch 60 / 996 Sammon Loss : 0.07883239537477493\n",
            "Batch 70 / 996 Sammon Loss : 0.12291055917739868\n",
            "Batch 80 / 996 Sammon Loss : 0.1425311416387558\n",
            "Batch 90 / 996 Sammon Loss : 0.04519304260611534\n",
            "Batch 100 / 996 Sammon Loss : 0.04039505496621132\n",
            "Batch 110 / 996 Sammon Loss : 0.018384914845228195\n",
            "Batch 120 / 996 Sammon Loss : 0.10209954530000687\n",
            "Batch 130 / 996 Sammon Loss : 0.004028654657304287\n",
            "Batch 140 / 996 Sammon Loss : 0.05743428319692612\n",
            "Batch 150 / 996 Sammon Loss : 0.0036336607299745083\n",
            "Batch 160 / 996 Sammon Loss : 0.016144920140504837\n",
            "Batch 170 / 996 Sammon Loss : 0.4859280288219452\n",
            "Batch 180 / 996 Sammon Loss : 0.25369012355804443\n",
            "Batch 190 / 996 Sammon Loss : 0.41471028327941895\n",
            "Batch 200 / 996 Sammon Loss : 0.21295586228370667\n",
            "Batch 210 / 996 Sammon Loss : 0.3082476854324341\n",
            "Batch 220 / 996 Sammon Loss : 0.09034281224012375\n",
            "Batch 230 / 996 Sammon Loss : 0.06170253828167915\n",
            "Batch 240 / 996 Sammon Loss : 0.11200668662786484\n",
            "Batch 250 / 996 Sammon Loss : 0.07851611822843552\n",
            "Batch 260 / 996 Sammon Loss : 0.012308330275118351\n",
            "Batch 270 / 996 Sammon Loss : 0.030966365709900856\n",
            "Batch 280 / 996 Sammon Loss : 0.02386641316115856\n",
            "Batch 290 / 996 Sammon Loss : 0.136982724070549\n",
            "Batch 300 / 996 Sammon Loss : 0.026798885315656662\n",
            "Batch 310 / 996 Sammon Loss : 0.055974334478378296\n",
            "Batch 320 / 996 Sammon Loss : 0.004273951053619385\n",
            "Batch 330 / 996 Sammon Loss : 0.06088273599743843\n",
            "Batch 340 / 996 Sammon Loss : 0.044775743037462234\n",
            "Batch 350 / 996 Sammon Loss : 0.006247549783438444\n",
            "Batch 360 / 996 Sammon Loss : 0.017748525366187096\n",
            "Batch 370 / 996 Sammon Loss : 0.11245974898338318\n",
            "Batch 380 / 996 Sammon Loss : 0.016434192657470703\n",
            "Batch 390 / 996 Sammon Loss : 0.08358185738325119\n",
            "Batch 400 / 996 Sammon Loss : 0.006586446426808834\n",
            "Batch 410 / 996 Sammon Loss : 0.3395689129829407\n",
            "Batch 420 / 996 Sammon Loss : 0.029742982238531113\n",
            "Batch 430 / 996 Sammon Loss : 0.11692529916763306\n",
            "Batch 440 / 996 Sammon Loss : 1.735817551612854\n",
            "Batch 450 / 996 Sammon Loss : 1.3682215213775635\n",
            "Batch 460 / 996 Sammon Loss : 0.6467912197113037\n",
            "Batch 470 / 996 Sammon Loss : 0.9904451966285706\n",
            "Batch 480 / 996 Sammon Loss : 0.017613964155316353\n",
            "Batch 490 / 996 Sammon Loss : 0.06334782391786575\n",
            "Batch 500 / 996 Sammon Loss : 0.07935891300439835\n",
            "Batch 510 / 996 Sammon Loss : 0.08470287919044495\n",
            "Batch 520 / 996 Sammon Loss : 0.23523087799549103\n",
            "Batch 530 / 996 Sammon Loss : 0.05122263729572296\n",
            "Batch 540 / 996 Sammon Loss : 0.021481750532984734\n",
            "Batch 550 / 996 Sammon Loss : 0.05017951503396034\n",
            "Batch 560 / 996 Sammon Loss : 0.48329320549964905\n",
            "Batch 570 / 996 Sammon Loss : 0.1966220587491989\n",
            "Batch 580 / 996 Sammon Loss : 0.12861011922359467\n",
            "Batch 590 / 996 Sammon Loss : 0.11202364414930344\n",
            "Batch 600 / 996 Sammon Loss : 0.009871350601315498\n",
            "Batch 610 / 996 Sammon Loss : 0.010869883000850677\n",
            "Batch 620 / 996 Sammon Loss : 0.0026678089052438736\n",
            "Batch 630 / 996 Sammon Loss : 0.048140350729227066\n",
            "Batch 640 / 996 Sammon Loss : 0.03548266738653183\n",
            "Batch 650 / 996 Sammon Loss : 0.039931777864694595\n",
            "Batch 660 / 996 Sammon Loss : 0.012310219928622246\n",
            "Batch 670 / 996 Sammon Loss : 0.008603288792073727\n",
            "Batch 680 / 996 Sammon Loss : 0.0358506441116333\n",
            "Batch 690 / 996 Sammon Loss : 0.1296980232000351\n",
            "Batch 700 / 996 Sammon Loss : 0.46019411087036133\n",
            "Batch 710 / 996 Sammon Loss : 0.057425301522016525\n",
            "Batch 720 / 996 Sammon Loss : 0.03581381216645241\n",
            "Batch 730 / 996 Sammon Loss : 0.012726493179798126\n",
            "Batch 740 / 996 Sammon Loss : 0.04381942003965378\n",
            "Batch 750 / 996 Sammon Loss : 0.2641753554344177\n",
            "Batch 760 / 996 Sammon Loss : 0.6067297458648682\n",
            "Batch 770 / 996 Sammon Loss : 0.5238189101219177\n",
            "Batch 780 / 996 Sammon Loss : 0.08815564960241318\n",
            "Batch 790 / 996 Sammon Loss : 0.0663556158542633\n",
            "Batch 800 / 996 Sammon Loss : 0.040857598185539246\n",
            "Batch 810 / 996 Sammon Loss : 0.09558994323015213\n",
            "Batch 820 / 996 Sammon Loss : 0.03156011551618576\n",
            "Batch 830 / 996 Sammon Loss : 0.07995663583278656\n",
            "Batch 840 / 996 Sammon Loss : 0.08731354773044586\n",
            "Batch 850 / 996 Sammon Loss : 0.07282628864049911\n",
            "Batch 860 / 996 Sammon Loss : 0.0019145015394315124\n",
            "Batch 870 / 996 Sammon Loss : 0.0565665028989315\n",
            "Batch 880 / 996 Sammon Loss : 0.01355814840644598\n",
            "Batch 890 / 996 Sammon Loss : 0.01601545698940754\n",
            "Batch 900 / 996 Sammon Loss : 0.01964319497346878\n",
            "Batch 910 / 996 Sammon Loss : 0.5307192206382751\n",
            "Batch 920 / 996 Sammon Loss : 0.08494897186756134\n",
            "Batch 930 / 996 Sammon Loss : 0.11853787302970886\n",
            "Batch 940 / 996 Sammon Loss : 0.4571523368358612\n",
            "Batch 950 / 996 Sammon Loss : 0.031093256548047066\n",
            "Batch 960 / 996 Sammon Loss : 0.01143194641917944\n",
            "Batch 970 / 996 Sammon Loss : 0.16954004764556885\n",
            "Batch 980 / 996 Sammon Loss : 0.05211907625198364\n",
            "Batch 990 / 996 Sammon Loss : 0.00949271209537983\n",
            "Batch 10 / 533 Regressor Loss : 145845.78125\n",
            "Batch 20 / 533 Regressor Loss : 118704.2734375\n",
            "Batch 30 / 533 Regressor Loss : 141006.546875\n",
            "Batch 40 / 533 Regressor Loss : 148464.40625\n",
            "Batch 50 / 533 Regressor Loss : 135981.21875\n",
            "Batch 60 / 533 Regressor Loss : 127456.1796875\n",
            "Batch 70 / 533 Regressor Loss : 133836.921875\n",
            "Batch 80 / 533 Regressor Loss : 122023.046875\n",
            "Batch 90 / 533 Regressor Loss : 132832.171875\n",
            "Batch 100 / 533 Regressor Loss : 147596.328125\n",
            "Batch 110 / 533 Regressor Loss : 125795.0859375\n",
            "Batch 120 / 533 Regressor Loss : 114057.625\n",
            "Batch 130 / 533 Regressor Loss : 143727.84375\n",
            "Batch 140 / 533 Regressor Loss : 115449.8984375\n",
            "Batch 150 / 533 Regressor Loss : 115672.359375\n",
            "Batch 160 / 533 Regressor Loss : 139769.546875\n",
            "Batch 170 / 533 Regressor Loss : 147013.53125\n",
            "Batch 180 / 533 Regressor Loss : 128275.015625\n",
            "Batch 190 / 533 Regressor Loss : 150154.0625\n",
            "Batch 200 / 533 Regressor Loss : 145008.0625\n",
            "Batch 210 / 533 Regressor Loss : 114970.6484375\n",
            "Batch 220 / 533 Regressor Loss : 111531.859375\n",
            "Batch 230 / 533 Regressor Loss : 136156.671875\n",
            "Batch 240 / 533 Regressor Loss : 114308.0234375\n",
            "Batch 250 / 533 Regressor Loss : 148391.359375\n",
            "Batch 260 / 533 Regressor Loss : 129486.5859375\n",
            "Batch 270 / 533 Regressor Loss : 114049.53125\n",
            "Batch 280 / 533 Regressor Loss : 138567.5\n",
            "Batch 290 / 533 Regressor Loss : 143509.765625\n",
            "Batch 300 / 533 Regressor Loss : 145616.921875\n",
            "Batch 310 / 533 Regressor Loss : 142252.421875\n",
            "Batch 320 / 533 Regressor Loss : 142258.453125\n",
            "Batch 330 / 533 Regressor Loss : 179297.921875\n",
            "Batch 340 / 533 Regressor Loss : 112713.6171875\n",
            "Batch 350 / 533 Regressor Loss : 117886.5\n",
            "Batch 360 / 533 Regressor Loss : 136976.828125\n",
            "Batch 370 / 533 Regressor Loss : 130507.640625\n",
            "Batch 380 / 533 Regressor Loss : 128337.7109375\n",
            "Batch 390 / 533 Regressor Loss : 134352.859375\n",
            "Batch 400 / 533 Regressor Loss : 117135.4609375\n",
            "Batch 410 / 533 Regressor Loss : 117767.984375\n",
            "Batch 420 / 533 Regressor Loss : 157989.375\n",
            "Batch 430 / 533 Regressor Loss : 112040.8984375\n",
            "Batch 440 / 533 Regressor Loss : 142301.296875\n",
            "Batch 450 / 533 Regressor Loss : 129025.3125\n",
            "Batch 460 / 533 Regressor Loss : 124764.953125\n",
            "Batch 470 / 533 Regressor Loss : 134285.90625\n",
            "Batch 480 / 533 Regressor Loss : 136114.90625\n",
            "Batch 490 / 533 Regressor Loss : 131091.046875\n",
            "Batch 500 / 533 Regressor Loss : 139758.5625\n",
            "Batch 510 / 533 Regressor Loss : 124413.25\n",
            "Batch 520 / 533 Regressor Loss : 144835.578125\n",
            "Batch 530 / 533 Regressor Loss : 133555.96875\n",
            "Epoch  5 / 100  Sammon Loss : 0.3046169877052307  Regressor Loss : 179413.03125\n",
            "Val Loss : 107165.9296875\n",
            "Batch 10 / 996 Sammon Loss : 0.348817378282547\n",
            "Batch 20 / 996 Sammon Loss : 0.39586299657821655\n",
            "Batch 30 / 996 Sammon Loss : 0.5528454184532166\n",
            "Batch 40 / 996 Sammon Loss : 1.2294000387191772\n",
            "Batch 50 / 996 Sammon Loss : 0.9172654747962952\n",
            "Batch 60 / 996 Sammon Loss : 0.07883239537477493\n",
            "Batch 70 / 996 Sammon Loss : 0.12291055917739868\n",
            "Batch 80 / 996 Sammon Loss : 0.1425311416387558\n",
            "Batch 90 / 996 Sammon Loss : 0.04519304260611534\n",
            "Batch 100 / 996 Sammon Loss : 0.04039505496621132\n",
            "Batch 110 / 996 Sammon Loss : 0.018384914845228195\n",
            "Batch 120 / 996 Sammon Loss : 0.10209954530000687\n",
            "Batch 130 / 996 Sammon Loss : 0.004028654657304287\n",
            "Batch 140 / 996 Sammon Loss : 0.05743428319692612\n",
            "Batch 150 / 996 Sammon Loss : 0.0036336607299745083\n",
            "Batch 160 / 996 Sammon Loss : 0.016144920140504837\n",
            "Batch 170 / 996 Sammon Loss : 0.4859280288219452\n",
            "Batch 180 / 996 Sammon Loss : 0.25369012355804443\n",
            "Batch 190 / 996 Sammon Loss : 0.41471028327941895\n",
            "Batch 200 / 996 Sammon Loss : 0.21295586228370667\n",
            "Batch 210 / 996 Sammon Loss : 0.3082476854324341\n",
            "Batch 220 / 996 Sammon Loss : 0.09034281224012375\n",
            "Batch 230 / 996 Sammon Loss : 0.06170253828167915\n",
            "Batch 240 / 996 Sammon Loss : 0.11200668662786484\n",
            "Batch 250 / 996 Sammon Loss : 0.07851611822843552\n",
            "Batch 260 / 996 Sammon Loss : 0.012308330275118351\n",
            "Batch 270 / 996 Sammon Loss : 0.030966365709900856\n",
            "Batch 280 / 996 Sammon Loss : 0.02386641316115856\n",
            "Batch 290 / 996 Sammon Loss : 0.136982724070549\n",
            "Batch 300 / 996 Sammon Loss : 0.026798885315656662\n",
            "Batch 310 / 996 Sammon Loss : 0.055974334478378296\n",
            "Batch 320 / 996 Sammon Loss : 0.004273951053619385\n",
            "Batch 330 / 996 Sammon Loss : 0.06088273599743843\n",
            "Batch 340 / 996 Sammon Loss : 0.044775743037462234\n",
            "Batch 350 / 996 Sammon Loss : 0.006247549783438444\n",
            "Batch 360 / 996 Sammon Loss : 0.017748525366187096\n",
            "Batch 370 / 996 Sammon Loss : 0.11245974898338318\n",
            "Batch 380 / 996 Sammon Loss : 0.016434192657470703\n",
            "Batch 390 / 996 Sammon Loss : 0.08358185738325119\n",
            "Batch 400 / 996 Sammon Loss : 0.006586446426808834\n",
            "Batch 410 / 996 Sammon Loss : 0.3395689129829407\n",
            "Batch 420 / 996 Sammon Loss : 0.029742982238531113\n",
            "Batch 430 / 996 Sammon Loss : 0.11692529916763306\n",
            "Batch 440 / 996 Sammon Loss : 1.735817551612854\n",
            "Batch 450 / 996 Sammon Loss : 1.3682215213775635\n",
            "Batch 460 / 996 Sammon Loss : 0.6467912197113037\n",
            "Batch 470 / 996 Sammon Loss : 0.9904451966285706\n",
            "Batch 480 / 996 Sammon Loss : 0.017613964155316353\n",
            "Batch 490 / 996 Sammon Loss : 0.06334782391786575\n",
            "Batch 500 / 996 Sammon Loss : 0.07935891300439835\n",
            "Batch 510 / 996 Sammon Loss : 0.08470287919044495\n",
            "Batch 520 / 996 Sammon Loss : 0.23523087799549103\n",
            "Batch 530 / 996 Sammon Loss : 0.05122263729572296\n",
            "Batch 540 / 996 Sammon Loss : 0.021481750532984734\n",
            "Batch 550 / 996 Sammon Loss : 0.05017951503396034\n",
            "Batch 560 / 996 Sammon Loss : 0.48329320549964905\n",
            "Batch 570 / 996 Sammon Loss : 0.1966220587491989\n",
            "Batch 580 / 996 Sammon Loss : 0.12861011922359467\n",
            "Batch 590 / 996 Sammon Loss : 0.11202364414930344\n",
            "Batch 600 / 996 Sammon Loss : 0.009871350601315498\n",
            "Batch 610 / 996 Sammon Loss : 0.010869883000850677\n",
            "Batch 620 / 996 Sammon Loss : 0.0026678089052438736\n",
            "Batch 630 / 996 Sammon Loss : 0.048140350729227066\n",
            "Batch 640 / 996 Sammon Loss : 0.03548266738653183\n",
            "Batch 650 / 996 Sammon Loss : 0.039931777864694595\n",
            "Batch 660 / 996 Sammon Loss : 0.012310219928622246\n",
            "Batch 670 / 996 Sammon Loss : 0.008603288792073727\n",
            "Batch 680 / 996 Sammon Loss : 0.0358506441116333\n",
            "Batch 690 / 996 Sammon Loss : 0.1296980232000351\n",
            "Batch 700 / 996 Sammon Loss : 0.46019411087036133\n",
            "Batch 710 / 996 Sammon Loss : 0.057425301522016525\n",
            "Batch 720 / 996 Sammon Loss : 0.03581381216645241\n",
            "Batch 730 / 996 Sammon Loss : 0.012726493179798126\n",
            "Batch 740 / 996 Sammon Loss : 0.04381942003965378\n",
            "Batch 750 / 996 Sammon Loss : 0.2641753554344177\n",
            "Batch 760 / 996 Sammon Loss : 0.6067297458648682\n",
            "Batch 770 / 996 Sammon Loss : 0.5238189101219177\n",
            "Batch 780 / 996 Sammon Loss : 0.08815564960241318\n",
            "Batch 790 / 996 Sammon Loss : 0.0663556158542633\n",
            "Batch 800 / 996 Sammon Loss : 0.040857598185539246\n",
            "Batch 810 / 996 Sammon Loss : 0.09558994323015213\n",
            "Batch 820 / 996 Sammon Loss : 0.03156011551618576\n",
            "Batch 830 / 996 Sammon Loss : 0.07995663583278656\n",
            "Batch 840 / 996 Sammon Loss : 0.08731354773044586\n",
            "Batch 850 / 996 Sammon Loss : 0.07282628864049911\n",
            "Batch 860 / 996 Sammon Loss : 0.0019145015394315124\n",
            "Batch 870 / 996 Sammon Loss : 0.0565665028989315\n",
            "Batch 880 / 996 Sammon Loss : 0.01355814840644598\n",
            "Batch 890 / 996 Sammon Loss : 0.01601545698940754\n",
            "Batch 900 / 996 Sammon Loss : 0.01964319497346878\n",
            "Batch 910 / 996 Sammon Loss : 0.5307192206382751\n",
            "Batch 920 / 996 Sammon Loss : 0.08494897186756134\n",
            "Batch 930 / 996 Sammon Loss : 0.11853787302970886\n",
            "Batch 940 / 996 Sammon Loss : 0.4571523368358612\n",
            "Batch 950 / 996 Sammon Loss : 0.031093256548047066\n",
            "Batch 960 / 996 Sammon Loss : 0.01143194641917944\n",
            "Batch 970 / 996 Sammon Loss : 0.16954004764556885\n",
            "Batch 980 / 996 Sammon Loss : 0.05211907625198364\n",
            "Batch 990 / 996 Sammon Loss : 0.00949271209537983\n",
            "Batch 10 / 533 Regressor Loss : 145569.171875\n",
            "Batch 20 / 533 Regressor Loss : 118436.8984375\n",
            "Batch 30 / 533 Regressor Loss : 140718.5\n",
            "Batch 40 / 533 Regressor Loss : 148179.90625\n",
            "Batch 50 / 533 Regressor Loss : 135719.078125\n",
            "Batch 60 / 533 Regressor Loss : 127176.1484375\n",
            "Batch 70 / 533 Regressor Loss : 133542.640625\n",
            "Batch 80 / 533 Regressor Loss : 121754.109375\n",
            "Batch 90 / 533 Regressor Loss : 132557.59375\n",
            "Batch 100 / 533 Regressor Loss : 147287.59375\n",
            "Batch 110 / 533 Regressor Loss : 125540.421875\n",
            "Batch 120 / 533 Regressor Loss : 113813.2734375\n",
            "Batch 130 / 533 Regressor Loss : 143429.0625\n",
            "Batch 140 / 533 Regressor Loss : 115193.109375\n",
            "Batch 150 / 533 Regressor Loss : 115407.9921875\n",
            "Batch 160 / 533 Regressor Loss : 139510.359375\n",
            "Batch 170 / 533 Regressor Loss : 146728.921875\n",
            "Batch 180 / 533 Regressor Loss : 127997.0859375\n",
            "Batch 190 / 533 Regressor Loss : 149877.734375\n",
            "Batch 200 / 533 Regressor Loss : 144727.6875\n",
            "Batch 210 / 533 Regressor Loss : 114712.9921875\n",
            "Batch 220 / 533 Regressor Loss : 111278.0625\n",
            "Batch 230 / 533 Regressor Loss : 135902.03125\n",
            "Batch 240 / 533 Regressor Loss : 114044.234375\n",
            "Batch 250 / 533 Regressor Loss : 148082.453125\n",
            "Batch 260 / 533 Regressor Loss : 129205.734375\n",
            "Batch 270 / 533 Regressor Loss : 113781.8125\n",
            "Batch 280 / 533 Regressor Loss : 138276.109375\n",
            "Batch 290 / 533 Regressor Loss : 143210.0625\n",
            "Batch 300 / 533 Regressor Loss : 145356.125\n",
            "Batch 310 / 533 Regressor Loss : 141967.6875\n",
            "Batch 320 / 533 Regressor Loss : 141969.421875\n",
            "Batch 330 / 533 Regressor Loss : 179008.71875\n",
            "Batch 340 / 533 Regressor Loss : 112451.8359375\n",
            "Batch 350 / 533 Regressor Loss : 117616.015625\n",
            "Batch 360 / 533 Regressor Loss : 136696.140625\n",
            "Batch 370 / 533 Regressor Loss : 130242.296875\n",
            "Batch 380 / 533 Regressor Loss : 128069.8046875\n",
            "Batch 390 / 533 Regressor Loss : 134077.296875\n",
            "Batch 400 / 533 Regressor Loss : 116875.0\n",
            "Batch 410 / 533 Regressor Loss : 117508.625\n",
            "Batch 420 / 533 Regressor Loss : 157698.90625\n",
            "Batch 430 / 533 Regressor Loss : 111790.46875\n",
            "Batch 440 / 533 Regressor Loss : 142013.1875\n",
            "Batch 450 / 533 Regressor Loss : 128748.65625\n",
            "Batch 460 / 533 Regressor Loss : 124518.203125\n",
            "Batch 470 / 533 Regressor Loss : 134007.671875\n",
            "Batch 480 / 533 Regressor Loss : 135819.0\n",
            "Batch 490 / 533 Regressor Loss : 130816.125\n",
            "Batch 500 / 533 Regressor Loss : 139486.375\n",
            "Batch 510 / 533 Regressor Loss : 124158.0859375\n",
            "Batch 520 / 533 Regressor Loss : 144534.96875\n",
            "Batch 530 / 533 Regressor Loss : 133276.859375\n",
            "Epoch  6 / 100  Sammon Loss : 0.3046169877052307  Regressor Loss : 179072.296875\n",
            "Val Loss : 106892.671875\n",
            "Batch 10 / 996 Sammon Loss : 0.348817378282547\n",
            "Batch 20 / 996 Sammon Loss : 0.39586299657821655\n",
            "Batch 30 / 996 Sammon Loss : 0.5528454184532166\n",
            "Batch 40 / 996 Sammon Loss : 1.2294000387191772\n",
            "Batch 50 / 996 Sammon Loss : 0.9172654747962952\n",
            "Batch 60 / 996 Sammon Loss : 0.07883239537477493\n",
            "Batch 70 / 996 Sammon Loss : 0.12291055917739868\n",
            "Batch 80 / 996 Sammon Loss : 0.1425311416387558\n",
            "Batch 90 / 996 Sammon Loss : 0.04519304260611534\n",
            "Batch 100 / 996 Sammon Loss : 0.04039505496621132\n",
            "Batch 110 / 996 Sammon Loss : 0.018384914845228195\n",
            "Batch 120 / 996 Sammon Loss : 0.10209954530000687\n",
            "Batch 130 / 996 Sammon Loss : 0.004028654657304287\n",
            "Batch 140 / 996 Sammon Loss : 0.05743428319692612\n",
            "Batch 150 / 996 Sammon Loss : 0.0036336607299745083\n",
            "Batch 160 / 996 Sammon Loss : 0.016144920140504837\n",
            "Batch 170 / 996 Sammon Loss : 0.4859280288219452\n",
            "Batch 180 / 996 Sammon Loss : 0.25369012355804443\n",
            "Batch 190 / 996 Sammon Loss : 0.41471028327941895\n",
            "Batch 200 / 996 Sammon Loss : 0.21295586228370667\n",
            "Batch 210 / 996 Sammon Loss : 0.3082476854324341\n",
            "Batch 220 / 996 Sammon Loss : 0.09034281224012375\n",
            "Batch 230 / 996 Sammon Loss : 0.06170253828167915\n",
            "Batch 240 / 996 Sammon Loss : 0.11200668662786484\n",
            "Batch 250 / 996 Sammon Loss : 0.07851611822843552\n",
            "Batch 260 / 996 Sammon Loss : 0.012308330275118351\n",
            "Batch 270 / 996 Sammon Loss : 0.030966365709900856\n",
            "Batch 280 / 996 Sammon Loss : 0.02386641316115856\n",
            "Batch 290 / 996 Sammon Loss : 0.136982724070549\n",
            "Batch 300 / 996 Sammon Loss : 0.026798885315656662\n",
            "Batch 310 / 996 Sammon Loss : 0.055974334478378296\n",
            "Batch 320 / 996 Sammon Loss : 0.004273951053619385\n",
            "Batch 330 / 996 Sammon Loss : 0.06088273599743843\n",
            "Batch 340 / 996 Sammon Loss : 0.044775743037462234\n",
            "Batch 350 / 996 Sammon Loss : 0.006247549783438444\n",
            "Batch 360 / 996 Sammon Loss : 0.017748525366187096\n",
            "Batch 370 / 996 Sammon Loss : 0.11245974898338318\n",
            "Batch 380 / 996 Sammon Loss : 0.016434192657470703\n",
            "Batch 390 / 996 Sammon Loss : 0.08358185738325119\n",
            "Batch 400 / 996 Sammon Loss : 0.006586446426808834\n",
            "Batch 410 / 996 Sammon Loss : 0.3395689129829407\n",
            "Batch 420 / 996 Sammon Loss : 0.029742982238531113\n",
            "Batch 430 / 996 Sammon Loss : 0.11692529916763306\n",
            "Batch 440 / 996 Sammon Loss : 1.735817551612854\n",
            "Batch 450 / 996 Sammon Loss : 1.3682215213775635\n",
            "Batch 460 / 996 Sammon Loss : 0.6467912197113037\n",
            "Batch 470 / 996 Sammon Loss : 0.9904451966285706\n",
            "Batch 480 / 996 Sammon Loss : 0.017613964155316353\n",
            "Batch 490 / 996 Sammon Loss : 0.06334782391786575\n",
            "Batch 500 / 996 Sammon Loss : 0.07935891300439835\n",
            "Batch 510 / 996 Sammon Loss : 0.08470287919044495\n",
            "Batch 520 / 996 Sammon Loss : 0.23523087799549103\n",
            "Batch 530 / 996 Sammon Loss : 0.05122263729572296\n",
            "Batch 540 / 996 Sammon Loss : 0.021481750532984734\n",
            "Batch 550 / 996 Sammon Loss : 0.05017951503396034\n",
            "Batch 560 / 996 Sammon Loss : 0.48329320549964905\n",
            "Batch 570 / 996 Sammon Loss : 0.1966220587491989\n",
            "Batch 580 / 996 Sammon Loss : 0.12861011922359467\n",
            "Batch 590 / 996 Sammon Loss : 0.11202364414930344\n",
            "Batch 600 / 996 Sammon Loss : 0.009871350601315498\n",
            "Batch 610 / 996 Sammon Loss : 0.010869883000850677\n",
            "Batch 620 / 996 Sammon Loss : 0.0026678089052438736\n",
            "Batch 630 / 996 Sammon Loss : 0.048140350729227066\n",
            "Batch 640 / 996 Sammon Loss : 0.03548266738653183\n",
            "Batch 650 / 996 Sammon Loss : 0.039931777864694595\n",
            "Batch 660 / 996 Sammon Loss : 0.012310219928622246\n",
            "Batch 670 / 996 Sammon Loss : 0.008603288792073727\n",
            "Batch 680 / 996 Sammon Loss : 0.0358506441116333\n",
            "Batch 690 / 996 Sammon Loss : 0.1296980232000351\n",
            "Batch 700 / 996 Sammon Loss : 0.46019411087036133\n",
            "Batch 710 / 996 Sammon Loss : 0.057425301522016525\n",
            "Batch 720 / 996 Sammon Loss : 0.03581381216645241\n",
            "Batch 730 / 996 Sammon Loss : 0.012726493179798126\n",
            "Batch 740 / 996 Sammon Loss : 0.04381942003965378\n",
            "Batch 750 / 996 Sammon Loss : 0.2641753554344177\n",
            "Batch 760 / 996 Sammon Loss : 0.6067297458648682\n",
            "Batch 770 / 996 Sammon Loss : 0.5238189101219177\n",
            "Batch 780 / 996 Sammon Loss : 0.08815564960241318\n",
            "Batch 790 / 996 Sammon Loss : 0.0663556158542633\n",
            "Batch 800 / 996 Sammon Loss : 0.040857598185539246\n",
            "Batch 810 / 996 Sammon Loss : 0.09558994323015213\n",
            "Batch 820 / 996 Sammon Loss : 0.03156011551618576\n",
            "Batch 830 / 996 Sammon Loss : 0.07995663583278656\n",
            "Batch 840 / 996 Sammon Loss : 0.08731354773044586\n",
            "Batch 850 / 996 Sammon Loss : 0.07282628864049911\n",
            "Batch 860 / 996 Sammon Loss : 0.0019145015394315124\n",
            "Batch 870 / 996 Sammon Loss : 0.0565665028989315\n",
            "Batch 880 / 996 Sammon Loss : 0.01355814840644598\n",
            "Batch 890 / 996 Sammon Loss : 0.01601545698940754\n",
            "Batch 900 / 996 Sammon Loss : 0.01964319497346878\n",
            "Batch 910 / 996 Sammon Loss : 0.5307192206382751\n",
            "Batch 920 / 996 Sammon Loss : 0.08494897186756134\n",
            "Batch 930 / 996 Sammon Loss : 0.11853787302970886\n",
            "Batch 940 / 996 Sammon Loss : 0.4571523368358612\n",
            "Batch 950 / 996 Sammon Loss : 0.031093256548047066\n",
            "Batch 960 / 996 Sammon Loss : 0.01143194641917944\n",
            "Batch 970 / 996 Sammon Loss : 0.16954004764556885\n",
            "Batch 980 / 996 Sammon Loss : 0.05211907625198364\n",
            "Batch 990 / 996 Sammon Loss : 0.00949271209537983\n",
            "Batch 10 / 533 Regressor Loss : 145295.8125\n",
            "Batch 20 / 533 Regressor Loss : 118172.6875\n",
            "Batch 30 / 533 Regressor Loss : 140433.78125\n",
            "Batch 40 / 533 Regressor Loss : 147898.625\n",
            "Batch 50 / 533 Regressor Loss : 135459.96875\n",
            "Batch 60 / 533 Regressor Loss : 126899.296875\n",
            "Batch 70 / 533 Regressor Loss : 133251.625\n",
            "Batch 80 / 533 Regressor Loss : 121488.15625\n",
            "Batch 90 / 533 Regressor Loss : 132286.0625\n",
            "Batch 100 / 533 Regressor Loss : 146982.140625\n",
            "Batch 110 / 533 Regressor Loss : 125288.578125\n",
            "Batch 120 / 533 Regressor Loss : 113571.640625\n",
            "Batch 130 / 533 Regressor Loss : 143133.4375\n",
            "Batch 140 / 533 Regressor Loss : 114939.09375\n",
            "Batch 150 / 533 Regressor Loss : 115146.4375\n",
            "Batch 160 / 533 Regressor Loss : 139253.9375\n",
            "Batch 170 / 533 Regressor Loss : 146447.28125\n",
            "Batch 180 / 533 Regressor Loss : 127722.0234375\n",
            "Batch 190 / 533 Regressor Loss : 149604.21875\n",
            "Batch 200 / 533 Regressor Loss : 144450.1875\n",
            "Batch 210 / 533 Regressor Loss : 114457.984375\n",
            "Batch 220 / 533 Regressor Loss : 111026.875\n",
            "Batch 230 / 533 Regressor Loss : 135649.984375\n",
            "Batch 240 / 533 Regressor Loss : 113783.0625\n",
            "Batch 250 / 533 Regressor Loss : 147776.5\n",
            "Batch 260 / 533 Regressor Loss : 128927.609375\n",
            "Batch 270 / 533 Regressor Loss : 113516.7109375\n",
            "Batch 280 / 533 Regressor Loss : 137987.484375\n",
            "Batch 290 / 533 Regressor Loss : 142913.15625\n",
            "Batch 300 / 533 Regressor Loss : 145097.78125\n",
            "Batch 310 / 533 Regressor Loss : 141685.59375\n",
            "Batch 320 / 533 Regressor Loss : 141683.0625\n",
            "Batch 330 / 533 Regressor Loss : 178722.09375\n",
            "Batch 340 / 533 Regressor Loss : 112192.5234375\n",
            "Batch 350 / 533 Regressor Loss : 117347.9921875\n",
            "Batch 360 / 533 Regressor Loss : 136417.96875\n",
            "Batch 370 / 533 Regressor Loss : 129979.3359375\n",
            "Batch 380 / 533 Regressor Loss : 127804.3046875\n",
            "Batch 390 / 533 Regressor Loss : 133804.21875\n",
            "Batch 400 / 533 Regressor Loss : 116616.875\n",
            "Batch 410 / 533 Regressor Loss : 117251.546875\n",
            "Batch 420 / 533 Regressor Loss : 157410.890625\n",
            "Batch 430 / 533 Regressor Loss : 111542.234375\n",
            "Batch 440 / 533 Regressor Loss : 141727.46875\n",
            "Batch 450 / 533 Regressor Loss : 128474.34375\n",
            "Batch 460 / 533 Regressor Loss : 124273.5625\n",
            "Batch 470 / 533 Regressor Loss : 133731.78125\n",
            "Batch 480 / 533 Regressor Loss : 135525.484375\n",
            "Batch 490 / 533 Regressor Loss : 130543.484375\n",
            "Batch 500 / 533 Regressor Loss : 139216.40625\n",
            "Batch 510 / 533 Regressor Loss : 123905.046875\n",
            "Batch 520 / 533 Regressor Loss : 144236.71875\n",
            "Batch 530 / 533 Regressor Loss : 132999.96875\n",
            "Epoch  7 / 100  Sammon Loss : 0.3046169877052307  Regressor Loss : 178734.171875\n",
            "Val Loss : 106621.59375\n",
            "Batch 10 / 996 Sammon Loss : 0.348817378282547\n",
            "Batch 20 / 996 Sammon Loss : 0.39586299657821655\n",
            "Batch 30 / 996 Sammon Loss : 0.5528454184532166\n",
            "Batch 40 / 996 Sammon Loss : 1.2294000387191772\n",
            "Batch 50 / 996 Sammon Loss : 0.9172654747962952\n",
            "Batch 60 / 996 Sammon Loss : 0.07883239537477493\n",
            "Batch 70 / 996 Sammon Loss : 0.12291055917739868\n",
            "Batch 80 / 996 Sammon Loss : 0.1425311416387558\n",
            "Batch 90 / 996 Sammon Loss : 0.04519304260611534\n",
            "Batch 100 / 996 Sammon Loss : 0.04039505496621132\n",
            "Batch 110 / 996 Sammon Loss : 0.018384914845228195\n",
            "Batch 120 / 996 Sammon Loss : 0.10209954530000687\n",
            "Batch 130 / 996 Sammon Loss : 0.004028654657304287\n",
            "Batch 140 / 996 Sammon Loss : 0.05743428319692612\n",
            "Batch 150 / 996 Sammon Loss : 0.0036336607299745083\n",
            "Batch 160 / 996 Sammon Loss : 0.016144920140504837\n",
            "Batch 170 / 996 Sammon Loss : 0.4859280288219452\n",
            "Batch 180 / 996 Sammon Loss : 0.25369012355804443\n",
            "Batch 190 / 996 Sammon Loss : 0.41471028327941895\n",
            "Batch 200 / 996 Sammon Loss : 0.21295586228370667\n",
            "Batch 210 / 996 Sammon Loss : 0.3082476854324341\n",
            "Batch 220 / 996 Sammon Loss : 0.09034281224012375\n",
            "Batch 230 / 996 Sammon Loss : 0.06170253828167915\n",
            "Batch 240 / 996 Sammon Loss : 0.11200668662786484\n",
            "Batch 250 / 996 Sammon Loss : 0.07851611822843552\n",
            "Batch 260 / 996 Sammon Loss : 0.012308330275118351\n",
            "Batch 270 / 996 Sammon Loss : 0.030966365709900856\n",
            "Batch 280 / 996 Sammon Loss : 0.02386641316115856\n",
            "Batch 290 / 996 Sammon Loss : 0.136982724070549\n",
            "Batch 300 / 996 Sammon Loss : 0.026798885315656662\n",
            "Batch 310 / 996 Sammon Loss : 0.055974334478378296\n",
            "Batch 320 / 996 Sammon Loss : 0.004273951053619385\n",
            "Batch 330 / 996 Sammon Loss : 0.06088273599743843\n",
            "Batch 340 / 996 Sammon Loss : 0.044775743037462234\n",
            "Batch 350 / 996 Sammon Loss : 0.006247549783438444\n",
            "Batch 360 / 996 Sammon Loss : 0.017748525366187096\n",
            "Batch 370 / 996 Sammon Loss : 0.11245974898338318\n",
            "Batch 380 / 996 Sammon Loss : 0.016434192657470703\n",
            "Batch 390 / 996 Sammon Loss : 0.08358185738325119\n",
            "Batch 400 / 996 Sammon Loss : 0.006586446426808834\n",
            "Batch 410 / 996 Sammon Loss : 0.3395689129829407\n",
            "Batch 420 / 996 Sammon Loss : 0.029742982238531113\n",
            "Batch 430 / 996 Sammon Loss : 0.11692529916763306\n",
            "Batch 440 / 996 Sammon Loss : 1.735817551612854\n",
            "Batch 450 / 996 Sammon Loss : 1.3682215213775635\n",
            "Batch 460 / 996 Sammon Loss : 0.6467912197113037\n",
            "Batch 470 / 996 Sammon Loss : 0.9904451966285706\n",
            "Batch 480 / 996 Sammon Loss : 0.017613964155316353\n",
            "Batch 490 / 996 Sammon Loss : 0.06334782391786575\n",
            "Batch 500 / 996 Sammon Loss : 0.07935891300439835\n",
            "Batch 510 / 996 Sammon Loss : 0.08470287919044495\n",
            "Batch 520 / 996 Sammon Loss : 0.23523087799549103\n",
            "Batch 530 / 996 Sammon Loss : 0.05122263729572296\n",
            "Batch 540 / 996 Sammon Loss : 0.021481750532984734\n",
            "Batch 550 / 996 Sammon Loss : 0.05017951503396034\n",
            "Batch 560 / 996 Sammon Loss : 0.48329320549964905\n",
            "Batch 570 / 996 Sammon Loss : 0.1966220587491989\n",
            "Batch 580 / 996 Sammon Loss : 0.12861011922359467\n",
            "Batch 590 / 996 Sammon Loss : 0.11202364414930344\n",
            "Batch 600 / 996 Sammon Loss : 0.009871350601315498\n",
            "Batch 610 / 996 Sammon Loss : 0.010869883000850677\n",
            "Batch 620 / 996 Sammon Loss : 0.0026678089052438736\n",
            "Batch 630 / 996 Sammon Loss : 0.048140350729227066\n",
            "Batch 640 / 996 Sammon Loss : 0.03548266738653183\n",
            "Batch 650 / 996 Sammon Loss : 0.039931777864694595\n",
            "Batch 660 / 996 Sammon Loss : 0.012310219928622246\n",
            "Batch 670 / 996 Sammon Loss : 0.008603288792073727\n",
            "Batch 680 / 996 Sammon Loss : 0.0358506441116333\n",
            "Batch 690 / 996 Sammon Loss : 0.1296980232000351\n",
            "Batch 700 / 996 Sammon Loss : 0.46019411087036133\n",
            "Batch 710 / 996 Sammon Loss : 0.057425301522016525\n",
            "Batch 720 / 996 Sammon Loss : 0.03581381216645241\n",
            "Batch 730 / 996 Sammon Loss : 0.012726493179798126\n",
            "Batch 740 / 996 Sammon Loss : 0.04381942003965378\n",
            "Batch 750 / 996 Sammon Loss : 0.2641753554344177\n",
            "Batch 760 / 996 Sammon Loss : 0.6067297458648682\n",
            "Batch 770 / 996 Sammon Loss : 0.5238189101219177\n",
            "Batch 780 / 996 Sammon Loss : 0.08815564960241318\n",
            "Batch 790 / 996 Sammon Loss : 0.0663556158542633\n",
            "Batch 800 / 996 Sammon Loss : 0.040857598185539246\n",
            "Batch 810 / 996 Sammon Loss : 0.09558994323015213\n",
            "Batch 820 / 996 Sammon Loss : 0.03156011551618576\n",
            "Batch 830 / 996 Sammon Loss : 0.07995663583278656\n",
            "Batch 840 / 996 Sammon Loss : 0.08731354773044586\n",
            "Batch 850 / 996 Sammon Loss : 0.07282628864049911\n",
            "Batch 860 / 996 Sammon Loss : 0.0019145015394315124\n",
            "Batch 870 / 996 Sammon Loss : 0.0565665028989315\n",
            "Batch 880 / 996 Sammon Loss : 0.01355814840644598\n",
            "Batch 890 / 996 Sammon Loss : 0.01601545698940754\n",
            "Batch 900 / 996 Sammon Loss : 0.01964319497346878\n",
            "Batch 910 / 996 Sammon Loss : 0.5307192206382751\n",
            "Batch 920 / 996 Sammon Loss : 0.08494897186756134\n",
            "Batch 930 / 996 Sammon Loss : 0.11853787302970886\n",
            "Batch 940 / 996 Sammon Loss : 0.4571523368358612\n",
            "Batch 950 / 996 Sammon Loss : 0.031093256548047066\n",
            "Batch 960 / 996 Sammon Loss : 0.01143194641917944\n",
            "Batch 970 / 996 Sammon Loss : 0.16954004764556885\n",
            "Batch 980 / 996 Sammon Loss : 0.05211907625198364\n",
            "Batch 990 / 996 Sammon Loss : 0.00949271209537983\n",
            "Batch 10 / 533 Regressor Loss : 145024.625\n",
            "Batch 20 / 533 Regressor Loss : 117910.578125\n",
            "Batch 30 / 533 Regressor Loss : 140151.234375\n",
            "Batch 40 / 533 Regressor Loss : 147619.5\n",
            "Batch 50 / 533 Regressor Loss : 135202.859375\n",
            "Batch 60 / 533 Regressor Loss : 126624.5625\n",
            "Batch 70 / 533 Regressor Loss : 132962.8125\n",
            "Batch 80 / 533 Regressor Loss : 121224.25\n",
            "Batch 90 / 533 Regressor Loss : 132016.59375\n",
            "Batch 100 / 533 Regressor Loss : 146678.90625\n",
            "Batch 110 / 533 Regressor Loss : 125038.625\n",
            "Batch 120 / 533 Regressor Loss : 113331.8359375\n",
            "Batch 130 / 533 Regressor Loss : 142839.953125\n",
            "Batch 140 / 533 Regressor Loss : 114687.0\n",
            "Batch 150 / 533 Regressor Loss : 114886.8125\n",
            "Batch 160 / 533 Regressor Loss : 138999.390625\n",
            "Batch 170 / 533 Regressor Loss : 146167.625\n",
            "Batch 180 / 533 Regressor Loss : 127448.921875\n",
            "Batch 190 / 533 Regressor Loss : 149332.59375\n",
            "Batch 200 / 533 Regressor Loss : 144174.59375\n",
            "Batch 210 / 533 Regressor Loss : 114204.796875\n",
            "Batch 220 / 533 Regressor Loss : 110777.453125\n",
            "Batch 230 / 533 Regressor Loss : 135399.6875\n",
            "Batch 240 / 533 Regressor Loss : 113523.6875\n",
            "Batch 250 / 533 Regressor Loss : 147472.5625\n",
            "Batch 260 / 533 Regressor Loss : 128651.328125\n",
            "Batch 270 / 533 Regressor Loss : 113253.3671875\n",
            "Batch 280 / 533 Regressor Loss : 137700.75\n",
            "Batch 290 / 533 Regressor Loss : 142618.171875\n",
            "Batch 300 / 533 Regressor Loss : 144841.125\n",
            "Batch 310 / 533 Regressor Loss : 141405.296875\n",
            "Batch 320 / 533 Regressor Loss : 141398.5\n",
            "Batch 330 / 533 Regressor Loss : 178437.25\n",
            "Batch 340 / 533 Regressor Loss : 111934.8671875\n",
            "Batch 350 / 533 Regressor Loss : 117081.6796875\n",
            "Batch 360 / 533 Regressor Loss : 136141.53125\n",
            "Batch 370 / 533 Regressor Loss : 129718.0\n",
            "Batch 380 / 533 Regressor Loss : 127540.484375\n",
            "Batch 390 / 533 Regressor Loss : 133532.796875\n",
            "Batch 400 / 533 Regressor Loss : 116360.359375\n",
            "Batch 410 / 533 Regressor Loss : 116996.0625\n",
            "Batch 420 / 533 Regressor Loss : 157124.59375\n",
            "Batch 430 / 533 Regressor Loss : 111295.53125\n",
            "Batch 440 / 533 Regressor Loss : 141443.46875\n",
            "Batch 450 / 533 Regressor Loss : 128201.671875\n",
            "Batch 460 / 533 Regressor Loss : 124030.4296875\n",
            "Batch 470 / 533 Regressor Loss : 133457.5\n",
            "Batch 480 / 533 Regressor Loss : 135233.671875\n",
            "Batch 490 / 533 Regressor Loss : 130272.4375\n",
            "Batch 500 / 533 Regressor Loss : 138948.0\n",
            "Batch 510 / 533 Regressor Loss : 123653.4921875\n",
            "Batch 520 / 533 Regressor Loss : 143940.140625\n",
            "Batch 530 / 533 Regressor Loss : 132724.625\n",
            "Epoch  8 / 100  Sammon Loss : 0.3046169877052307  Regressor Loss : 178397.875\n",
            "Val Loss : 106352.078125\n",
            "Batch 10 / 996 Sammon Loss : 0.348817378282547\n",
            "Batch 20 / 996 Sammon Loss : 0.39586299657821655\n",
            "Batch 30 / 996 Sammon Loss : 0.5528454184532166\n",
            "Batch 40 / 996 Sammon Loss : 1.2294000387191772\n",
            "Batch 50 / 996 Sammon Loss : 0.9172654747962952\n",
            "Batch 60 / 996 Sammon Loss : 0.07883239537477493\n",
            "Batch 70 / 996 Sammon Loss : 0.12291055917739868\n",
            "Batch 80 / 996 Sammon Loss : 0.1425311416387558\n",
            "Batch 90 / 996 Sammon Loss : 0.04519304260611534\n",
            "Batch 100 / 996 Sammon Loss : 0.04039505496621132\n",
            "Batch 110 / 996 Sammon Loss : 0.018384914845228195\n",
            "Batch 120 / 996 Sammon Loss : 0.10209954530000687\n",
            "Batch 130 / 996 Sammon Loss : 0.004028654657304287\n",
            "Batch 140 / 996 Sammon Loss : 0.05743428319692612\n",
            "Batch 150 / 996 Sammon Loss : 0.0036336607299745083\n",
            "Batch 160 / 996 Sammon Loss : 0.016144920140504837\n",
            "Batch 170 / 996 Sammon Loss : 0.4859280288219452\n",
            "Batch 180 / 996 Sammon Loss : 0.25369012355804443\n",
            "Batch 190 / 996 Sammon Loss : 0.41471028327941895\n",
            "Batch 200 / 996 Sammon Loss : 0.21295586228370667\n",
            "Batch 210 / 996 Sammon Loss : 0.3082476854324341\n",
            "Batch 220 / 996 Sammon Loss : 0.09034281224012375\n",
            "Batch 230 / 996 Sammon Loss : 0.06170253828167915\n",
            "Batch 240 / 996 Sammon Loss : 0.11200668662786484\n",
            "Batch 250 / 996 Sammon Loss : 0.07851611822843552\n",
            "Batch 260 / 996 Sammon Loss : 0.012308330275118351\n",
            "Batch 270 / 996 Sammon Loss : 0.030966365709900856\n",
            "Batch 280 / 996 Sammon Loss : 0.02386641316115856\n",
            "Batch 290 / 996 Sammon Loss : 0.136982724070549\n",
            "Batch 300 / 996 Sammon Loss : 0.026798885315656662\n",
            "Batch 310 / 996 Sammon Loss : 0.055974334478378296\n",
            "Batch 320 / 996 Sammon Loss : 0.004273951053619385\n",
            "Batch 330 / 996 Sammon Loss : 0.06088273599743843\n",
            "Batch 340 / 996 Sammon Loss : 0.044775743037462234\n",
            "Batch 350 / 996 Sammon Loss : 0.006247549783438444\n",
            "Batch 360 / 996 Sammon Loss : 0.017748525366187096\n",
            "Batch 370 / 996 Sammon Loss : 0.11245974898338318\n",
            "Batch 380 / 996 Sammon Loss : 0.016434192657470703\n",
            "Batch 390 / 996 Sammon Loss : 0.08358185738325119\n",
            "Batch 400 / 996 Sammon Loss : 0.006586446426808834\n",
            "Batch 410 / 996 Sammon Loss : 0.3395689129829407\n",
            "Batch 420 / 996 Sammon Loss : 0.029742982238531113\n",
            "Batch 430 / 996 Sammon Loss : 0.11692529916763306\n",
            "Batch 440 / 996 Sammon Loss : 1.735817551612854\n",
            "Batch 450 / 996 Sammon Loss : 1.3682215213775635\n",
            "Batch 460 / 996 Sammon Loss : 0.6467912197113037\n",
            "Batch 470 / 996 Sammon Loss : 0.9904451966285706\n",
            "Batch 480 / 996 Sammon Loss : 0.017613964155316353\n",
            "Batch 490 / 996 Sammon Loss : 0.06334782391786575\n",
            "Batch 500 / 996 Sammon Loss : 0.07935891300439835\n",
            "Batch 510 / 996 Sammon Loss : 0.08470287919044495\n",
            "Batch 520 / 996 Sammon Loss : 0.23523087799549103\n",
            "Batch 530 / 996 Sammon Loss : 0.05122263729572296\n",
            "Batch 540 / 996 Sammon Loss : 0.021481750532984734\n",
            "Batch 550 / 996 Sammon Loss : 0.05017951503396034\n",
            "Batch 560 / 996 Sammon Loss : 0.48329320549964905\n",
            "Batch 570 / 996 Sammon Loss : 0.1966220587491989\n",
            "Batch 580 / 996 Sammon Loss : 0.12861011922359467\n",
            "Batch 590 / 996 Sammon Loss : 0.11202364414930344\n",
            "Batch 600 / 996 Sammon Loss : 0.009871350601315498\n",
            "Batch 610 / 996 Sammon Loss : 0.010869883000850677\n",
            "Batch 620 / 996 Sammon Loss : 0.0026678089052438736\n",
            "Batch 630 / 996 Sammon Loss : 0.048140350729227066\n",
            "Batch 640 / 996 Sammon Loss : 0.03548266738653183\n",
            "Batch 650 / 996 Sammon Loss : 0.039931777864694595\n",
            "Batch 660 / 996 Sammon Loss : 0.012310219928622246\n",
            "Batch 670 / 996 Sammon Loss : 0.008603288792073727\n",
            "Batch 680 / 996 Sammon Loss : 0.0358506441116333\n",
            "Batch 690 / 996 Sammon Loss : 0.1296980232000351\n",
            "Batch 700 / 996 Sammon Loss : 0.46019411087036133\n",
            "Batch 710 / 996 Sammon Loss : 0.057425301522016525\n",
            "Batch 720 / 996 Sammon Loss : 0.03581381216645241\n",
            "Batch 730 / 996 Sammon Loss : 0.012726493179798126\n",
            "Batch 740 / 996 Sammon Loss : 0.04381942003965378\n",
            "Batch 750 / 996 Sammon Loss : 0.2641753554344177\n",
            "Batch 760 / 996 Sammon Loss : 0.6067297458648682\n",
            "Batch 770 / 996 Sammon Loss : 0.5238189101219177\n",
            "Batch 780 / 996 Sammon Loss : 0.08815564960241318\n",
            "Batch 790 / 996 Sammon Loss : 0.0663556158542633\n",
            "Batch 800 / 996 Sammon Loss : 0.040857598185539246\n",
            "Batch 810 / 996 Sammon Loss : 0.09558994323015213\n",
            "Batch 820 / 996 Sammon Loss : 0.03156011551618576\n",
            "Batch 830 / 996 Sammon Loss : 0.07995663583278656\n",
            "Batch 840 / 996 Sammon Loss : 0.08731354773044586\n",
            "Batch 850 / 996 Sammon Loss : 0.07282628864049911\n",
            "Batch 860 / 996 Sammon Loss : 0.0019145015394315124\n",
            "Batch 870 / 996 Sammon Loss : 0.0565665028989315\n",
            "Batch 880 / 996 Sammon Loss : 0.01355814840644598\n",
            "Batch 890 / 996 Sammon Loss : 0.01601545698940754\n",
            "Batch 900 / 996 Sammon Loss : 0.01964319497346878\n",
            "Batch 910 / 996 Sammon Loss : 0.5307192206382751\n",
            "Batch 920 / 996 Sammon Loss : 0.08494897186756134\n",
            "Batch 930 / 996 Sammon Loss : 0.11853787302970886\n",
            "Batch 940 / 996 Sammon Loss : 0.4571523368358612\n",
            "Batch 950 / 996 Sammon Loss : 0.031093256548047066\n",
            "Batch 960 / 996 Sammon Loss : 0.01143194641917944\n",
            "Batch 970 / 996 Sammon Loss : 0.16954004764556885\n",
            "Batch 980 / 996 Sammon Loss : 0.05211907625198364\n",
            "Batch 990 / 996 Sammon Loss : 0.00949271209537983\n",
            "Batch 10 / 533 Regressor Loss : 144754.984375\n",
            "Batch 20 / 533 Regressor Loss : 117649.9375\n",
            "Batch 30 / 533 Regressor Loss : 139870.25\n",
            "Batch 40 / 533 Regressor Loss : 147341.90625\n",
            "Batch 50 / 533 Regressor Loss : 134947.1875\n",
            "Batch 60 / 533 Regressor Loss : 126351.34375\n",
            "Batch 70 / 533 Regressor Loss : 132675.53125\n",
            "Batch 80 / 533 Regressor Loss : 120961.7734375\n",
            "Batch 90 / 533 Regressor Loss : 131748.546875\n",
            "Batch 100 / 533 Regressor Loss : 146377.25\n",
            "Batch 110 / 533 Regressor Loss : 124790.046875\n",
            "Batch 120 / 533 Regressor Loss : 113093.359375\n",
            "Batch 130 / 533 Regressor Loss : 142548.0\n",
            "Batch 140 / 533 Regressor Loss : 114436.265625\n",
            "Batch 150 / 533 Regressor Loss : 114628.5625\n",
            "Batch 160 / 533 Regressor Loss : 138746.171875\n",
            "Batch 170 / 533 Regressor Loss : 145889.40625\n",
            "Batch 180 / 533 Regressor Loss : 127177.203125\n",
            "Batch 190 / 533 Regressor Loss : 149062.34375\n",
            "Batch 200 / 533 Regressor Loss : 143900.375\n",
            "Batch 210 / 533 Regressor Loss : 113952.8984375\n",
            "Batch 220 / 533 Regressor Loss : 110529.3125\n",
            "Batch 230 / 533 Regressor Loss : 135150.6875\n",
            "Batch 240 / 533 Regressor Loss : 113265.6484375\n",
            "Batch 250 / 533 Regressor Loss : 147170.078125\n",
            "Batch 260 / 533 Regressor Loss : 128376.390625\n",
            "Batch 270 / 533 Regressor Loss : 112991.3359375\n",
            "Batch 280 / 533 Regressor Loss : 137415.375\n",
            "Batch 290 / 533 Regressor Loss : 142324.5625\n",
            "Batch 300 / 533 Regressor Loss : 144585.71875\n",
            "Batch 310 / 533 Regressor Loss : 141126.328125\n",
            "Batch 320 / 533 Regressor Loss : 141115.28125\n",
            "Batch 330 / 533 Regressor Loss : 178153.71875\n",
            "Batch 340 / 533 Regressor Loss : 111678.453125\n",
            "Batch 350 / 533 Regressor Loss : 116816.640625\n",
            "Batch 360 / 533 Regressor Loss : 135866.390625\n",
            "Batch 370 / 533 Regressor Loss : 129457.921875\n",
            "Batch 380 / 533 Regressor Loss : 127277.890625\n",
            "Batch 390 / 533 Regressor Loss : 133262.65625\n",
            "Batch 400 / 533 Regressor Loss : 116105.0234375\n",
            "Batch 410 / 533 Regressor Loss : 116741.75\n",
            "Batch 420 / 533 Regressor Loss : 156839.53125\n",
            "Batch 430 / 533 Regressor Loss : 111049.984375\n",
            "Batch 440 / 533 Regressor Loss : 141160.71875\n",
            "Batch 450 / 533 Regressor Loss : 127930.234375\n",
            "Batch 460 / 533 Regressor Loss : 123788.375\n",
            "Batch 470 / 533 Regressor Loss : 133184.4375\n",
            "Batch 480 / 533 Regressor Loss : 134943.09375\n",
            "Batch 490 / 533 Regressor Loss : 130002.5859375\n",
            "Batch 500 / 533 Regressor Loss : 138680.75\n",
            "Batch 510 / 533 Regressor Loss : 123403.0625\n",
            "Batch 520 / 533 Regressor Loss : 143644.78125\n",
            "Batch 530 / 533 Regressor Loss : 132450.46875\n",
            "Epoch  9 / 100  Sammon Loss : 0.3046169877052307  Regressor Loss : 178062.96875\n",
            "Val Loss : 106083.703125\n",
            "Batch 10 / 996 Sammon Loss : 0.348817378282547\n",
            "Batch 20 / 996 Sammon Loss : 0.39586299657821655\n",
            "Batch 30 / 996 Sammon Loss : 0.5528454184532166\n",
            "Batch 40 / 996 Sammon Loss : 1.2294000387191772\n",
            "Batch 50 / 996 Sammon Loss : 0.9172654747962952\n",
            "Batch 60 / 996 Sammon Loss : 0.07883239537477493\n",
            "Batch 70 / 996 Sammon Loss : 0.12291055917739868\n",
            "Batch 80 / 996 Sammon Loss : 0.1425311416387558\n",
            "Batch 90 / 996 Sammon Loss : 0.04519304260611534\n",
            "Batch 100 / 996 Sammon Loss : 0.04039505496621132\n",
            "Batch 110 / 996 Sammon Loss : 0.018384914845228195\n",
            "Batch 120 / 996 Sammon Loss : 0.10209954530000687\n",
            "Batch 130 / 996 Sammon Loss : 0.004028654657304287\n",
            "Batch 140 / 996 Sammon Loss : 0.05743428319692612\n",
            "Batch 150 / 996 Sammon Loss : 0.0036336607299745083\n",
            "Batch 160 / 996 Sammon Loss : 0.016144920140504837\n",
            "Batch 170 / 996 Sammon Loss : 0.4859280288219452\n",
            "Batch 180 / 996 Sammon Loss : 0.25369012355804443\n",
            "Batch 190 / 996 Sammon Loss : 0.41471028327941895\n",
            "Batch 200 / 996 Sammon Loss : 0.21295586228370667\n",
            "Batch 210 / 996 Sammon Loss : 0.3082476854324341\n",
            "Batch 220 / 996 Sammon Loss : 0.09034281224012375\n",
            "Batch 230 / 996 Sammon Loss : 0.06170253828167915\n",
            "Batch 240 / 996 Sammon Loss : 0.11200668662786484\n",
            "Batch 250 / 996 Sammon Loss : 0.07851611822843552\n",
            "Batch 260 / 996 Sammon Loss : 0.012308330275118351\n",
            "Batch 270 / 996 Sammon Loss : 0.030966365709900856\n",
            "Batch 280 / 996 Sammon Loss : 0.02386641316115856\n",
            "Batch 290 / 996 Sammon Loss : 0.136982724070549\n",
            "Batch 300 / 996 Sammon Loss : 0.026798885315656662\n",
            "Batch 310 / 996 Sammon Loss : 0.055974334478378296\n",
            "Batch 320 / 996 Sammon Loss : 0.004273951053619385\n",
            "Batch 330 / 996 Sammon Loss : 0.06088273599743843\n",
            "Batch 340 / 996 Sammon Loss : 0.044775743037462234\n",
            "Batch 350 / 996 Sammon Loss : 0.006247549783438444\n",
            "Batch 360 / 996 Sammon Loss : 0.017748525366187096\n",
            "Batch 370 / 996 Sammon Loss : 0.11245974898338318\n",
            "Batch 380 / 996 Sammon Loss : 0.016434192657470703\n",
            "Batch 390 / 996 Sammon Loss : 0.08358185738325119\n",
            "Batch 400 / 996 Sammon Loss : 0.006586446426808834\n",
            "Batch 410 / 996 Sammon Loss : 0.3395689129829407\n",
            "Batch 420 / 996 Sammon Loss : 0.029742982238531113\n",
            "Batch 430 / 996 Sammon Loss : 0.11692529916763306\n",
            "Batch 440 / 996 Sammon Loss : 1.735817551612854\n",
            "Batch 450 / 996 Sammon Loss : 1.3682215213775635\n",
            "Batch 460 / 996 Sammon Loss : 0.6467912197113037\n",
            "Batch 470 / 996 Sammon Loss : 0.9904451966285706\n",
            "Batch 480 / 996 Sammon Loss : 0.017613964155316353\n",
            "Batch 490 / 996 Sammon Loss : 0.06334782391786575\n",
            "Batch 500 / 996 Sammon Loss : 0.07935891300439835\n",
            "Batch 510 / 996 Sammon Loss : 0.08470287919044495\n",
            "Batch 520 / 996 Sammon Loss : 0.23523087799549103\n",
            "Batch 530 / 996 Sammon Loss : 0.05122263729572296\n",
            "Batch 540 / 996 Sammon Loss : 0.021481750532984734\n",
            "Batch 550 / 996 Sammon Loss : 0.05017951503396034\n",
            "Batch 560 / 996 Sammon Loss : 0.48329320549964905\n",
            "Batch 570 / 996 Sammon Loss : 0.1966220587491989\n",
            "Batch 580 / 996 Sammon Loss : 0.12861011922359467\n",
            "Batch 590 / 996 Sammon Loss : 0.11202364414930344\n",
            "Batch 600 / 996 Sammon Loss : 0.009871350601315498\n",
            "Batch 610 / 996 Sammon Loss : 0.010869883000850677\n",
            "Batch 620 / 996 Sammon Loss : 0.0026678089052438736\n",
            "Batch 630 / 996 Sammon Loss : 0.048140350729227066\n",
            "Batch 640 / 996 Sammon Loss : 0.03548266738653183\n",
            "Batch 650 / 996 Sammon Loss : 0.039931777864694595\n",
            "Batch 660 / 996 Sammon Loss : 0.012310219928622246\n",
            "Batch 670 / 996 Sammon Loss : 0.008603288792073727\n",
            "Batch 680 / 996 Sammon Loss : 0.0358506441116333\n",
            "Batch 690 / 996 Sammon Loss : 0.1296980232000351\n",
            "Batch 700 / 996 Sammon Loss : 0.46019411087036133\n",
            "Batch 710 / 996 Sammon Loss : 0.057425301522016525\n",
            "Batch 720 / 996 Sammon Loss : 0.03581381216645241\n",
            "Batch 730 / 996 Sammon Loss : 0.012726493179798126\n",
            "Batch 740 / 996 Sammon Loss : 0.04381942003965378\n",
            "Batch 750 / 996 Sammon Loss : 0.2641753554344177\n",
            "Batch 760 / 996 Sammon Loss : 0.6067297458648682\n",
            "Batch 770 / 996 Sammon Loss : 0.5238189101219177\n",
            "Batch 780 / 996 Sammon Loss : 0.08815564960241318\n",
            "Batch 790 / 996 Sammon Loss : 0.0663556158542633\n",
            "Batch 800 / 996 Sammon Loss : 0.040857598185539246\n",
            "Batch 810 / 996 Sammon Loss : 0.09558994323015213\n",
            "Batch 820 / 996 Sammon Loss : 0.03156011551618576\n",
            "Batch 830 / 996 Sammon Loss : 0.07995663583278656\n",
            "Batch 840 / 996 Sammon Loss : 0.08731354773044586\n",
            "Batch 850 / 996 Sammon Loss : 0.07282628864049911\n",
            "Batch 860 / 996 Sammon Loss : 0.0019145015394315124\n",
            "Batch 870 / 996 Sammon Loss : 0.0565665028989315\n",
            "Batch 880 / 996 Sammon Loss : 0.01355814840644598\n",
            "Batch 890 / 996 Sammon Loss : 0.01601545698940754\n",
            "Batch 900 / 996 Sammon Loss : 0.01964319497346878\n",
            "Batch 910 / 996 Sammon Loss : 0.5307192206382751\n",
            "Batch 920 / 996 Sammon Loss : 0.08494897186756134\n",
            "Batch 930 / 996 Sammon Loss : 0.11853787302970886\n",
            "Batch 940 / 996 Sammon Loss : 0.4571523368358612\n",
            "Batch 950 / 996 Sammon Loss : 0.031093256548047066\n",
            "Batch 960 / 996 Sammon Loss : 0.01143194641917944\n",
            "Batch 970 / 996 Sammon Loss : 0.16954004764556885\n",
            "Batch 980 / 996 Sammon Loss : 0.05211907625198364\n",
            "Batch 990 / 996 Sammon Loss : 0.00949271209537983\n",
            "Batch 10 / 533 Regressor Loss : 144486.4375\n",
            "Batch 20 / 533 Regressor Loss : 117390.40625\n",
            "Batch 30 / 533 Regressor Loss : 139590.4375\n",
            "Batch 40 / 533 Regressor Loss : 147065.4375\n",
            "Batch 50 / 533 Regressor Loss : 134692.609375\n",
            "Batch 60 / 533 Regressor Loss : 126079.25\n",
            "Batch 70 / 533 Regressor Loss : 132389.40625\n",
            "Batch 80 / 533 Regressor Loss : 120700.375\n",
            "Batch 90 / 533 Regressor Loss : 131481.609375\n",
            "Batch 100 / 533 Regressor Loss : 146076.78125\n",
            "Batch 110 / 533 Regressor Loss : 124542.484375\n",
            "Batch 120 / 533 Regressor Loss : 112855.875\n",
            "Batch 130 / 533 Regressor Loss : 142257.171875\n",
            "Batch 140 / 533 Regressor Loss : 114186.5625\n",
            "Batch 150 / 533 Regressor Loss : 114371.3671875\n",
            "Batch 160 / 533 Regressor Loss : 138494.0\n",
            "Batch 170 / 533 Regressor Loss : 145612.265625\n",
            "Batch 180 / 533 Regressor Loss : 126906.546875\n",
            "Batch 190 / 533 Regressor Loss : 148793.125\n",
            "Batch 200 / 533 Regressor Loss : 143627.25\n",
            "Batch 210 / 533 Regressor Loss : 113702.015625\n",
            "Batch 220 / 533 Regressor Loss : 110282.171875\n",
            "Batch 230 / 533 Regressor Loss : 134902.65625\n",
            "Batch 240 / 533 Regressor Loss : 113008.609375\n",
            "Batch 250 / 533 Regressor Loss : 146868.71875\n",
            "Batch 260 / 533 Regressor Loss : 128102.484375\n",
            "Batch 270 / 533 Regressor Loss : 112730.28125\n",
            "Batch 280 / 533 Regressor Loss : 137131.078125\n",
            "Batch 290 / 533 Regressor Loss : 142032.015625\n",
            "Batch 300 / 533 Regressor Loss : 144331.234375\n",
            "Batch 310 / 533 Regressor Loss : 140848.375\n",
            "Batch 320 / 533 Regressor Loss : 140833.0625\n",
            "Batch 330 / 533 Regressor Loss : 177871.171875\n",
            "Batch 340 / 533 Regressor Loss : 111423.0\n",
            "Batch 350 / 533 Regressor Loss : 116552.546875\n",
            "Batch 360 / 533 Regressor Loss : 135592.21875\n",
            "Batch 370 / 533 Regressor Loss : 129198.7734375\n",
            "Batch 380 / 533 Regressor Loss : 127016.25\n",
            "Batch 390 / 533 Regressor Loss : 132993.46875\n",
            "Batch 400 / 533 Regressor Loss : 115850.609375\n",
            "Batch 410 / 533 Regressor Loss : 116488.375\n",
            "Batch 420 / 533 Regressor Loss : 156555.46875\n",
            "Batch 430 / 533 Regressor Loss : 110805.3125\n",
            "Batch 440 / 533 Regressor Loss : 140878.921875\n",
            "Batch 450 / 533 Regressor Loss : 127659.734375\n",
            "Batch 460 / 533 Regressor Loss : 123547.2109375\n",
            "Batch 470 / 533 Regressor Loss : 132912.3125\n",
            "Batch 480 / 533 Regressor Loss : 134653.484375\n",
            "Batch 490 / 533 Regressor Loss : 129733.6484375\n",
            "Batch 500 / 533 Regressor Loss : 138414.40625\n",
            "Batch 510 / 533 Regressor Loss : 123153.5234375\n",
            "Batch 520 / 533 Regressor Loss : 143350.40625\n",
            "Batch 530 / 533 Regressor Loss : 132177.21875\n",
            "Epoch  10 / 100  Sammon Loss : 0.3046169877052307  Regressor Loss : 177729.109375\n",
            "Val Loss : 105816.25\n",
            "Batch 10 / 996 Sammon Loss : 0.348817378282547\n",
            "Batch 20 / 996 Sammon Loss : 0.39586299657821655\n",
            "Batch 30 / 996 Sammon Loss : 0.5528454184532166\n",
            "Batch 40 / 996 Sammon Loss : 1.2294000387191772\n",
            "Batch 50 / 996 Sammon Loss : 0.9172654747962952\n",
            "Batch 60 / 996 Sammon Loss : 0.07883239537477493\n",
            "Batch 70 / 996 Sammon Loss : 0.12291055917739868\n",
            "Batch 80 / 996 Sammon Loss : 0.1425311416387558\n",
            "Batch 90 / 996 Sammon Loss : 0.04519304260611534\n",
            "Batch 100 / 996 Sammon Loss : 0.04039505496621132\n",
            "Batch 110 / 996 Sammon Loss : 0.018384914845228195\n",
            "Batch 120 / 996 Sammon Loss : 0.10209954530000687\n",
            "Batch 130 / 996 Sammon Loss : 0.004028654657304287\n",
            "Batch 140 / 996 Sammon Loss : 0.05743428319692612\n",
            "Batch 150 / 996 Sammon Loss : 0.0036336607299745083\n",
            "Batch 160 / 996 Sammon Loss : 0.016144920140504837\n",
            "Batch 170 / 996 Sammon Loss : 0.4859280288219452\n",
            "Batch 180 / 996 Sammon Loss : 0.25369012355804443\n",
            "Batch 190 / 996 Sammon Loss : 0.41471028327941895\n",
            "Batch 200 / 996 Sammon Loss : 0.21295586228370667\n",
            "Batch 210 / 996 Sammon Loss : 0.3082476854324341\n",
            "Batch 220 / 996 Sammon Loss : 0.09034281224012375\n",
            "Batch 230 / 996 Sammon Loss : 0.06170253828167915\n",
            "Batch 240 / 996 Sammon Loss : 0.11200668662786484\n",
            "Batch 250 / 996 Sammon Loss : 0.07851611822843552\n",
            "Batch 260 / 996 Sammon Loss : 0.012308330275118351\n",
            "Batch 270 / 996 Sammon Loss : 0.030966365709900856\n",
            "Batch 280 / 996 Sammon Loss : 0.02386641316115856\n",
            "Batch 290 / 996 Sammon Loss : 0.136982724070549\n",
            "Batch 300 / 996 Sammon Loss : 0.026798885315656662\n",
            "Batch 310 / 996 Sammon Loss : 0.055974334478378296\n",
            "Batch 320 / 996 Sammon Loss : 0.004273951053619385\n",
            "Batch 330 / 996 Sammon Loss : 0.06088273599743843\n",
            "Batch 340 / 996 Sammon Loss : 0.044775743037462234\n",
            "Batch 350 / 996 Sammon Loss : 0.006247549783438444\n",
            "Batch 360 / 996 Sammon Loss : 0.017748525366187096\n",
            "Batch 370 / 996 Sammon Loss : 0.11245974898338318\n",
            "Batch 380 / 996 Sammon Loss : 0.016434192657470703\n",
            "Batch 390 / 996 Sammon Loss : 0.08358185738325119\n",
            "Batch 400 / 996 Sammon Loss : 0.006586446426808834\n",
            "Batch 410 / 996 Sammon Loss : 0.3395689129829407\n",
            "Batch 420 / 996 Sammon Loss : 0.029742982238531113\n",
            "Batch 430 / 996 Sammon Loss : 0.11692529916763306\n",
            "Batch 440 / 996 Sammon Loss : 1.735817551612854\n",
            "Batch 450 / 996 Sammon Loss : 1.3682215213775635\n",
            "Batch 460 / 996 Sammon Loss : 0.6467912197113037\n",
            "Batch 470 / 996 Sammon Loss : 0.9904451966285706\n",
            "Batch 480 / 996 Sammon Loss : 0.017613964155316353\n",
            "Batch 490 / 996 Sammon Loss : 0.06334782391786575\n",
            "Batch 500 / 996 Sammon Loss : 0.07935891300439835\n",
            "Batch 510 / 996 Sammon Loss : 0.08470287919044495\n",
            "Batch 520 / 996 Sammon Loss : 0.23523087799549103\n",
            "Batch 530 / 996 Sammon Loss : 0.05122263729572296\n",
            "Batch 540 / 996 Sammon Loss : 0.021481750532984734\n",
            "Batch 550 / 996 Sammon Loss : 0.05017951503396034\n",
            "Batch 560 / 996 Sammon Loss : 0.48329320549964905\n",
            "Batch 570 / 996 Sammon Loss : 0.1966220587491989\n",
            "Batch 580 / 996 Sammon Loss : 0.12861011922359467\n",
            "Batch 590 / 996 Sammon Loss : 0.11202364414930344\n",
            "Batch 600 / 996 Sammon Loss : 0.009871350601315498\n",
            "Batch 610 / 996 Sammon Loss : 0.010869883000850677\n",
            "Batch 620 / 996 Sammon Loss : 0.0026678089052438736\n",
            "Batch 630 / 996 Sammon Loss : 0.048140350729227066\n",
            "Batch 640 / 996 Sammon Loss : 0.03548266738653183\n",
            "Batch 650 / 996 Sammon Loss : 0.039931777864694595\n",
            "Batch 660 / 996 Sammon Loss : 0.012310219928622246\n",
            "Batch 670 / 996 Sammon Loss : 0.008603288792073727\n",
            "Batch 680 / 996 Sammon Loss : 0.0358506441116333\n",
            "Batch 690 / 996 Sammon Loss : 0.1296980232000351\n",
            "Batch 700 / 996 Sammon Loss : 0.46019411087036133\n",
            "Batch 710 / 996 Sammon Loss : 0.057425301522016525\n",
            "Batch 720 / 996 Sammon Loss : 0.03581381216645241\n",
            "Batch 730 / 996 Sammon Loss : 0.012726493179798126\n",
            "Batch 740 / 996 Sammon Loss : 0.04381942003965378\n",
            "Batch 750 / 996 Sammon Loss : 0.2641753554344177\n",
            "Batch 760 / 996 Sammon Loss : 0.6067297458648682\n",
            "Batch 770 / 996 Sammon Loss : 0.5238189101219177\n",
            "Batch 780 / 996 Sammon Loss : 0.08815564960241318\n",
            "Batch 790 / 996 Sammon Loss : 0.0663556158542633\n",
            "Batch 800 / 996 Sammon Loss : 0.040857598185539246\n",
            "Batch 810 / 996 Sammon Loss : 0.09558994323015213\n",
            "Batch 820 / 996 Sammon Loss : 0.03156011551618576\n",
            "Batch 830 / 996 Sammon Loss : 0.07995663583278656\n",
            "Batch 840 / 996 Sammon Loss : 0.08731354773044586\n",
            "Batch 850 / 996 Sammon Loss : 0.07282628864049911\n",
            "Batch 860 / 996 Sammon Loss : 0.0019145015394315124\n",
            "Batch 870 / 996 Sammon Loss : 0.0565665028989315\n",
            "Batch 880 / 996 Sammon Loss : 0.01355814840644598\n",
            "Batch 890 / 996 Sammon Loss : 0.01601545698940754\n",
            "Batch 900 / 996 Sammon Loss : 0.01964319497346878\n",
            "Batch 910 / 996 Sammon Loss : 0.5307192206382751\n",
            "Batch 920 / 996 Sammon Loss : 0.08494897186756134\n",
            "Batch 930 / 996 Sammon Loss : 0.11853787302970886\n",
            "Batch 940 / 996 Sammon Loss : 0.4571523368358612\n",
            "Batch 950 / 996 Sammon Loss : 0.031093256548047066\n",
            "Batch 960 / 996 Sammon Loss : 0.01143194641917944\n",
            "Batch 970 / 996 Sammon Loss : 0.16954004764556885\n",
            "Batch 980 / 996 Sammon Loss : 0.05211907625198364\n",
            "Batch 990 / 996 Sammon Loss : 0.00949271209537983\n",
            "Batch 10 / 533 Regressor Loss : 144218.828125\n",
            "Batch 20 / 533 Regressor Loss : 117131.75\n",
            "Batch 30 / 533 Regressor Loss : 139311.53125\n",
            "Batch 40 / 533 Regressor Loss : 146789.875\n",
            "Batch 50 / 533 Regressor Loss : 134438.859375\n",
            "Batch 60 / 533 Regressor Loss : 125808.0859375\n",
            "Batch 70 / 533 Regressor Loss : 132104.21875\n",
            "Batch 80 / 533 Regressor Loss : 120439.8359375\n",
            "Batch 90 / 533 Regressor Loss : 131215.53125\n",
            "Batch 100 / 533 Regressor Loss : 145777.234375\n",
            "Batch 110 / 533 Regressor Loss : 124295.75\n",
            "Batch 120 / 533 Regressor Loss : 112619.203125\n",
            "Batch 130 / 533 Regressor Loss : 141967.28125\n",
            "Batch 140 / 533 Regressor Loss : 113937.7109375\n",
            "Batch 150 / 533 Regressor Loss : 114115.0234375\n",
            "Batch 160 / 533 Regressor Loss : 138242.625\n",
            "Batch 170 / 533 Regressor Loss : 145336.0\n",
            "Batch 180 / 533 Regressor Loss : 126636.734375\n",
            "Batch 190 / 533 Regressor Loss : 148524.75\n",
            "Batch 200 / 533 Regressor Loss : 143354.96875\n",
            "Batch 210 / 533 Regressor Loss : 113451.9375\n",
            "Batch 220 / 533 Regressor Loss : 110035.828125\n",
            "Batch 230 / 533 Regressor Loss : 134655.40625\n",
            "Batch 240 / 533 Regressor Loss : 112752.3984375\n",
            "Batch 250 / 533 Regressor Loss : 146568.25\n",
            "Batch 260 / 533 Regressor Loss : 127829.421875\n",
            "Batch 270 / 533 Regressor Loss : 112470.046875\n",
            "Batch 280 / 533 Regressor Loss : 136847.609375\n",
            "Batch 290 / 533 Regressor Loss : 141740.34375\n",
            "Batch 300 / 533 Regressor Loss : 144077.53125\n",
            "Batch 310 / 533 Regressor Loss : 140571.234375\n",
            "Batch 320 / 533 Regressor Loss : 140551.6875\n",
            "Batch 330 / 533 Regressor Loss : 177589.421875\n",
            "Batch 340 / 533 Regressor Loss : 111168.34375\n",
            "Batch 350 / 533 Regressor Loss : 116289.25\n",
            "Batch 360 / 533 Regressor Loss : 135318.875\n",
            "Batch 370 / 533 Regressor Loss : 128940.375\n",
            "Batch 380 / 533 Regressor Loss : 126755.3984375\n",
            "Batch 390 / 533 Regressor Loss : 132725.0625\n",
            "Batch 400 / 533 Regressor Loss : 115596.9609375\n",
            "Batch 410 / 533 Regressor Loss : 116235.765625\n",
            "Batch 420 / 533 Regressor Loss : 156272.1875\n",
            "Batch 430 / 533 Regressor Loss : 110561.375\n",
            "Batch 440 / 533 Regressor Loss : 140597.9375\n",
            "Batch 450 / 533 Regressor Loss : 127390.0234375\n",
            "Batch 460 / 533 Regressor Loss : 123306.765625\n",
            "Batch 470 / 533 Regressor Loss : 132641.0\n",
            "Batch 480 / 533 Regressor Loss : 134364.671875\n",
            "Batch 490 / 533 Regressor Loss : 129465.4921875\n",
            "Batch 500 / 533 Regressor Loss : 138148.8125\n",
            "Batch 510 / 533 Regressor Loss : 122904.7109375\n",
            "Batch 520 / 533 Regressor Loss : 143056.8125\n",
            "Batch 530 / 533 Regressor Loss : 131904.734375\n",
            "Epoch  11 / 100  Sammon Loss : 0.3046169877052307  Regressor Loss : 177396.125\n",
            "Val Loss : 105549.5625\n",
            "Batch 10 / 996 Sammon Loss : 0.348817378282547\n",
            "Batch 20 / 996 Sammon Loss : 0.39586299657821655\n",
            "Batch 30 / 996 Sammon Loss : 0.5528454184532166\n",
            "Batch 40 / 996 Sammon Loss : 1.2294000387191772\n",
            "Batch 50 / 996 Sammon Loss : 0.9172654747962952\n",
            "Batch 60 / 996 Sammon Loss : 0.07883239537477493\n",
            "Batch 70 / 996 Sammon Loss : 0.12291055917739868\n",
            "Batch 80 / 996 Sammon Loss : 0.1425311416387558\n",
            "Batch 90 / 996 Sammon Loss : 0.04519304260611534\n",
            "Batch 100 / 996 Sammon Loss : 0.04039505496621132\n",
            "Batch 110 / 996 Sammon Loss : 0.018384914845228195\n",
            "Batch 120 / 996 Sammon Loss : 0.10209954530000687\n",
            "Batch 130 / 996 Sammon Loss : 0.004028654657304287\n",
            "Batch 140 / 996 Sammon Loss : 0.05743428319692612\n",
            "Batch 150 / 996 Sammon Loss : 0.0036336607299745083\n",
            "Batch 160 / 996 Sammon Loss : 0.016144920140504837\n",
            "Batch 170 / 996 Sammon Loss : 0.4859280288219452\n",
            "Batch 180 / 996 Sammon Loss : 0.25369012355804443\n",
            "Batch 190 / 996 Sammon Loss : 0.41471028327941895\n",
            "Batch 200 / 996 Sammon Loss : 0.21295586228370667\n",
            "Batch 210 / 996 Sammon Loss : 0.3082476854324341\n",
            "Batch 220 / 996 Sammon Loss : 0.09034281224012375\n",
            "Batch 230 / 996 Sammon Loss : 0.06170253828167915\n",
            "Batch 240 / 996 Sammon Loss : 0.11200668662786484\n",
            "Batch 250 / 996 Sammon Loss : 0.07851611822843552\n",
            "Batch 260 / 996 Sammon Loss : 0.012308330275118351\n",
            "Batch 270 / 996 Sammon Loss : 0.030966365709900856\n",
            "Batch 280 / 996 Sammon Loss : 0.02386641316115856\n",
            "Batch 290 / 996 Sammon Loss : 0.136982724070549\n",
            "Batch 300 / 996 Sammon Loss : 0.026798885315656662\n",
            "Batch 310 / 996 Sammon Loss : 0.055974334478378296\n",
            "Batch 320 / 996 Sammon Loss : 0.004273951053619385\n",
            "Batch 330 / 996 Sammon Loss : 0.06088273599743843\n",
            "Batch 340 / 996 Sammon Loss : 0.044775743037462234\n",
            "Batch 350 / 996 Sammon Loss : 0.006247549783438444\n",
            "Batch 360 / 996 Sammon Loss : 0.017748525366187096\n",
            "Batch 370 / 996 Sammon Loss : 0.11245974898338318\n",
            "Batch 380 / 996 Sammon Loss : 0.016434192657470703\n",
            "Batch 390 / 996 Sammon Loss : 0.08358185738325119\n",
            "Batch 400 / 996 Sammon Loss : 0.006586446426808834\n",
            "Batch 410 / 996 Sammon Loss : 0.3395689129829407\n",
            "Batch 420 / 996 Sammon Loss : 0.029742982238531113\n",
            "Batch 430 / 996 Sammon Loss : 0.11692529916763306\n",
            "Batch 440 / 996 Sammon Loss : 1.735817551612854\n",
            "Batch 450 / 996 Sammon Loss : 1.3682215213775635\n",
            "Batch 460 / 996 Sammon Loss : 0.6467912197113037\n",
            "Batch 470 / 996 Sammon Loss : 0.9904451966285706\n",
            "Batch 480 / 996 Sammon Loss : 0.017613964155316353\n",
            "Batch 490 / 996 Sammon Loss : 0.06334782391786575\n",
            "Batch 500 / 996 Sammon Loss : 0.07935891300439835\n",
            "Batch 510 / 996 Sammon Loss : 0.08470287919044495\n",
            "Batch 520 / 996 Sammon Loss : 0.23523087799549103\n",
            "Batch 530 / 996 Sammon Loss : 0.05122263729572296\n",
            "Batch 540 / 996 Sammon Loss : 0.021481750532984734\n",
            "Batch 550 / 996 Sammon Loss : 0.05017951503396034\n",
            "Batch 560 / 996 Sammon Loss : 0.48329320549964905\n",
            "Batch 570 / 996 Sammon Loss : 0.1966220587491989\n",
            "Batch 580 / 996 Sammon Loss : 0.12861011922359467\n",
            "Batch 590 / 996 Sammon Loss : 0.11202364414930344\n",
            "Batch 600 / 996 Sammon Loss : 0.009871350601315498\n",
            "Batch 610 / 996 Sammon Loss : 0.010869883000850677\n",
            "Batch 620 / 996 Sammon Loss : 0.0026678089052438736\n",
            "Batch 630 / 996 Sammon Loss : 0.048140350729227066\n",
            "Batch 640 / 996 Sammon Loss : 0.03548266738653183\n",
            "Batch 650 / 996 Sammon Loss : 0.039931777864694595\n",
            "Batch 660 / 996 Sammon Loss : 0.012310219928622246\n",
            "Batch 670 / 996 Sammon Loss : 0.008603288792073727\n",
            "Batch 680 / 996 Sammon Loss : 0.0358506441116333\n",
            "Batch 690 / 996 Sammon Loss : 0.1296980232000351\n",
            "Batch 700 / 996 Sammon Loss : 0.46019411087036133\n",
            "Batch 710 / 996 Sammon Loss : 0.057425301522016525\n",
            "Batch 720 / 996 Sammon Loss : 0.03581381216645241\n",
            "Batch 730 / 996 Sammon Loss : 0.012726493179798126\n",
            "Batch 740 / 996 Sammon Loss : 0.04381942003965378\n",
            "Batch 750 / 996 Sammon Loss : 0.2641753554344177\n",
            "Batch 760 / 996 Sammon Loss : 0.6067297458648682\n",
            "Batch 770 / 996 Sammon Loss : 0.5238189101219177\n",
            "Batch 780 / 996 Sammon Loss : 0.08815564960241318\n",
            "Batch 790 / 996 Sammon Loss : 0.0663556158542633\n",
            "Batch 800 / 996 Sammon Loss : 0.040857598185539246\n",
            "Batch 810 / 996 Sammon Loss : 0.09558994323015213\n",
            "Batch 820 / 996 Sammon Loss : 0.03156011551618576\n",
            "Batch 830 / 996 Sammon Loss : 0.07995663583278656\n",
            "Batch 840 / 996 Sammon Loss : 0.08731354773044586\n",
            "Batch 850 / 996 Sammon Loss : 0.07282628864049911\n",
            "Batch 860 / 996 Sammon Loss : 0.0019145015394315124\n",
            "Batch 870 / 996 Sammon Loss : 0.0565665028989315\n",
            "Batch 880 / 996 Sammon Loss : 0.01355814840644598\n",
            "Batch 890 / 996 Sammon Loss : 0.01601545698940754\n",
            "Batch 900 / 996 Sammon Loss : 0.01964319497346878\n",
            "Batch 910 / 996 Sammon Loss : 0.5307192206382751\n",
            "Batch 920 / 996 Sammon Loss : 0.08494897186756134\n",
            "Batch 930 / 996 Sammon Loss : 0.11853787302970886\n",
            "Batch 940 / 996 Sammon Loss : 0.4571523368358612\n",
            "Batch 950 / 996 Sammon Loss : 0.031093256548047066\n",
            "Batch 960 / 996 Sammon Loss : 0.01143194641917944\n",
            "Batch 970 / 996 Sammon Loss : 0.16954004764556885\n",
            "Batch 980 / 996 Sammon Loss : 0.05211907625198364\n",
            "Batch 990 / 996 Sammon Loss : 0.00949271209537983\n",
            "Batch 10 / 533 Regressor Loss : 143951.9375\n",
            "Batch 20 / 533 Regressor Loss : 116873.8359375\n",
            "Batch 30 / 533 Regressor Loss : 139033.375\n",
            "Batch 40 / 533 Regressor Loss : 146515.03125\n",
            "Batch 50 / 533 Regressor Loss : 134185.84375\n",
            "Batch 60 / 533 Regressor Loss : 125537.671875\n",
            "Batch 70 / 533 Regressor Loss : 131819.78125\n",
            "Batch 80 / 533 Regressor Loss : 120180.0234375\n",
            "Batch 90 / 533 Regressor Loss : 130950.1796875\n",
            "Batch 100 / 533 Regressor Loss : 145478.46875\n",
            "Batch 110 / 533 Regressor Loss : 124049.71875\n",
            "Batch 120 / 533 Regressor Loss : 112383.21875\n",
            "Batch 130 / 533 Regressor Loss : 141678.15625\n",
            "Batch 140 / 533 Regressor Loss : 113689.5546875\n",
            "Batch 150 / 533 Regressor Loss : 113859.375\n",
            "Batch 160 / 533 Regressor Loss : 137991.9375\n",
            "Batch 170 / 533 Regressor Loss : 145060.46875\n",
            "Batch 180 / 533 Regressor Loss : 126367.671875\n",
            "Batch 190 / 533 Regressor Loss : 148257.0625\n",
            "Batch 200 / 533 Regressor Loss : 143083.40625\n",
            "Batch 210 / 533 Regressor Loss : 113202.5625\n",
            "Batch 220 / 533 Regressor Loss : 109790.15625\n",
            "Batch 230 / 533 Regressor Loss : 134408.84375\n",
            "Batch 240 / 533 Regressor Loss : 112496.8671875\n",
            "Batch 250 / 533 Regressor Loss : 146268.53125\n",
            "Batch 260 / 533 Regressor Loss : 127557.0625\n",
            "Batch 270 / 533 Regressor Loss : 112210.484375\n",
            "Batch 280 / 533 Regressor Loss : 136564.90625\n",
            "Batch 290 / 533 Regressor Loss : 141449.375\n",
            "Batch 300 / 533 Regressor Loss : 143824.46875\n",
            "Batch 310 / 533 Regressor Loss : 140294.796875\n",
            "Batch 320 / 533 Regressor Loss : 140271.015625\n",
            "Batch 330 / 533 Regressor Loss : 177308.34375\n",
            "Batch 340 / 533 Regressor Loss : 110914.359375\n",
            "Batch 350 / 533 Regressor Loss : 116026.6484375\n",
            "Batch 360 / 533 Regressor Loss : 135046.1875\n",
            "Batch 370 / 533 Regressor Loss : 128682.671875\n",
            "Batch 380 / 533 Regressor Loss : 126495.2421875\n",
            "Batch 390 / 533 Regressor Loss : 132457.375\n",
            "Batch 400 / 533 Regressor Loss : 115343.984375\n",
            "Batch 410 / 533 Regressor Loss : 115983.78125\n",
            "Batch 420 / 533 Regressor Loss : 155989.59375\n",
            "Batch 430 / 533 Regressor Loss : 110318.0859375\n",
            "Batch 440 / 533 Regressor Loss : 140317.65625\n",
            "Batch 450 / 533 Regressor Loss : 127121.0\n",
            "Batch 460 / 533 Regressor Loss : 123066.9375\n",
            "Batch 470 / 533 Regressor Loss : 132370.359375\n",
            "Batch 480 / 533 Regressor Loss : 134076.5625\n",
            "Batch 490 / 533 Regressor Loss : 129198.015625\n",
            "Batch 500 / 533 Regressor Loss : 137883.875\n",
            "Batch 510 / 533 Regressor Loss : 122656.546875\n",
            "Batch 520 / 533 Regressor Loss : 142763.921875\n",
            "Batch 530 / 533 Regressor Loss : 131632.90625\n",
            "Epoch  12 / 100  Sammon Loss : 0.3046169877052307  Regressor Loss : 177063.921875\n",
            "Val Loss : 105283.5234375\n",
            "Batch 10 / 996 Sammon Loss : 0.348817378282547\n",
            "Batch 20 / 996 Sammon Loss : 0.39586299657821655\n",
            "Batch 30 / 996 Sammon Loss : 0.5528454184532166\n",
            "Batch 40 / 996 Sammon Loss : 1.2294000387191772\n",
            "Batch 50 / 996 Sammon Loss : 0.9172654747962952\n",
            "Batch 60 / 996 Sammon Loss : 0.07883239537477493\n",
            "Batch 70 / 996 Sammon Loss : 0.12291055917739868\n",
            "Batch 80 / 996 Sammon Loss : 0.1425311416387558\n",
            "Batch 90 / 996 Sammon Loss : 0.04519304260611534\n",
            "Batch 100 / 996 Sammon Loss : 0.04039505496621132\n",
            "Batch 110 / 996 Sammon Loss : 0.018384914845228195\n",
            "Batch 120 / 996 Sammon Loss : 0.10209954530000687\n",
            "Batch 130 / 996 Sammon Loss : 0.004028654657304287\n",
            "Batch 140 / 996 Sammon Loss : 0.05743428319692612\n",
            "Batch 150 / 996 Sammon Loss : 0.0036336607299745083\n",
            "Batch 160 / 996 Sammon Loss : 0.016144920140504837\n",
            "Batch 170 / 996 Sammon Loss : 0.4859280288219452\n",
            "Batch 180 / 996 Sammon Loss : 0.25369012355804443\n",
            "Batch 190 / 996 Sammon Loss : 0.41471028327941895\n",
            "Batch 200 / 996 Sammon Loss : 0.21295586228370667\n",
            "Batch 210 / 996 Sammon Loss : 0.3082476854324341\n",
            "Batch 220 / 996 Sammon Loss : 0.09034281224012375\n",
            "Batch 230 / 996 Sammon Loss : 0.06170253828167915\n",
            "Batch 240 / 996 Sammon Loss : 0.11200668662786484\n",
            "Batch 250 / 996 Sammon Loss : 0.07851611822843552\n",
            "Batch 260 / 996 Sammon Loss : 0.012308330275118351\n",
            "Batch 270 / 996 Sammon Loss : 0.030966365709900856\n",
            "Batch 280 / 996 Sammon Loss : 0.02386641316115856\n",
            "Batch 290 / 996 Sammon Loss : 0.136982724070549\n",
            "Batch 300 / 996 Sammon Loss : 0.026798885315656662\n",
            "Batch 310 / 996 Sammon Loss : 0.055974334478378296\n",
            "Batch 320 / 996 Sammon Loss : 0.004273951053619385\n",
            "Batch 330 / 996 Sammon Loss : 0.06088273599743843\n",
            "Batch 340 / 996 Sammon Loss : 0.044775743037462234\n",
            "Batch 350 / 996 Sammon Loss : 0.006247549783438444\n",
            "Batch 360 / 996 Sammon Loss : 0.017748525366187096\n",
            "Batch 370 / 996 Sammon Loss : 0.11245974898338318\n",
            "Batch 380 / 996 Sammon Loss : 0.016434192657470703\n",
            "Batch 390 / 996 Sammon Loss : 0.08358185738325119\n",
            "Batch 400 / 996 Sammon Loss : 0.006586446426808834\n",
            "Batch 410 / 996 Sammon Loss : 0.3395689129829407\n",
            "Batch 420 / 996 Sammon Loss : 0.029742982238531113\n",
            "Batch 430 / 996 Sammon Loss : 0.11692529916763306\n",
            "Batch 440 / 996 Sammon Loss : 1.735817551612854\n",
            "Batch 450 / 996 Sammon Loss : 1.3682215213775635\n",
            "Batch 460 / 996 Sammon Loss : 0.6467912197113037\n",
            "Batch 470 / 996 Sammon Loss : 0.9904451966285706\n",
            "Batch 480 / 996 Sammon Loss : 0.017613964155316353\n",
            "Batch 490 / 996 Sammon Loss : 0.06334782391786575\n",
            "Batch 500 / 996 Sammon Loss : 0.07935891300439835\n",
            "Batch 510 / 996 Sammon Loss : 0.08470287919044495\n",
            "Batch 520 / 996 Sammon Loss : 0.23523087799549103\n",
            "Batch 530 / 996 Sammon Loss : 0.05122263729572296\n",
            "Batch 540 / 996 Sammon Loss : 0.021481750532984734\n",
            "Batch 550 / 996 Sammon Loss : 0.05017951503396034\n",
            "Batch 560 / 996 Sammon Loss : 0.48329320549964905\n",
            "Batch 570 / 996 Sammon Loss : 0.1966220587491989\n",
            "Batch 580 / 996 Sammon Loss : 0.12861011922359467\n",
            "Batch 590 / 996 Sammon Loss : 0.11202364414930344\n",
            "Batch 600 / 996 Sammon Loss : 0.009871350601315498\n",
            "Batch 610 / 996 Sammon Loss : 0.010869883000850677\n",
            "Batch 620 / 996 Sammon Loss : 0.0026678089052438736\n",
            "Batch 630 / 996 Sammon Loss : 0.048140350729227066\n",
            "Batch 640 / 996 Sammon Loss : 0.03548266738653183\n",
            "Batch 650 / 996 Sammon Loss : 0.039931777864694595\n",
            "Batch 660 / 996 Sammon Loss : 0.012310219928622246\n",
            "Batch 670 / 996 Sammon Loss : 0.008603288792073727\n",
            "Batch 680 / 996 Sammon Loss : 0.0358506441116333\n",
            "Batch 690 / 996 Sammon Loss : 0.1296980232000351\n",
            "Batch 700 / 996 Sammon Loss : 0.46019411087036133\n",
            "Batch 710 / 996 Sammon Loss : 0.057425301522016525\n",
            "Batch 720 / 996 Sammon Loss : 0.03581381216645241\n",
            "Batch 730 / 996 Sammon Loss : 0.012726493179798126\n",
            "Batch 740 / 996 Sammon Loss : 0.04381942003965378\n",
            "Batch 750 / 996 Sammon Loss : 0.2641753554344177\n",
            "Batch 760 / 996 Sammon Loss : 0.6067297458648682\n",
            "Batch 770 / 996 Sammon Loss : 0.5238189101219177\n",
            "Batch 780 / 996 Sammon Loss : 0.08815564960241318\n",
            "Batch 790 / 996 Sammon Loss : 0.0663556158542633\n",
            "Batch 800 / 996 Sammon Loss : 0.040857598185539246\n",
            "Batch 810 / 996 Sammon Loss : 0.09558994323015213\n",
            "Batch 820 / 996 Sammon Loss : 0.03156011551618576\n",
            "Batch 830 / 996 Sammon Loss : 0.07995663583278656\n",
            "Batch 840 / 996 Sammon Loss : 0.08731354773044586\n",
            "Batch 850 / 996 Sammon Loss : 0.07282628864049911\n",
            "Batch 860 / 996 Sammon Loss : 0.0019145015394315124\n",
            "Batch 870 / 996 Sammon Loss : 0.0565665028989315\n",
            "Batch 880 / 996 Sammon Loss : 0.01355814840644598\n",
            "Batch 890 / 996 Sammon Loss : 0.01601545698940754\n",
            "Batch 900 / 996 Sammon Loss : 0.01964319497346878\n",
            "Batch 910 / 996 Sammon Loss : 0.5307192206382751\n",
            "Batch 920 / 996 Sammon Loss : 0.08494897186756134\n",
            "Batch 930 / 996 Sammon Loss : 0.11853787302970886\n",
            "Batch 940 / 996 Sammon Loss : 0.4571523368358612\n",
            "Batch 950 / 996 Sammon Loss : 0.031093256548047066\n",
            "Batch 960 / 996 Sammon Loss : 0.01143194641917944\n",
            "Batch 970 / 996 Sammon Loss : 0.16954004764556885\n",
            "Batch 980 / 996 Sammon Loss : 0.05211907625198364\n",
            "Batch 990 / 996 Sammon Loss : 0.00949271209537983\n",
            "Batch 10 / 533 Regressor Loss : 143685.703125\n",
            "Batch 20 / 533 Regressor Loss : 116616.5625\n",
            "Batch 30 / 533 Regressor Loss : 138755.90625\n",
            "Batch 40 / 533 Regressor Loss : 146240.859375\n",
            "Batch 50 / 533 Regressor Loss : 133933.4375\n",
            "Batch 60 / 533 Regressor Loss : 125267.8984375\n",
            "Batch 70 / 533 Regressor Loss : 131536.0\n",
            "Batch 80 / 533 Regressor Loss : 119920.828125\n",
            "Batch 90 / 533 Regressor Loss : 130685.46875\n",
            "Batch 100 / 533 Regressor Loss : 145180.375\n",
            "Batch 110 / 533 Regressor Loss : 123804.2734375\n",
            "Batch 120 / 533 Regressor Loss : 112147.8359375\n",
            "Batch 130 / 533 Regressor Loss : 141389.703125\n",
            "Batch 140 / 533 Regressor Loss : 113442.03125\n",
            "Batch 150 / 533 Regressor Loss : 113604.359375\n",
            "Batch 160 / 533 Regressor Loss : 137741.890625\n",
            "Batch 170 / 533 Regressor Loss : 144785.59375\n",
            "Batch 180 / 533 Regressor Loss : 126099.2109375\n",
            "Batch 190 / 533 Regressor Loss : 147990.0\n",
            "Batch 200 / 533 Regressor Loss : 142812.484375\n",
            "Batch 210 / 533 Regressor Loss : 112953.796875\n",
            "Batch 220 / 533 Regressor Loss : 109545.109375\n",
            "Batch 230 / 533 Regressor Loss : 134162.875\n",
            "Batch 240 / 533 Regressor Loss : 112241.96875\n",
            "Batch 250 / 533 Regressor Loss : 145969.46875\n",
            "Batch 260 / 533 Regressor Loss : 127285.3125\n",
            "Batch 270 / 533 Regressor Loss : 111951.5234375\n",
            "Batch 280 / 533 Regressor Loss : 136282.796875\n",
            "Batch 290 / 533 Regressor Loss : 141159.0625\n",
            "Batch 300 / 533 Regressor Loss : 143571.984375\n",
            "Batch 310 / 533 Regressor Loss : 140019.0\n",
            "Batch 320 / 533 Regressor Loss : 139990.9375\n",
            "Batch 330 / 533 Regressor Loss : 177027.84375\n",
            "Batch 340 / 533 Regressor Loss : 110660.96875\n",
            "Batch 350 / 533 Regressor Loss : 115764.640625\n",
            "Batch 360 / 533 Regressor Loss : 134774.140625\n",
            "Batch 370 / 533 Regressor Loss : 128425.53125\n",
            "Batch 380 / 533 Regressor Loss : 126235.6875\n",
            "Batch 390 / 533 Regressor Loss : 132190.28125\n",
            "Batch 400 / 533 Regressor Loss : 115091.578125\n",
            "Batch 410 / 533 Regressor Loss : 115732.3984375\n",
            "Batch 420 / 533 Regressor Loss : 155707.59375\n",
            "Batch 430 / 533 Regressor Loss : 110075.359375\n",
            "Batch 440 / 533 Regressor Loss : 140037.984375\n",
            "Batch 450 / 533 Regressor Loss : 126852.578125\n",
            "Batch 460 / 533 Regressor Loss : 122827.671875\n",
            "Batch 470 / 533 Regressor Loss : 132100.3125\n",
            "Batch 480 / 533 Regressor Loss : 133789.0625\n",
            "Batch 490 / 533 Regressor Loss : 128931.125\n",
            "Batch 500 / 533 Regressor Loss : 137619.546875\n",
            "Batch 510 / 533 Regressor Loss : 122408.9609375\n",
            "Batch 520 / 533 Regressor Loss : 142471.640625\n",
            "Batch 530 / 533 Regressor Loss : 131361.671875\n",
            "Epoch  13 / 100  Sammon Loss : 0.3046169877052307  Regressor Loss : 176732.375\n",
            "Val Loss : 105018.078125\n",
            "Batch 10 / 996 Sammon Loss : 0.348817378282547\n",
            "Batch 20 / 996 Sammon Loss : 0.39586299657821655\n",
            "Batch 30 / 996 Sammon Loss : 0.5528454184532166\n",
            "Batch 40 / 996 Sammon Loss : 1.2294000387191772\n",
            "Batch 50 / 996 Sammon Loss : 0.9172654747962952\n",
            "Batch 60 / 996 Sammon Loss : 0.07883239537477493\n",
            "Batch 70 / 996 Sammon Loss : 0.12291055917739868\n",
            "Batch 80 / 996 Sammon Loss : 0.1425311416387558\n",
            "Batch 90 / 996 Sammon Loss : 0.04519304260611534\n",
            "Batch 100 / 996 Sammon Loss : 0.04039505496621132\n",
            "Batch 110 / 996 Sammon Loss : 0.018384914845228195\n",
            "Batch 120 / 996 Sammon Loss : 0.10209954530000687\n",
            "Batch 130 / 996 Sammon Loss : 0.004028654657304287\n",
            "Batch 140 / 996 Sammon Loss : 0.05743428319692612\n",
            "Batch 150 / 996 Sammon Loss : 0.0036336607299745083\n",
            "Batch 160 / 996 Sammon Loss : 0.016144920140504837\n",
            "Batch 170 / 996 Sammon Loss : 0.4859280288219452\n",
            "Batch 180 / 996 Sammon Loss : 0.25369012355804443\n",
            "Batch 190 / 996 Sammon Loss : 0.41471028327941895\n",
            "Batch 200 / 996 Sammon Loss : 0.21295586228370667\n",
            "Batch 210 / 996 Sammon Loss : 0.3082476854324341\n",
            "Batch 220 / 996 Sammon Loss : 0.09034281224012375\n",
            "Batch 230 / 996 Sammon Loss : 0.06170253828167915\n",
            "Batch 240 / 996 Sammon Loss : 0.11200668662786484\n",
            "Batch 250 / 996 Sammon Loss : 0.07851611822843552\n",
            "Batch 260 / 996 Sammon Loss : 0.012308330275118351\n",
            "Batch 270 / 996 Sammon Loss : 0.030966365709900856\n",
            "Batch 280 / 996 Sammon Loss : 0.02386641316115856\n",
            "Batch 290 / 996 Sammon Loss : 0.136982724070549\n",
            "Batch 300 / 996 Sammon Loss : 0.026798885315656662\n",
            "Batch 310 / 996 Sammon Loss : 0.055974334478378296\n",
            "Batch 320 / 996 Sammon Loss : 0.004273951053619385\n",
            "Batch 330 / 996 Sammon Loss : 0.06088273599743843\n",
            "Batch 340 / 996 Sammon Loss : 0.044775743037462234\n",
            "Batch 350 / 996 Sammon Loss : 0.006247549783438444\n",
            "Batch 360 / 996 Sammon Loss : 0.017748525366187096\n",
            "Batch 370 / 996 Sammon Loss : 0.11245974898338318\n",
            "Batch 380 / 996 Sammon Loss : 0.016434192657470703\n",
            "Batch 390 / 996 Sammon Loss : 0.08358185738325119\n",
            "Batch 400 / 996 Sammon Loss : 0.006586446426808834\n",
            "Batch 410 / 996 Sammon Loss : 0.3395689129829407\n",
            "Batch 420 / 996 Sammon Loss : 0.029742982238531113\n",
            "Batch 430 / 996 Sammon Loss : 0.11692529916763306\n",
            "Batch 440 / 996 Sammon Loss : 1.735817551612854\n",
            "Batch 450 / 996 Sammon Loss : 1.3682215213775635\n",
            "Batch 460 / 996 Sammon Loss : 0.6467912197113037\n",
            "Batch 470 / 996 Sammon Loss : 0.9904451966285706\n",
            "Batch 480 / 996 Sammon Loss : 0.017613964155316353\n",
            "Batch 490 / 996 Sammon Loss : 0.06334782391786575\n",
            "Batch 500 / 996 Sammon Loss : 0.07935891300439835\n",
            "Batch 510 / 996 Sammon Loss : 0.08470287919044495\n",
            "Batch 520 / 996 Sammon Loss : 0.23523087799549103\n",
            "Batch 530 / 996 Sammon Loss : 0.05122263729572296\n",
            "Batch 540 / 996 Sammon Loss : 0.021481750532984734\n",
            "Batch 550 / 996 Sammon Loss : 0.05017951503396034\n",
            "Batch 560 / 996 Sammon Loss : 0.48329320549964905\n",
            "Batch 570 / 996 Sammon Loss : 0.1966220587491989\n",
            "Batch 580 / 996 Sammon Loss : 0.12861011922359467\n",
            "Batch 590 / 996 Sammon Loss : 0.11202364414930344\n",
            "Batch 600 / 996 Sammon Loss : 0.009871350601315498\n",
            "Batch 610 / 996 Sammon Loss : 0.010869883000850677\n",
            "Batch 620 / 996 Sammon Loss : 0.0026678089052438736\n",
            "Batch 630 / 996 Sammon Loss : 0.048140350729227066\n",
            "Batch 640 / 996 Sammon Loss : 0.03548266738653183\n",
            "Batch 650 / 996 Sammon Loss : 0.039931777864694595\n",
            "Batch 660 / 996 Sammon Loss : 0.012310219928622246\n",
            "Batch 670 / 996 Sammon Loss : 0.008603288792073727\n",
            "Batch 680 / 996 Sammon Loss : 0.0358506441116333\n",
            "Batch 690 / 996 Sammon Loss : 0.1296980232000351\n",
            "Batch 700 / 996 Sammon Loss : 0.46019411087036133\n",
            "Batch 710 / 996 Sammon Loss : 0.057425301522016525\n",
            "Batch 720 / 996 Sammon Loss : 0.03581381216645241\n",
            "Batch 730 / 996 Sammon Loss : 0.012726493179798126\n",
            "Batch 740 / 996 Sammon Loss : 0.04381942003965378\n",
            "Batch 750 / 996 Sammon Loss : 0.2641753554344177\n",
            "Batch 760 / 996 Sammon Loss : 0.6067297458648682\n",
            "Batch 770 / 996 Sammon Loss : 0.5238189101219177\n",
            "Batch 780 / 996 Sammon Loss : 0.08815564960241318\n",
            "Batch 790 / 996 Sammon Loss : 0.0663556158542633\n",
            "Batch 800 / 996 Sammon Loss : 0.040857598185539246\n",
            "Batch 810 / 996 Sammon Loss : 0.09558994323015213\n",
            "Batch 820 / 996 Sammon Loss : 0.03156011551618576\n",
            "Batch 830 / 996 Sammon Loss : 0.07995663583278656\n",
            "Batch 840 / 996 Sammon Loss : 0.08731354773044586\n",
            "Batch 850 / 996 Sammon Loss : 0.07282628864049911\n",
            "Batch 860 / 996 Sammon Loss : 0.0019145015394315124\n",
            "Batch 870 / 996 Sammon Loss : 0.0565665028989315\n",
            "Batch 880 / 996 Sammon Loss : 0.01355814840644598\n",
            "Batch 890 / 996 Sammon Loss : 0.01601545698940754\n",
            "Batch 900 / 996 Sammon Loss : 0.01964319497346878\n",
            "Batch 910 / 996 Sammon Loss : 0.5307192206382751\n",
            "Batch 920 / 996 Sammon Loss : 0.08494897186756134\n",
            "Batch 930 / 996 Sammon Loss : 0.11853787302970886\n",
            "Batch 940 / 996 Sammon Loss : 0.4571523368358612\n",
            "Batch 950 / 996 Sammon Loss : 0.031093256548047066\n",
            "Batch 960 / 996 Sammon Loss : 0.01143194641917944\n",
            "Batch 970 / 996 Sammon Loss : 0.16954004764556885\n",
            "Batch 980 / 996 Sammon Loss : 0.05211907625198364\n",
            "Batch 990 / 996 Sammon Loss : 0.00949271209537983\n",
            "Batch 10 / 533 Regressor Loss : 143420.03125\n",
            "Batch 20 / 533 Regressor Loss : 116359.859375\n",
            "Batch 30 / 533 Regressor Loss : 138479.0\n",
            "Batch 40 / 533 Regressor Loss : 145967.25\n",
            "Batch 50 / 533 Regressor Loss : 133681.59375\n",
            "Batch 60 / 533 Regressor Loss : 124998.734375\n",
            "Batch 70 / 533 Regressor Loss : 131252.8125\n",
            "Batch 80 / 533 Regressor Loss : 119662.1875\n",
            "Batch 90 / 533 Regressor Loss : 130421.359375\n",
            "Batch 100 / 533 Regressor Loss : 144882.875\n",
            "Batch 110 / 533 Regressor Loss : 123559.40625\n",
            "Batch 120 / 533 Regressor Loss : 111912.9921875\n",
            "Batch 130 / 533 Regressor Loss : 141101.859375\n",
            "Batch 140 / 533 Regressor Loss : 113195.078125\n",
            "Batch 150 / 533 Regressor Loss : 113349.921875\n",
            "Batch 160 / 533 Regressor Loss : 137492.390625\n",
            "Batch 170 / 533 Regressor Loss : 144511.3125\n",
            "Batch 180 / 533 Regressor Loss : 125831.3359375\n",
            "Batch 190 / 533 Regressor Loss : 147723.484375\n",
            "Batch 200 / 533 Regressor Loss : 142542.125\n",
            "Batch 210 / 533 Regressor Loss : 112705.59375\n",
            "Batch 220 / 533 Regressor Loss : 109300.5859375\n",
            "Batch 230 / 533 Regressor Loss : 133917.4375\n",
            "Batch 240 / 533 Regressor Loss : 111987.625\n",
            "Batch 250 / 533 Regressor Loss : 145671.03125\n",
            "Batch 260 / 533 Regressor Loss : 127014.140625\n",
            "Batch 270 / 533 Regressor Loss : 111693.140625\n",
            "Batch 280 / 533 Regressor Loss : 136001.296875\n",
            "Batch 290 / 533 Regressor Loss : 140869.34375\n",
            "Batch 300 / 533 Regressor Loss : 143320.046875\n",
            "Batch 310 / 533 Regressor Loss : 139743.75\n",
            "Batch 320 / 533 Regressor Loss : 139711.46875\n",
            "Batch 330 / 533 Regressor Loss : 176747.921875\n",
            "Batch 340 / 533 Regressor Loss : 110408.125\n",
            "Batch 350 / 533 Regressor Loss : 115503.1875\n",
            "Batch 360 / 533 Regressor Loss : 134502.65625\n",
            "Batch 370 / 533 Regressor Loss : 128168.9375\n",
            "Batch 380 / 533 Regressor Loss : 125976.6875\n",
            "Batch 390 / 533 Regressor Loss : 131923.75\n",
            "Batch 400 / 533 Regressor Loss : 114839.71875\n",
            "Batch 410 / 533 Regressor Loss : 115481.546875\n",
            "Batch 420 / 533 Regressor Loss : 155426.125\n",
            "Batch 430 / 533 Regressor Loss : 109833.1796875\n",
            "Batch 440 / 533 Regressor Loss : 139758.84375\n",
            "Batch 450 / 533 Regressor Loss : 126584.734375\n",
            "Batch 460 / 533 Regressor Loss : 122588.921875\n",
            "Batch 470 / 533 Regressor Loss : 131830.84375\n",
            "Batch 480 / 533 Regressor Loss : 133502.125\n",
            "Batch 490 / 533 Regressor Loss : 128664.8046875\n",
            "Batch 500 / 533 Regressor Loss : 137355.734375\n",
            "Batch 510 / 533 Regressor Loss : 122161.9296875\n",
            "Batch 520 / 533 Regressor Loss : 142179.921875\n",
            "Batch 530 / 533 Regressor Loss : 131091.0\n",
            "Epoch  14 / 100  Sammon Loss : 0.3046169877052307  Regressor Loss : 176401.4375\n",
            "Val Loss : 104753.203125\n",
            "Batch 10 / 996 Sammon Loss : 0.348817378282547\n",
            "Batch 20 / 996 Sammon Loss : 0.39586299657821655\n",
            "Batch 30 / 996 Sammon Loss : 0.5528454184532166\n",
            "Batch 40 / 996 Sammon Loss : 1.2294000387191772\n",
            "Batch 50 / 996 Sammon Loss : 0.9172654747962952\n",
            "Batch 60 / 996 Sammon Loss : 0.07883239537477493\n",
            "Batch 70 / 996 Sammon Loss : 0.12291055917739868\n",
            "Batch 80 / 996 Sammon Loss : 0.1425311416387558\n",
            "Batch 90 / 996 Sammon Loss : 0.04519304260611534\n",
            "Batch 100 / 996 Sammon Loss : 0.04039505496621132\n",
            "Batch 110 / 996 Sammon Loss : 0.018384914845228195\n",
            "Batch 120 / 996 Sammon Loss : 0.10209954530000687\n",
            "Batch 130 / 996 Sammon Loss : 0.004028654657304287\n",
            "Batch 140 / 996 Sammon Loss : 0.05743428319692612\n",
            "Batch 150 / 996 Sammon Loss : 0.0036336607299745083\n",
            "Batch 160 / 996 Sammon Loss : 0.016144920140504837\n",
            "Batch 170 / 996 Sammon Loss : 0.4859280288219452\n",
            "Batch 180 / 996 Sammon Loss : 0.25369012355804443\n",
            "Batch 190 / 996 Sammon Loss : 0.41471028327941895\n",
            "Batch 200 / 996 Sammon Loss : 0.21295586228370667\n",
            "Batch 210 / 996 Sammon Loss : 0.3082476854324341\n",
            "Batch 220 / 996 Sammon Loss : 0.09034281224012375\n",
            "Batch 230 / 996 Sammon Loss : 0.06170253828167915\n",
            "Batch 240 / 996 Sammon Loss : 0.11200668662786484\n",
            "Batch 250 / 996 Sammon Loss : 0.07851611822843552\n",
            "Batch 260 / 996 Sammon Loss : 0.012308330275118351\n",
            "Batch 270 / 996 Sammon Loss : 0.030966365709900856\n",
            "Batch 280 / 996 Sammon Loss : 0.02386641316115856\n",
            "Batch 290 / 996 Sammon Loss : 0.136982724070549\n",
            "Batch 300 / 996 Sammon Loss : 0.026798885315656662\n",
            "Batch 310 / 996 Sammon Loss : 0.055974334478378296\n",
            "Batch 320 / 996 Sammon Loss : 0.004273951053619385\n",
            "Batch 330 / 996 Sammon Loss : 0.06088273599743843\n",
            "Batch 340 / 996 Sammon Loss : 0.044775743037462234\n",
            "Batch 350 / 996 Sammon Loss : 0.006247549783438444\n",
            "Batch 360 / 996 Sammon Loss : 0.017748525366187096\n",
            "Batch 370 / 996 Sammon Loss : 0.11245974898338318\n",
            "Batch 380 / 996 Sammon Loss : 0.016434192657470703\n",
            "Batch 390 / 996 Sammon Loss : 0.08358185738325119\n",
            "Batch 400 / 996 Sammon Loss : 0.006586446426808834\n",
            "Batch 410 / 996 Sammon Loss : 0.3395689129829407\n",
            "Batch 420 / 996 Sammon Loss : 0.029742982238531113\n",
            "Batch 430 / 996 Sammon Loss : 0.11692529916763306\n",
            "Batch 440 / 996 Sammon Loss : 1.735817551612854\n",
            "Batch 450 / 996 Sammon Loss : 1.3682215213775635\n",
            "Batch 460 / 996 Sammon Loss : 0.6467912197113037\n",
            "Batch 470 / 996 Sammon Loss : 0.9904451966285706\n",
            "Batch 480 / 996 Sammon Loss : 0.017613964155316353\n",
            "Batch 490 / 996 Sammon Loss : 0.06334782391786575\n",
            "Batch 500 / 996 Sammon Loss : 0.07935891300439835\n",
            "Batch 510 / 996 Sammon Loss : 0.08470287919044495\n",
            "Batch 520 / 996 Sammon Loss : 0.23523087799549103\n",
            "Batch 530 / 996 Sammon Loss : 0.05122263729572296\n",
            "Batch 540 / 996 Sammon Loss : 0.021481750532984734\n",
            "Batch 550 / 996 Sammon Loss : 0.05017951503396034\n",
            "Batch 560 / 996 Sammon Loss : 0.48329320549964905\n",
            "Batch 570 / 996 Sammon Loss : 0.1966220587491989\n",
            "Batch 580 / 996 Sammon Loss : 0.12861011922359467\n",
            "Batch 590 / 996 Sammon Loss : 0.11202364414930344\n",
            "Batch 600 / 996 Sammon Loss : 0.009871350601315498\n",
            "Batch 610 / 996 Sammon Loss : 0.010869883000850677\n",
            "Batch 620 / 996 Sammon Loss : 0.0026678089052438736\n",
            "Batch 630 / 996 Sammon Loss : 0.048140350729227066\n",
            "Batch 640 / 996 Sammon Loss : 0.03548266738653183\n",
            "Batch 650 / 996 Sammon Loss : 0.039931777864694595\n",
            "Batch 660 / 996 Sammon Loss : 0.012310219928622246\n",
            "Batch 670 / 996 Sammon Loss : 0.008603288792073727\n",
            "Batch 680 / 996 Sammon Loss : 0.0358506441116333\n",
            "Batch 690 / 996 Sammon Loss : 0.1296980232000351\n",
            "Batch 700 / 996 Sammon Loss : 0.46019411087036133\n",
            "Batch 710 / 996 Sammon Loss : 0.057425301522016525\n",
            "Batch 720 / 996 Sammon Loss : 0.03581381216645241\n",
            "Batch 730 / 996 Sammon Loss : 0.012726493179798126\n",
            "Batch 740 / 996 Sammon Loss : 0.04381942003965378\n",
            "Batch 750 / 996 Sammon Loss : 0.2641753554344177\n",
            "Batch 760 / 996 Sammon Loss : 0.6067297458648682\n",
            "Batch 770 / 996 Sammon Loss : 0.5238189101219177\n",
            "Batch 780 / 996 Sammon Loss : 0.08815564960241318\n",
            "Batch 790 / 996 Sammon Loss : 0.0663556158542633\n",
            "Batch 800 / 996 Sammon Loss : 0.040857598185539246\n",
            "Batch 810 / 996 Sammon Loss : 0.09558994323015213\n",
            "Batch 820 / 996 Sammon Loss : 0.03156011551618576\n",
            "Batch 830 / 996 Sammon Loss : 0.07995663583278656\n",
            "Batch 840 / 996 Sammon Loss : 0.08731354773044586\n",
            "Batch 850 / 996 Sammon Loss : 0.07282628864049911\n",
            "Batch 860 / 996 Sammon Loss : 0.0019145015394315124\n",
            "Batch 870 / 996 Sammon Loss : 0.0565665028989315\n",
            "Batch 880 / 996 Sammon Loss : 0.01355814840644598\n",
            "Batch 890 / 996 Sammon Loss : 0.01601545698940754\n",
            "Batch 900 / 996 Sammon Loss : 0.01964319497346878\n",
            "Batch 910 / 996 Sammon Loss : 0.5307192206382751\n",
            "Batch 920 / 996 Sammon Loss : 0.08494897186756134\n",
            "Batch 930 / 996 Sammon Loss : 0.11853787302970886\n",
            "Batch 940 / 996 Sammon Loss : 0.4571523368358612\n",
            "Batch 950 / 996 Sammon Loss : 0.031093256548047066\n",
            "Batch 960 / 996 Sammon Loss : 0.01143194641917944\n",
            "Batch 970 / 996 Sammon Loss : 0.16954004764556885\n",
            "Batch 980 / 996 Sammon Loss : 0.05211907625198364\n",
            "Batch 990 / 996 Sammon Loss : 0.00949271209537983\n",
            "Batch 10 / 533 Regressor Loss : 143154.921875\n",
            "Batch 20 / 533 Regressor Loss : 116103.6875\n",
            "Batch 30 / 533 Regressor Loss : 138202.65625\n",
            "Batch 40 / 533 Regressor Loss : 145694.1875\n",
            "Batch 50 / 533 Regressor Loss : 133430.28125\n",
            "Batch 60 / 533 Regressor Loss : 124730.125\n",
            "Batch 70 / 533 Regressor Loss : 130970.203125\n",
            "Batch 80 / 533 Regressor Loss : 119404.109375\n",
            "Batch 90 / 533 Regressor Loss : 130157.75\n",
            "Batch 100 / 533 Regressor Loss : 144585.96875\n",
            "Batch 110 / 533 Regressor Loss : 123315.0546875\n",
            "Batch 120 / 533 Regressor Loss : 111678.671875\n",
            "Batch 130 / 533 Regressor Loss : 140814.546875\n",
            "Batch 140 / 533 Regressor Loss : 112948.65625\n",
            "Batch 150 / 533 Regressor Loss : 113096.0\n",
            "Batch 160 / 533 Regressor Loss : 137243.40625\n",
            "Batch 170 / 533 Regressor Loss : 144237.546875\n",
            "Batch 180 / 533 Regressor Loss : 125564.0\n",
            "Batch 190 / 533 Regressor Loss : 147457.484375\n",
            "Batch 200 / 533 Regressor Loss : 142272.296875\n",
            "Batch 210 / 533 Regressor Loss : 112457.921875\n",
            "Batch 220 / 533 Regressor Loss : 109056.609375\n",
            "Batch 230 / 533 Regressor Loss : 133672.546875\n",
            "Batch 240 / 533 Regressor Loss : 111733.828125\n",
            "Batch 250 / 533 Regressor Loss : 145373.125\n",
            "Batch 260 / 533 Regressor Loss : 126743.5234375\n",
            "Batch 270 / 533 Regressor Loss : 111435.2734375\n",
            "Batch 280 / 533 Regressor Loss : 135720.34375\n",
            "Batch 290 / 533 Regressor Loss : 140580.15625\n",
            "Batch 300 / 533 Regressor Loss : 143068.59375\n",
            "Batch 310 / 533 Regressor Loss : 139469.03125\n",
            "Batch 320 / 533 Regressor Loss : 139432.53125\n",
            "Batch 330 / 533 Regressor Loss : 176468.484375\n",
            "Batch 340 / 533 Regressor Loss : 110155.8359375\n",
            "Batch 350 / 533 Regressor Loss : 115242.2734375\n",
            "Batch 360 / 533 Regressor Loss : 134231.6875\n",
            "Batch 370 / 533 Regressor Loss : 127912.875\n",
            "Batch 380 / 533 Regressor Loss : 125718.2109375\n",
            "Batch 390 / 533 Regressor Loss : 131657.75\n",
            "Batch 400 / 533 Regressor Loss : 114588.375\n",
            "Batch 410 / 533 Regressor Loss : 115231.1875\n",
            "Batch 420 / 533 Regressor Loss : 155145.21875\n",
            "Batch 430 / 533 Regressor Loss : 109591.484375\n",
            "Batch 440 / 533 Regressor Loss : 139480.28125\n",
            "Batch 450 / 533 Regressor Loss : 126317.3984375\n",
            "Batch 460 / 533 Regressor Loss : 122350.6875\n",
            "Batch 470 / 533 Regressor Loss : 131561.90625\n",
            "Batch 480 / 533 Regressor Loss : 133215.75\n",
            "Batch 490 / 533 Regressor Loss : 128399.0\n",
            "Batch 500 / 533 Regressor Loss : 137092.46875\n",
            "Batch 510 / 533 Regressor Loss : 121915.3984375\n",
            "Batch 520 / 533 Regressor Loss : 141888.75\n",
            "Batch 530 / 533 Regressor Loss : 130820.828125\n",
            "Epoch  15 / 100  Sammon Loss : 0.3046169877052307  Regressor Loss : 176071.125\n",
            "Val Loss : 104488.8359375\n",
            "Batch 10 / 996 Sammon Loss : 0.348817378282547\n",
            "Batch 20 / 996 Sammon Loss : 0.39586299657821655\n",
            "Batch 30 / 996 Sammon Loss : 0.5528454184532166\n",
            "Batch 40 / 996 Sammon Loss : 1.2294000387191772\n",
            "Batch 50 / 996 Sammon Loss : 0.9172654747962952\n",
            "Batch 60 / 996 Sammon Loss : 0.07883239537477493\n",
            "Batch 70 / 996 Sammon Loss : 0.12291055917739868\n",
            "Batch 80 / 996 Sammon Loss : 0.1425311416387558\n",
            "Batch 90 / 996 Sammon Loss : 0.04519304260611534\n",
            "Batch 100 / 996 Sammon Loss : 0.04039505496621132\n",
            "Batch 110 / 996 Sammon Loss : 0.018384914845228195\n",
            "Batch 120 / 996 Sammon Loss : 0.10209954530000687\n",
            "Batch 130 / 996 Sammon Loss : 0.004028654657304287\n",
            "Batch 140 / 996 Sammon Loss : 0.05743428319692612\n",
            "Batch 150 / 996 Sammon Loss : 0.0036336607299745083\n",
            "Batch 160 / 996 Sammon Loss : 0.016144920140504837\n",
            "Batch 170 / 996 Sammon Loss : 0.4859280288219452\n",
            "Batch 180 / 996 Sammon Loss : 0.25369012355804443\n",
            "Batch 190 / 996 Sammon Loss : 0.41471028327941895\n",
            "Batch 200 / 996 Sammon Loss : 0.21295586228370667\n",
            "Batch 210 / 996 Sammon Loss : 0.3082476854324341\n",
            "Batch 220 / 996 Sammon Loss : 0.09034281224012375\n",
            "Batch 230 / 996 Sammon Loss : 0.06170253828167915\n",
            "Batch 240 / 996 Sammon Loss : 0.11200668662786484\n",
            "Batch 250 / 996 Sammon Loss : 0.07851611822843552\n",
            "Batch 260 / 996 Sammon Loss : 0.012308330275118351\n",
            "Batch 270 / 996 Sammon Loss : 0.030966365709900856\n",
            "Batch 280 / 996 Sammon Loss : 0.02386641316115856\n",
            "Batch 290 / 996 Sammon Loss : 0.136982724070549\n",
            "Batch 300 / 996 Sammon Loss : 0.026798885315656662\n",
            "Batch 310 / 996 Sammon Loss : 0.055974334478378296\n",
            "Batch 320 / 996 Sammon Loss : 0.004273951053619385\n",
            "Batch 330 / 996 Sammon Loss : 0.06088273599743843\n",
            "Batch 340 / 996 Sammon Loss : 0.044775743037462234\n",
            "Batch 350 / 996 Sammon Loss : 0.006247549783438444\n",
            "Batch 360 / 996 Sammon Loss : 0.017748525366187096\n",
            "Batch 370 / 996 Sammon Loss : 0.11245974898338318\n",
            "Batch 380 / 996 Sammon Loss : 0.016434192657470703\n",
            "Batch 390 / 996 Sammon Loss : 0.08358185738325119\n",
            "Batch 400 / 996 Sammon Loss : 0.006586446426808834\n",
            "Batch 410 / 996 Sammon Loss : 0.3395689129829407\n",
            "Batch 420 / 996 Sammon Loss : 0.029742982238531113\n",
            "Batch 430 / 996 Sammon Loss : 0.11692529916763306\n",
            "Batch 440 / 996 Sammon Loss : 1.735817551612854\n",
            "Batch 450 / 996 Sammon Loss : 1.3682215213775635\n",
            "Batch 460 / 996 Sammon Loss : 0.6467912197113037\n",
            "Batch 470 / 996 Sammon Loss : 0.9904451966285706\n",
            "Batch 480 / 996 Sammon Loss : 0.017613964155316353\n",
            "Batch 490 / 996 Sammon Loss : 0.06334782391786575\n",
            "Batch 500 / 996 Sammon Loss : 0.07935891300439835\n",
            "Batch 510 / 996 Sammon Loss : 0.08470287919044495\n",
            "Batch 520 / 996 Sammon Loss : 0.23523087799549103\n",
            "Batch 530 / 996 Sammon Loss : 0.05122263729572296\n",
            "Batch 540 / 996 Sammon Loss : 0.021481750532984734\n",
            "Batch 550 / 996 Sammon Loss : 0.05017951503396034\n",
            "Batch 560 / 996 Sammon Loss : 0.48329320549964905\n",
            "Batch 570 / 996 Sammon Loss : 0.1966220587491989\n",
            "Batch 580 / 996 Sammon Loss : 0.12861011922359467\n",
            "Batch 590 / 996 Sammon Loss : 0.11202364414930344\n",
            "Batch 600 / 996 Sammon Loss : 0.009871350601315498\n",
            "Batch 610 / 996 Sammon Loss : 0.010869883000850677\n",
            "Batch 620 / 996 Sammon Loss : 0.0026678089052438736\n",
            "Batch 630 / 996 Sammon Loss : 0.048140350729227066\n",
            "Batch 640 / 996 Sammon Loss : 0.03548266738653183\n",
            "Batch 650 / 996 Sammon Loss : 0.039931777864694595\n",
            "Batch 660 / 996 Sammon Loss : 0.012310219928622246\n",
            "Batch 670 / 996 Sammon Loss : 0.008603288792073727\n",
            "Batch 680 / 996 Sammon Loss : 0.0358506441116333\n",
            "Batch 690 / 996 Sammon Loss : 0.1296980232000351\n",
            "Batch 700 / 996 Sammon Loss : 0.46019411087036133\n",
            "Batch 710 / 996 Sammon Loss : 0.057425301522016525\n",
            "Batch 720 / 996 Sammon Loss : 0.03581381216645241\n",
            "Batch 730 / 996 Sammon Loss : 0.012726493179798126\n",
            "Batch 740 / 996 Sammon Loss : 0.04381942003965378\n",
            "Batch 750 / 996 Sammon Loss : 0.2641753554344177\n",
            "Batch 760 / 996 Sammon Loss : 0.6067297458648682\n",
            "Batch 770 / 996 Sammon Loss : 0.5238189101219177\n",
            "Batch 780 / 996 Sammon Loss : 0.08815564960241318\n",
            "Batch 790 / 996 Sammon Loss : 0.0663556158542633\n",
            "Batch 800 / 996 Sammon Loss : 0.040857598185539246\n",
            "Batch 810 / 996 Sammon Loss : 0.09558994323015213\n",
            "Batch 820 / 996 Sammon Loss : 0.03156011551618576\n",
            "Batch 830 / 996 Sammon Loss : 0.07995663583278656\n",
            "Batch 840 / 996 Sammon Loss : 0.08731354773044586\n",
            "Batch 850 / 996 Sammon Loss : 0.07282628864049911\n",
            "Batch 860 / 996 Sammon Loss : 0.0019145015394315124\n",
            "Batch 870 / 996 Sammon Loss : 0.0565665028989315\n",
            "Batch 880 / 996 Sammon Loss : 0.01355814840644598\n",
            "Batch 890 / 996 Sammon Loss : 0.01601545698940754\n",
            "Batch 900 / 996 Sammon Loss : 0.01964319497346878\n",
            "Batch 910 / 996 Sammon Loss : 0.5307192206382751\n",
            "Batch 920 / 996 Sammon Loss : 0.08494897186756134\n",
            "Batch 930 / 996 Sammon Loss : 0.11853787302970886\n",
            "Batch 940 / 996 Sammon Loss : 0.4571523368358612\n",
            "Batch 950 / 996 Sammon Loss : 0.031093256548047066\n",
            "Batch 960 / 996 Sammon Loss : 0.01143194641917944\n",
            "Batch 970 / 996 Sammon Loss : 0.16954004764556885\n",
            "Batch 980 / 996 Sammon Loss : 0.05211907625198364\n",
            "Batch 990 / 996 Sammon Loss : 0.00949271209537983\n",
            "Batch 10 / 533 Regressor Loss : 142890.3125\n",
            "Batch 20 / 533 Regressor Loss : 115848.046875\n",
            "Batch 30 / 533 Regressor Loss : 137926.84375\n",
            "Batch 40 / 533 Regressor Loss : 145421.625\n",
            "Batch 50 / 533 Regressor Loss : 133179.46875\n",
            "Batch 60 / 533 Regressor Loss : 124462.03125\n",
            "Batch 70 / 533 Regressor Loss : 130688.109375\n",
            "Batch 80 / 533 Regressor Loss : 119146.5234375\n",
            "Batch 90 / 533 Regressor Loss : 129894.671875\n",
            "Batch 100 / 533 Regressor Loss : 144289.5625\n",
            "Batch 110 / 533 Regressor Loss : 123071.1875\n",
            "Batch 120 / 533 Regressor Loss : 111444.8359375\n",
            "Batch 130 / 533 Regressor Loss : 140527.8125\n",
            "Batch 140 / 533 Regressor Loss : 112702.765625\n",
            "Batch 150 / 533 Regressor Loss : 112842.609375\n",
            "Batch 160 / 533 Regressor Loss : 136994.9375\n",
            "Batch 170 / 533 Regressor Loss : 143964.34375\n",
            "Batch 180 / 533 Regressor Loss : 125297.1875\n",
            "Batch 190 / 533 Regressor Loss : 147191.984375\n",
            "Batch 200 / 533 Regressor Loss : 142003.0\n",
            "Batch 210 / 533 Regressor Loss : 112210.7421875\n",
            "Batch 220 / 533 Regressor Loss : 108813.125\n",
            "Batch 230 / 533 Regressor Loss : 133428.125\n",
            "Batch 240 / 533 Regressor Loss : 111480.53125\n",
            "Batch 250 / 533 Regressor Loss : 145075.78125\n",
            "Batch 260 / 533 Regressor Loss : 126473.3984375\n",
            "Batch 270 / 533 Regressor Loss : 111177.8984375\n",
            "Batch 280 / 533 Regressor Loss : 135439.921875\n",
            "Batch 290 / 533 Regressor Loss : 140291.53125\n",
            "Batch 300 / 533 Regressor Loss : 142817.609375\n",
            "Batch 310 / 533 Regressor Loss : 139194.84375\n",
            "Batch 320 / 533 Regressor Loss : 139154.078125\n",
            "Batch 330 / 533 Regressor Loss : 176189.546875\n",
            "Batch 340 / 533 Regressor Loss : 109904.0234375\n",
            "Batch 350 / 533 Regressor Loss : 114981.859375\n",
            "Batch 360 / 533 Regressor Loss : 133961.234375\n",
            "Batch 370 / 533 Regressor Loss : 127657.2734375\n",
            "Batch 380 / 533 Regressor Loss : 125460.25\n",
            "Batch 390 / 533 Regressor Loss : 131392.25\n",
            "Batch 400 / 533 Regressor Loss : 114337.546875\n",
            "Batch 410 / 533 Regressor Loss : 114981.3359375\n",
            "Batch 420 / 533 Regressor Loss : 154864.796875\n",
            "Batch 430 / 533 Regressor Loss : 109350.2734375\n",
            "Batch 440 / 533 Regressor Loss : 139202.1875\n",
            "Batch 450 / 533 Regressor Loss : 126050.5859375\n",
            "Batch 460 / 533 Regressor Loss : 122112.8984375\n",
            "Batch 470 / 533 Regressor Loss : 131293.46875\n",
            "Batch 480 / 533 Regressor Loss : 132929.859375\n",
            "Batch 490 / 533 Regressor Loss : 128133.7109375\n",
            "Batch 500 / 533 Regressor Loss : 136829.6875\n",
            "Batch 510 / 533 Regressor Loss : 121669.359375\n",
            "Batch 520 / 533 Regressor Loss : 141598.09375\n",
            "Batch 530 / 533 Regressor Loss : 130551.1484375\n",
            "Epoch  16 / 100  Sammon Loss : 0.3046169877052307  Regressor Loss : 175741.359375\n",
            "Val Loss : 104224.984375\n",
            "Batch 10 / 996 Sammon Loss : 0.348817378282547\n",
            "Batch 20 / 996 Sammon Loss : 0.39586299657821655\n",
            "Batch 30 / 996 Sammon Loss : 0.5528454184532166\n",
            "Batch 40 / 996 Sammon Loss : 1.2294000387191772\n",
            "Batch 50 / 996 Sammon Loss : 0.9172654747962952\n",
            "Batch 60 / 996 Sammon Loss : 0.07883239537477493\n",
            "Batch 70 / 996 Sammon Loss : 0.12291055917739868\n",
            "Batch 80 / 996 Sammon Loss : 0.1425311416387558\n",
            "Batch 90 / 996 Sammon Loss : 0.04519304260611534\n",
            "Batch 100 / 996 Sammon Loss : 0.04039505496621132\n",
            "Batch 110 / 996 Sammon Loss : 0.018384914845228195\n",
            "Batch 120 / 996 Sammon Loss : 0.10209954530000687\n",
            "Batch 130 / 996 Sammon Loss : 0.004028654657304287\n",
            "Batch 140 / 996 Sammon Loss : 0.05743428319692612\n",
            "Batch 150 / 996 Sammon Loss : 0.0036336607299745083\n",
            "Batch 160 / 996 Sammon Loss : 0.016144920140504837\n",
            "Batch 170 / 996 Sammon Loss : 0.4859280288219452\n",
            "Batch 180 / 996 Sammon Loss : 0.25369012355804443\n",
            "Batch 190 / 996 Sammon Loss : 0.41471028327941895\n",
            "Batch 200 / 996 Sammon Loss : 0.21295586228370667\n",
            "Batch 210 / 996 Sammon Loss : 0.3082476854324341\n",
            "Batch 220 / 996 Sammon Loss : 0.09034281224012375\n",
            "Batch 230 / 996 Sammon Loss : 0.06170253828167915\n",
            "Batch 240 / 996 Sammon Loss : 0.11200668662786484\n",
            "Batch 250 / 996 Sammon Loss : 0.07851611822843552\n",
            "Batch 260 / 996 Sammon Loss : 0.012308330275118351\n",
            "Batch 270 / 996 Sammon Loss : 0.030966365709900856\n",
            "Batch 280 / 996 Sammon Loss : 0.02386641316115856\n",
            "Batch 290 / 996 Sammon Loss : 0.136982724070549\n",
            "Batch 300 / 996 Sammon Loss : 0.026798885315656662\n",
            "Batch 310 / 996 Sammon Loss : 0.055974334478378296\n",
            "Batch 320 / 996 Sammon Loss : 0.004273951053619385\n",
            "Batch 330 / 996 Sammon Loss : 0.06088273599743843\n",
            "Batch 340 / 996 Sammon Loss : 0.044775743037462234\n",
            "Batch 350 / 996 Sammon Loss : 0.006247549783438444\n",
            "Batch 360 / 996 Sammon Loss : 0.017748525366187096\n",
            "Batch 370 / 996 Sammon Loss : 0.11245974898338318\n",
            "Batch 380 / 996 Sammon Loss : 0.016434192657470703\n",
            "Batch 390 / 996 Sammon Loss : 0.08358185738325119\n",
            "Batch 400 / 996 Sammon Loss : 0.006586446426808834\n",
            "Batch 410 / 996 Sammon Loss : 0.3395689129829407\n",
            "Batch 420 / 996 Sammon Loss : 0.029742982238531113\n",
            "Batch 430 / 996 Sammon Loss : 0.11692529916763306\n",
            "Batch 440 / 996 Sammon Loss : 1.735817551612854\n",
            "Batch 450 / 996 Sammon Loss : 1.3682215213775635\n",
            "Batch 460 / 996 Sammon Loss : 0.6467912197113037\n",
            "Batch 470 / 996 Sammon Loss : 0.9904451966285706\n",
            "Batch 480 / 996 Sammon Loss : 0.017613964155316353\n",
            "Batch 490 / 996 Sammon Loss : 0.06334782391786575\n",
            "Batch 500 / 996 Sammon Loss : 0.07935891300439835\n",
            "Batch 510 / 996 Sammon Loss : 0.08470287919044495\n",
            "Batch 520 / 996 Sammon Loss : 0.23523087799549103\n",
            "Batch 530 / 996 Sammon Loss : 0.05122263729572296\n",
            "Batch 540 / 996 Sammon Loss : 0.021481750532984734\n",
            "Batch 550 / 996 Sammon Loss : 0.05017951503396034\n",
            "Batch 560 / 996 Sammon Loss : 0.48329320549964905\n",
            "Batch 570 / 996 Sammon Loss : 0.1966220587491989\n",
            "Batch 580 / 996 Sammon Loss : 0.12861011922359467\n",
            "Batch 590 / 996 Sammon Loss : 0.11202364414930344\n",
            "Batch 600 / 996 Sammon Loss : 0.009871350601315498\n",
            "Batch 610 / 996 Sammon Loss : 0.010869883000850677\n",
            "Batch 620 / 996 Sammon Loss : 0.0026678089052438736\n",
            "Batch 630 / 996 Sammon Loss : 0.048140350729227066\n",
            "Batch 640 / 996 Sammon Loss : 0.03548266738653183\n",
            "Batch 650 / 996 Sammon Loss : 0.039931777864694595\n",
            "Batch 660 / 996 Sammon Loss : 0.012310219928622246\n",
            "Batch 670 / 996 Sammon Loss : 0.008603288792073727\n",
            "Batch 680 / 996 Sammon Loss : 0.0358506441116333\n",
            "Batch 690 / 996 Sammon Loss : 0.1296980232000351\n",
            "Batch 700 / 996 Sammon Loss : 0.46019411087036133\n",
            "Batch 710 / 996 Sammon Loss : 0.057425301522016525\n",
            "Batch 720 / 996 Sammon Loss : 0.03581381216645241\n",
            "Batch 730 / 996 Sammon Loss : 0.012726493179798126\n",
            "Batch 740 / 996 Sammon Loss : 0.04381942003965378\n",
            "Batch 750 / 996 Sammon Loss : 0.2641753554344177\n",
            "Batch 760 / 996 Sammon Loss : 0.6067297458648682\n",
            "Batch 770 / 996 Sammon Loss : 0.5238189101219177\n",
            "Batch 780 / 996 Sammon Loss : 0.08815564960241318\n",
            "Batch 790 / 996 Sammon Loss : 0.0663556158542633\n",
            "Batch 800 / 996 Sammon Loss : 0.040857598185539246\n",
            "Batch 810 / 996 Sammon Loss : 0.09558994323015213\n",
            "Batch 820 / 996 Sammon Loss : 0.03156011551618576\n",
            "Batch 830 / 996 Sammon Loss : 0.07995663583278656\n",
            "Batch 840 / 996 Sammon Loss : 0.08731354773044586\n",
            "Batch 850 / 996 Sammon Loss : 0.07282628864049911\n",
            "Batch 860 / 996 Sammon Loss : 0.0019145015394315124\n",
            "Batch 870 / 996 Sammon Loss : 0.0565665028989315\n",
            "Batch 880 / 996 Sammon Loss : 0.01355814840644598\n",
            "Batch 890 / 996 Sammon Loss : 0.01601545698940754\n",
            "Batch 900 / 996 Sammon Loss : 0.01964319497346878\n",
            "Batch 910 / 996 Sammon Loss : 0.5307192206382751\n",
            "Batch 920 / 996 Sammon Loss : 0.08494897186756134\n",
            "Batch 930 / 996 Sammon Loss : 0.11853787302970886\n",
            "Batch 940 / 996 Sammon Loss : 0.4571523368358612\n",
            "Batch 950 / 996 Sammon Loss : 0.031093256548047066\n",
            "Batch 960 / 996 Sammon Loss : 0.01143194641917944\n",
            "Batch 970 / 996 Sammon Loss : 0.16954004764556885\n",
            "Batch 980 / 996 Sammon Loss : 0.05211907625198364\n",
            "Batch 990 / 996 Sammon Loss : 0.00949271209537983\n",
            "Batch 10 / 533 Regressor Loss : 142626.1875\n",
            "Batch 20 / 533 Regressor Loss : 115592.875\n",
            "Batch 30 / 533 Regressor Loss : 137651.53125\n",
            "Batch 40 / 533 Regressor Loss : 145149.5625\n",
            "Batch 50 / 533 Regressor Loss : 132929.125\n",
            "Batch 60 / 533 Regressor Loss : 124194.4609375\n",
            "Batch 70 / 533 Regressor Loss : 130406.5234375\n",
            "Batch 80 / 533 Regressor Loss : 118889.421875\n",
            "Batch 90 / 533 Regressor Loss : 129632.0625\n",
            "Batch 100 / 533 Regressor Loss : 143993.6875\n",
            "Batch 110 / 533 Regressor Loss : 122827.8125\n",
            "Batch 120 / 533 Regressor Loss : 111211.484375\n",
            "Batch 130 / 533 Regressor Loss : 140241.578125\n",
            "Batch 140 / 533 Regressor Loss : 112457.359375\n",
            "Batch 150 / 533 Regressor Loss : 112589.7109375\n",
            "Batch 160 / 533 Regressor Loss : 136746.921875\n",
            "Batch 170 / 533 Regressor Loss : 143691.625\n",
            "Batch 180 / 533 Regressor Loss : 125030.859375\n",
            "Batch 190 / 533 Regressor Loss : 146926.96875\n",
            "Batch 200 / 533 Regressor Loss : 141734.21875\n",
            "Batch 210 / 533 Regressor Loss : 111964.0546875\n",
            "Batch 220 / 533 Regressor Loss : 108570.1171875\n",
            "Batch 230 / 533 Regressor Loss : 133184.171875\n",
            "Batch 240 / 533 Regressor Loss : 111227.734375\n",
            "Batch 250 / 533 Regressor Loss : 144778.9375\n",
            "Batch 260 / 533 Regressor Loss : 126203.7734375\n",
            "Batch 270 / 533 Regressor Loss : 110921.0\n",
            "Batch 280 / 533 Regressor Loss : 135160.0\n",
            "Batch 290 / 533 Regressor Loss : 140003.375\n",
            "Batch 300 / 533 Regressor Loss : 142567.0625\n",
            "Batch 310 / 533 Regressor Loss : 138921.125\n",
            "Batch 320 / 533 Regressor Loss : 138876.125\n",
            "Batch 330 / 533 Regressor Loss : 175911.09375\n",
            "Batch 340 / 533 Regressor Loss : 109652.703125\n",
            "Batch 350 / 533 Regressor Loss : 114721.921875\n",
            "Batch 360 / 533 Regressor Loss : 133691.265625\n",
            "Batch 370 / 533 Regressor Loss : 127402.171875\n",
            "Batch 380 / 533 Regressor Loss : 125202.7734375\n",
            "Batch 390 / 533 Regressor Loss : 131127.25\n",
            "Batch 400 / 533 Regressor Loss : 114087.171875\n",
            "Batch 410 / 533 Regressor Loss : 114731.9609375\n",
            "Batch 420 / 533 Regressor Loss : 154584.859375\n",
            "Batch 430 / 533 Regressor Loss : 109109.546875\n",
            "Batch 440 / 533 Regressor Loss : 138924.609375\n",
            "Batch 450 / 533 Regressor Loss : 125784.2734375\n",
            "Batch 460 / 533 Regressor Loss : 121875.5859375\n",
            "Batch 470 / 533 Regressor Loss : 131025.53125\n",
            "Batch 480 / 533 Regressor Loss : 132644.484375\n",
            "Batch 490 / 533 Regressor Loss : 127868.921875\n",
            "Batch 500 / 533 Regressor Loss : 136567.359375\n",
            "Batch 510 / 533 Regressor Loss : 121423.8125\n",
            "Batch 520 / 533 Regressor Loss : 141307.9375\n",
            "Batch 530 / 533 Regressor Loss : 130281.984375\n",
            "Epoch  17 / 100  Sammon Loss : 0.3046169877052307  Regressor Loss : 175412.125\n",
            "Val Loss : 103961.609375\n",
            "Batch 10 / 996 Sammon Loss : 0.348817378282547\n",
            "Batch 20 / 996 Sammon Loss : 0.39586299657821655\n",
            "Batch 30 / 996 Sammon Loss : 0.5528454184532166\n",
            "Batch 40 / 996 Sammon Loss : 1.2294000387191772\n",
            "Batch 50 / 996 Sammon Loss : 0.9172654747962952\n",
            "Batch 60 / 996 Sammon Loss : 0.07883239537477493\n",
            "Batch 70 / 996 Sammon Loss : 0.12291055917739868\n",
            "Batch 80 / 996 Sammon Loss : 0.1425311416387558\n",
            "Batch 90 / 996 Sammon Loss : 0.04519304260611534\n",
            "Batch 100 / 996 Sammon Loss : 0.04039505496621132\n",
            "Batch 110 / 996 Sammon Loss : 0.018384914845228195\n",
            "Batch 120 / 996 Sammon Loss : 0.10209954530000687\n",
            "Batch 130 / 996 Sammon Loss : 0.004028654657304287\n",
            "Batch 140 / 996 Sammon Loss : 0.05743428319692612\n",
            "Batch 150 / 996 Sammon Loss : 0.0036336607299745083\n",
            "Batch 160 / 996 Sammon Loss : 0.016144920140504837\n",
            "Batch 170 / 996 Sammon Loss : 0.4859280288219452\n",
            "Batch 180 / 996 Sammon Loss : 0.25369012355804443\n",
            "Batch 190 / 996 Sammon Loss : 0.41471028327941895\n",
            "Batch 200 / 996 Sammon Loss : 0.21295586228370667\n",
            "Batch 210 / 996 Sammon Loss : 0.3082476854324341\n",
            "Batch 220 / 996 Sammon Loss : 0.09034281224012375\n",
            "Batch 230 / 996 Sammon Loss : 0.06170253828167915\n",
            "Batch 240 / 996 Sammon Loss : 0.11200668662786484\n",
            "Batch 250 / 996 Sammon Loss : 0.07851611822843552\n",
            "Batch 260 / 996 Sammon Loss : 0.012308330275118351\n",
            "Batch 270 / 996 Sammon Loss : 0.030966365709900856\n",
            "Batch 280 / 996 Sammon Loss : 0.02386641316115856\n",
            "Batch 290 / 996 Sammon Loss : 0.136982724070549\n",
            "Batch 300 / 996 Sammon Loss : 0.026798885315656662\n",
            "Batch 310 / 996 Sammon Loss : 0.055974334478378296\n",
            "Batch 320 / 996 Sammon Loss : 0.004273951053619385\n",
            "Batch 330 / 996 Sammon Loss : 0.06088273599743843\n",
            "Batch 340 / 996 Sammon Loss : 0.044775743037462234\n",
            "Batch 350 / 996 Sammon Loss : 0.006247549783438444\n",
            "Batch 360 / 996 Sammon Loss : 0.017748525366187096\n",
            "Batch 370 / 996 Sammon Loss : 0.11245974898338318\n",
            "Batch 380 / 996 Sammon Loss : 0.016434192657470703\n",
            "Batch 390 / 996 Sammon Loss : 0.08358185738325119\n",
            "Batch 400 / 996 Sammon Loss : 0.006586446426808834\n",
            "Batch 410 / 996 Sammon Loss : 0.3395689129829407\n",
            "Batch 420 / 996 Sammon Loss : 0.029742982238531113\n",
            "Batch 430 / 996 Sammon Loss : 0.11692529916763306\n",
            "Batch 440 / 996 Sammon Loss : 1.735817551612854\n",
            "Batch 450 / 996 Sammon Loss : 1.3682215213775635\n",
            "Batch 460 / 996 Sammon Loss : 0.6467912197113037\n",
            "Batch 470 / 996 Sammon Loss : 0.9904451966285706\n",
            "Batch 480 / 996 Sammon Loss : 0.017613964155316353\n",
            "Batch 490 / 996 Sammon Loss : 0.06334782391786575\n",
            "Batch 500 / 996 Sammon Loss : 0.07935891300439835\n",
            "Batch 510 / 996 Sammon Loss : 0.08470287919044495\n",
            "Batch 520 / 996 Sammon Loss : 0.23523087799549103\n",
            "Batch 530 / 996 Sammon Loss : 0.05122263729572296\n",
            "Batch 540 / 996 Sammon Loss : 0.021481750532984734\n",
            "Batch 550 / 996 Sammon Loss : 0.05017951503396034\n",
            "Batch 560 / 996 Sammon Loss : 0.48329320549964905\n",
            "Batch 570 / 996 Sammon Loss : 0.1966220587491989\n",
            "Batch 580 / 996 Sammon Loss : 0.12861011922359467\n",
            "Batch 590 / 996 Sammon Loss : 0.11202364414930344\n",
            "Batch 600 / 996 Sammon Loss : 0.009871350601315498\n",
            "Batch 610 / 996 Sammon Loss : 0.010869883000850677\n",
            "Batch 620 / 996 Sammon Loss : 0.0026678089052438736\n",
            "Batch 630 / 996 Sammon Loss : 0.048140350729227066\n",
            "Batch 640 / 996 Sammon Loss : 0.03548266738653183\n",
            "Batch 650 / 996 Sammon Loss : 0.039931777864694595\n",
            "Batch 660 / 996 Sammon Loss : 0.012310219928622246\n",
            "Batch 670 / 996 Sammon Loss : 0.008603288792073727\n",
            "Batch 680 / 996 Sammon Loss : 0.0358506441116333\n",
            "Batch 690 / 996 Sammon Loss : 0.1296980232000351\n",
            "Batch 700 / 996 Sammon Loss : 0.46019411087036133\n",
            "Batch 710 / 996 Sammon Loss : 0.057425301522016525\n",
            "Batch 720 / 996 Sammon Loss : 0.03581381216645241\n",
            "Batch 730 / 996 Sammon Loss : 0.012726493179798126\n",
            "Batch 740 / 996 Sammon Loss : 0.04381942003965378\n",
            "Batch 750 / 996 Sammon Loss : 0.2641753554344177\n",
            "Batch 760 / 996 Sammon Loss : 0.6067297458648682\n",
            "Batch 770 / 996 Sammon Loss : 0.5238189101219177\n",
            "Batch 780 / 996 Sammon Loss : 0.08815564960241318\n",
            "Batch 790 / 996 Sammon Loss : 0.0663556158542633\n",
            "Batch 800 / 996 Sammon Loss : 0.040857598185539246\n",
            "Batch 810 / 996 Sammon Loss : 0.09558994323015213\n",
            "Batch 820 / 996 Sammon Loss : 0.03156011551618576\n",
            "Batch 830 / 996 Sammon Loss : 0.07995663583278656\n",
            "Batch 840 / 996 Sammon Loss : 0.08731354773044586\n",
            "Batch 850 / 996 Sammon Loss : 0.07282628864049911\n",
            "Batch 860 / 996 Sammon Loss : 0.0019145015394315124\n",
            "Batch 870 / 996 Sammon Loss : 0.0565665028989315\n",
            "Batch 880 / 996 Sammon Loss : 0.01355814840644598\n",
            "Batch 890 / 996 Sammon Loss : 0.01601545698940754\n",
            "Batch 900 / 996 Sammon Loss : 0.01964319497346878\n",
            "Batch 910 / 996 Sammon Loss : 0.5307192206382751\n",
            "Batch 920 / 996 Sammon Loss : 0.08494897186756134\n",
            "Batch 930 / 996 Sammon Loss : 0.11853787302970886\n",
            "Batch 940 / 996 Sammon Loss : 0.4571523368358612\n",
            "Batch 950 / 996 Sammon Loss : 0.031093256548047066\n",
            "Batch 960 / 996 Sammon Loss : 0.01143194641917944\n",
            "Batch 970 / 996 Sammon Loss : 0.16954004764556885\n",
            "Batch 980 / 996 Sammon Loss : 0.05211907625198364\n",
            "Batch 990 / 996 Sammon Loss : 0.00949271209537983\n",
            "Batch 10 / 533 Regressor Loss : 142362.546875\n",
            "Batch 20 / 533 Regressor Loss : 115338.171875\n",
            "Batch 30 / 533 Regressor Loss : 137376.6875\n",
            "Batch 40 / 533 Regressor Loss : 144877.953125\n",
            "Batch 50 / 533 Regressor Loss : 132679.25\n",
            "Batch 60 / 533 Regressor Loss : 123927.375\n",
            "Batch 70 / 533 Regressor Loss : 130125.4375\n",
            "Batch 80 / 533 Regressor Loss : 118632.78125\n",
            "Batch 90 / 533 Regressor Loss : 129369.9609375\n",
            "Batch 100 / 533 Regressor Loss : 143698.296875\n",
            "Batch 110 / 533 Regressor Loss : 122584.875\n",
            "Batch 120 / 533 Regressor Loss : 110978.5859375\n",
            "Batch 130 / 533 Regressor Loss : 139955.84375\n",
            "Batch 140 / 533 Regressor Loss : 112212.421875\n",
            "Batch 150 / 533 Regressor Loss : 112337.296875\n",
            "Batch 160 / 533 Regressor Loss : 136499.375\n",
            "Batch 170 / 533 Regressor Loss : 143419.40625\n",
            "Batch 180 / 533 Regressor Loss : 124765.0234375\n",
            "Batch 190 / 533 Regressor Loss : 146662.40625\n",
            "Batch 200 / 533 Regressor Loss : 141465.890625\n",
            "Batch 210 / 533 Regressor Loss : 111717.8359375\n",
            "Batch 220 / 533 Regressor Loss : 108327.578125\n",
            "Batch 230 / 533 Regressor Loss : 132940.6875\n",
            "Batch 240 / 533 Regressor Loss : 110975.40625\n",
            "Batch 250 / 533 Regressor Loss : 144482.609375\n",
            "Batch 260 / 533 Regressor Loss : 125934.625\n",
            "Batch 270 / 533 Regressor Loss : 110664.578125\n",
            "Batch 280 / 533 Regressor Loss : 134880.5625\n",
            "Batch 290 / 533 Regressor Loss : 139715.734375\n",
            "Batch 300 / 533 Regressor Loss : 142316.984375\n",
            "Batch 310 / 533 Regressor Loss : 138647.875\n",
            "Batch 320 / 533 Regressor Loss : 138598.671875\n",
            "Batch 330 / 533 Regressor Loss : 175633.03125\n",
            "Batch 340 / 533 Regressor Loss : 109401.859375\n",
            "Batch 350 / 533 Regressor Loss : 114462.46875\n",
            "Batch 360 / 533 Regressor Loss : 133421.78125\n",
            "Batch 370 / 533 Regressor Loss : 127147.515625\n",
            "Batch 380 / 533 Regressor Loss : 124945.7734375\n",
            "Batch 390 / 533 Regressor Loss : 130862.7421875\n",
            "Batch 400 / 533 Regressor Loss : 113837.25\n",
            "Batch 410 / 533 Regressor Loss : 114483.046875\n",
            "Batch 420 / 533 Regressor Loss : 154305.375\n",
            "Batch 430 / 533 Regressor Loss : 108869.265625\n",
            "Batch 440 / 533 Regressor Loss : 138647.53125\n",
            "Batch 450 / 533 Regressor Loss : 125518.453125\n",
            "Batch 460 / 533 Regressor Loss : 121638.71875\n",
            "Batch 470 / 533 Regressor Loss : 130758.0859375\n",
            "Batch 480 / 533 Regressor Loss : 132359.59375\n",
            "Batch 490 / 533 Regressor Loss : 127604.5859375\n",
            "Batch 500 / 533 Regressor Loss : 136305.515625\n",
            "Batch 510 / 533 Regressor Loss : 121178.7109375\n",
            "Batch 520 / 533 Regressor Loss : 141018.265625\n",
            "Batch 530 / 533 Regressor Loss : 130013.2734375\n",
            "Epoch  18 / 100  Sammon Loss : 0.3046169877052307  Regressor Loss : 175083.4375\n",
            "Val Loss : 103698.7109375\n",
            "Batch 10 / 996 Sammon Loss : 0.348817378282547\n",
            "Batch 20 / 996 Sammon Loss : 0.39586299657821655\n",
            "Batch 30 / 996 Sammon Loss : 0.5528454184532166\n",
            "Batch 40 / 996 Sammon Loss : 1.2294000387191772\n",
            "Batch 50 / 996 Sammon Loss : 0.9172654747962952\n",
            "Batch 60 / 996 Sammon Loss : 0.07883239537477493\n",
            "Batch 70 / 996 Sammon Loss : 0.12291055917739868\n",
            "Batch 80 / 996 Sammon Loss : 0.1425311416387558\n",
            "Batch 90 / 996 Sammon Loss : 0.04519304260611534\n",
            "Batch 100 / 996 Sammon Loss : 0.04039505496621132\n",
            "Batch 110 / 996 Sammon Loss : 0.018384914845228195\n",
            "Batch 120 / 996 Sammon Loss : 0.10209954530000687\n",
            "Batch 130 / 996 Sammon Loss : 0.004028654657304287\n",
            "Batch 140 / 996 Sammon Loss : 0.05743428319692612\n",
            "Batch 150 / 996 Sammon Loss : 0.0036336607299745083\n",
            "Batch 160 / 996 Sammon Loss : 0.016144920140504837\n",
            "Batch 170 / 996 Sammon Loss : 0.4859280288219452\n",
            "Batch 180 / 996 Sammon Loss : 0.25369012355804443\n",
            "Batch 190 / 996 Sammon Loss : 0.41471028327941895\n",
            "Batch 200 / 996 Sammon Loss : 0.21295586228370667\n",
            "Batch 210 / 996 Sammon Loss : 0.3082476854324341\n",
            "Batch 220 / 996 Sammon Loss : 0.09034281224012375\n",
            "Batch 230 / 996 Sammon Loss : 0.06170253828167915\n",
            "Batch 240 / 996 Sammon Loss : 0.11200668662786484\n",
            "Batch 250 / 996 Sammon Loss : 0.07851611822843552\n",
            "Batch 260 / 996 Sammon Loss : 0.012308330275118351\n",
            "Batch 270 / 996 Sammon Loss : 0.030966365709900856\n",
            "Batch 280 / 996 Sammon Loss : 0.02386641316115856\n",
            "Batch 290 / 996 Sammon Loss : 0.136982724070549\n",
            "Batch 300 / 996 Sammon Loss : 0.026798885315656662\n",
            "Batch 310 / 996 Sammon Loss : 0.055974334478378296\n",
            "Batch 320 / 996 Sammon Loss : 0.004273951053619385\n",
            "Batch 330 / 996 Sammon Loss : 0.06088273599743843\n",
            "Batch 340 / 996 Sammon Loss : 0.044775743037462234\n",
            "Batch 350 / 996 Sammon Loss : 0.006247549783438444\n",
            "Batch 360 / 996 Sammon Loss : 0.017748525366187096\n",
            "Batch 370 / 996 Sammon Loss : 0.11245974898338318\n",
            "Batch 380 / 996 Sammon Loss : 0.016434192657470703\n",
            "Batch 390 / 996 Sammon Loss : 0.08358185738325119\n",
            "Batch 400 / 996 Sammon Loss : 0.006586446426808834\n",
            "Batch 410 / 996 Sammon Loss : 0.3395689129829407\n",
            "Batch 420 / 996 Sammon Loss : 0.029742982238531113\n",
            "Batch 430 / 996 Sammon Loss : 0.11692529916763306\n",
            "Batch 440 / 996 Sammon Loss : 1.735817551612854\n",
            "Batch 450 / 996 Sammon Loss : 1.3682215213775635\n",
            "Batch 460 / 996 Sammon Loss : 0.6467912197113037\n",
            "Batch 470 / 996 Sammon Loss : 0.9904451966285706\n",
            "Batch 480 / 996 Sammon Loss : 0.017613964155316353\n",
            "Batch 490 / 996 Sammon Loss : 0.06334782391786575\n",
            "Batch 500 / 996 Sammon Loss : 0.07935891300439835\n",
            "Batch 510 / 996 Sammon Loss : 0.08470287919044495\n",
            "Batch 520 / 996 Sammon Loss : 0.23523087799549103\n",
            "Batch 530 / 996 Sammon Loss : 0.05122263729572296\n",
            "Batch 540 / 996 Sammon Loss : 0.021481750532984734\n",
            "Batch 550 / 996 Sammon Loss : 0.05017951503396034\n",
            "Batch 560 / 996 Sammon Loss : 0.48329320549964905\n",
            "Batch 570 / 996 Sammon Loss : 0.1966220587491989\n",
            "Batch 580 / 996 Sammon Loss : 0.12861011922359467\n",
            "Batch 590 / 996 Sammon Loss : 0.11202364414930344\n",
            "Batch 600 / 996 Sammon Loss : 0.009871350601315498\n",
            "Batch 610 / 996 Sammon Loss : 0.010869883000850677\n",
            "Batch 620 / 996 Sammon Loss : 0.0026678089052438736\n",
            "Batch 630 / 996 Sammon Loss : 0.048140350729227066\n",
            "Batch 640 / 996 Sammon Loss : 0.03548266738653183\n",
            "Batch 650 / 996 Sammon Loss : 0.039931777864694595\n",
            "Batch 660 / 996 Sammon Loss : 0.012310219928622246\n",
            "Batch 670 / 996 Sammon Loss : 0.008603288792073727\n",
            "Batch 680 / 996 Sammon Loss : 0.0358506441116333\n",
            "Batch 690 / 996 Sammon Loss : 0.1296980232000351\n",
            "Batch 700 / 996 Sammon Loss : 0.46019411087036133\n",
            "Batch 710 / 996 Sammon Loss : 0.057425301522016525\n",
            "Batch 720 / 996 Sammon Loss : 0.03581381216645241\n",
            "Batch 730 / 996 Sammon Loss : 0.012726493179798126\n",
            "Batch 740 / 996 Sammon Loss : 0.04381942003965378\n",
            "Batch 750 / 996 Sammon Loss : 0.2641753554344177\n",
            "Batch 760 / 996 Sammon Loss : 0.6067297458648682\n",
            "Batch 770 / 996 Sammon Loss : 0.5238189101219177\n",
            "Batch 780 / 996 Sammon Loss : 0.08815564960241318\n",
            "Batch 790 / 996 Sammon Loss : 0.0663556158542633\n",
            "Batch 800 / 996 Sammon Loss : 0.040857598185539246\n",
            "Batch 810 / 996 Sammon Loss : 0.09558994323015213\n",
            "Batch 820 / 996 Sammon Loss : 0.03156011551618576\n",
            "Batch 830 / 996 Sammon Loss : 0.07995663583278656\n",
            "Batch 840 / 996 Sammon Loss : 0.08731354773044586\n",
            "Batch 850 / 996 Sammon Loss : 0.07282628864049911\n",
            "Batch 860 / 996 Sammon Loss : 0.0019145015394315124\n",
            "Batch 870 / 996 Sammon Loss : 0.0565665028989315\n",
            "Batch 880 / 996 Sammon Loss : 0.01355814840644598\n",
            "Batch 890 / 996 Sammon Loss : 0.01601545698940754\n",
            "Batch 900 / 996 Sammon Loss : 0.01964319497346878\n",
            "Batch 910 / 996 Sammon Loss : 0.5307192206382751\n",
            "Batch 920 / 996 Sammon Loss : 0.08494897186756134\n",
            "Batch 930 / 996 Sammon Loss : 0.11853787302970886\n",
            "Batch 940 / 996 Sammon Loss : 0.4571523368358612\n",
            "Batch 950 / 996 Sammon Loss : 0.031093256548047066\n",
            "Batch 960 / 996 Sammon Loss : 0.01143194641917944\n",
            "Batch 970 / 996 Sammon Loss : 0.16954004764556885\n",
            "Batch 980 / 996 Sammon Loss : 0.05211907625198364\n",
            "Batch 990 / 996 Sammon Loss : 0.00949271209537983\n",
            "Batch 10 / 533 Regressor Loss : 142099.359375\n",
            "Batch 20 / 533 Regressor Loss : 115083.9375\n",
            "Batch 30 / 533 Regressor Loss : 137102.34375\n",
            "Batch 40 / 533 Regressor Loss : 144606.8125\n",
            "Batch 50 / 533 Regressor Loss : 132429.828125\n",
            "Batch 60 / 533 Regressor Loss : 123660.7734375\n",
            "Batch 70 / 533 Regressor Loss : 129844.8359375\n",
            "Batch 80 / 533 Regressor Loss : 118376.6171875\n",
            "Batch 90 / 533 Regressor Loss : 129108.3125\n",
            "Batch 100 / 533 Regressor Loss : 143403.40625\n",
            "Batch 110 / 533 Regressor Loss : 122342.421875\n",
            "Batch 120 / 533 Regressor Loss : 110746.125\n",
            "Batch 130 / 533 Regressor Loss : 139670.609375\n",
            "Batch 140 / 533 Regressor Loss : 111967.96875\n",
            "Batch 150 / 533 Regressor Loss : 112085.34375\n",
            "Batch 160 / 533 Regressor Loss : 136252.296875\n",
            "Batch 170 / 533 Regressor Loss : 143147.640625\n",
            "Batch 180 / 533 Regressor Loss : 124499.65625\n",
            "Batch 190 / 533 Regressor Loss : 146398.3125\n",
            "Batch 200 / 533 Regressor Loss : 141198.046875\n",
            "Batch 210 / 533 Regressor Loss : 111472.0859375\n",
            "Batch 220 / 533 Regressor Loss : 108085.5\n",
            "Batch 230 / 533 Regressor Loss : 132697.65625\n",
            "Batch 240 / 533 Regressor Loss : 110723.5546875\n",
            "Batch 250 / 533 Regressor Loss : 144186.78125\n",
            "Batch 260 / 533 Regressor Loss : 125665.953125\n",
            "Batch 270 / 533 Regressor Loss : 110408.625\n",
            "Batch 280 / 533 Regressor Loss : 134601.625\n",
            "Batch 290 / 533 Regressor Loss : 139428.5625\n",
            "Batch 300 / 533 Regressor Loss : 142067.359375\n",
            "Batch 310 / 533 Regressor Loss : 138375.125\n",
            "Batch 320 / 533 Regressor Loss : 138321.65625\n",
            "Batch 330 / 533 Regressor Loss : 175355.46875\n",
            "Batch 340 / 533 Regressor Loss : 109151.484375\n",
            "Batch 350 / 533 Regressor Loss : 114203.484375\n",
            "Batch 360 / 533 Regressor Loss : 133152.78125\n",
            "Batch 370 / 533 Regressor Loss : 126893.3125\n",
            "Batch 380 / 533 Regressor Loss : 124689.265625\n",
            "Batch 390 / 533 Regressor Loss : 130598.703125\n",
            "Batch 400 / 533 Regressor Loss : 113587.8125\n",
            "Batch 410 / 533 Regressor Loss : 114234.5859375\n",
            "Batch 420 / 533 Regressor Loss : 154026.375\n",
            "Batch 430 / 533 Regressor Loss : 108629.4296875\n",
            "Batch 440 / 533 Regressor Loss : 138370.90625\n",
            "Batch 450 / 533 Regressor Loss : 125253.109375\n",
            "Batch 460 / 533 Regressor Loss : 121402.296875\n",
            "Batch 470 / 533 Regressor Loss : 130491.125\n",
            "Batch 480 / 533 Regressor Loss : 132075.171875\n",
            "Batch 490 / 533 Regressor Loss : 127340.75\n",
            "Batch 500 / 533 Regressor Loss : 136044.125\n",
            "Batch 510 / 533 Regressor Loss : 120934.109375\n",
            "Batch 520 / 533 Regressor Loss : 140729.0625\n",
            "Batch 530 / 533 Regressor Loss : 129745.046875\n",
            "Epoch  19 / 100  Sammon Loss : 0.3046169877052307  Regressor Loss : 174755.296875\n",
            "Val Loss : 103436.3046875\n",
            "Batch 10 / 996 Sammon Loss : 0.348817378282547\n",
            "Batch 20 / 996 Sammon Loss : 0.39586299657821655\n",
            "Batch 30 / 996 Sammon Loss : 0.5528454184532166\n",
            "Batch 40 / 996 Sammon Loss : 1.2294000387191772\n",
            "Batch 50 / 996 Sammon Loss : 0.9172654747962952\n",
            "Batch 60 / 996 Sammon Loss : 0.07883239537477493\n",
            "Batch 70 / 996 Sammon Loss : 0.12291055917739868\n",
            "Batch 80 / 996 Sammon Loss : 0.1425311416387558\n",
            "Batch 90 / 996 Sammon Loss : 0.04519304260611534\n",
            "Batch 100 / 996 Sammon Loss : 0.04039505496621132\n",
            "Batch 110 / 996 Sammon Loss : 0.018384914845228195\n",
            "Batch 120 / 996 Sammon Loss : 0.10209954530000687\n",
            "Batch 130 / 996 Sammon Loss : 0.004028654657304287\n",
            "Batch 140 / 996 Sammon Loss : 0.05743428319692612\n",
            "Batch 150 / 996 Sammon Loss : 0.0036336607299745083\n",
            "Batch 160 / 996 Sammon Loss : 0.016144920140504837\n",
            "Batch 170 / 996 Sammon Loss : 0.4859280288219452\n",
            "Batch 180 / 996 Sammon Loss : 0.25369012355804443\n",
            "Batch 190 / 996 Sammon Loss : 0.41471028327941895\n",
            "Batch 200 / 996 Sammon Loss : 0.21295586228370667\n",
            "Batch 210 / 996 Sammon Loss : 0.3082476854324341\n",
            "Batch 220 / 996 Sammon Loss : 0.09034281224012375\n",
            "Batch 230 / 996 Sammon Loss : 0.06170253828167915\n",
            "Batch 240 / 996 Sammon Loss : 0.11200668662786484\n",
            "Batch 250 / 996 Sammon Loss : 0.07851611822843552\n",
            "Batch 260 / 996 Sammon Loss : 0.012308330275118351\n",
            "Batch 270 / 996 Sammon Loss : 0.030966365709900856\n",
            "Batch 280 / 996 Sammon Loss : 0.02386641316115856\n",
            "Batch 290 / 996 Sammon Loss : 0.136982724070549\n",
            "Batch 300 / 996 Sammon Loss : 0.026798885315656662\n",
            "Batch 310 / 996 Sammon Loss : 0.055974334478378296\n",
            "Batch 320 / 996 Sammon Loss : 0.004273951053619385\n",
            "Batch 330 / 996 Sammon Loss : 0.06088273599743843\n",
            "Batch 340 / 996 Sammon Loss : 0.044775743037462234\n",
            "Batch 350 / 996 Sammon Loss : 0.006247549783438444\n",
            "Batch 360 / 996 Sammon Loss : 0.017748525366187096\n",
            "Batch 370 / 996 Sammon Loss : 0.11245974898338318\n",
            "Batch 380 / 996 Sammon Loss : 0.016434192657470703\n",
            "Batch 390 / 996 Sammon Loss : 0.08358185738325119\n",
            "Batch 400 / 996 Sammon Loss : 0.006586446426808834\n",
            "Batch 410 / 996 Sammon Loss : 0.3395689129829407\n",
            "Batch 420 / 996 Sammon Loss : 0.029742982238531113\n",
            "Batch 430 / 996 Sammon Loss : 0.11692529916763306\n",
            "Batch 440 / 996 Sammon Loss : 1.735817551612854\n",
            "Batch 450 / 996 Sammon Loss : 1.3682215213775635\n",
            "Batch 460 / 996 Sammon Loss : 0.6467912197113037\n",
            "Batch 470 / 996 Sammon Loss : 0.9904451966285706\n",
            "Batch 480 / 996 Sammon Loss : 0.017613964155316353\n",
            "Batch 490 / 996 Sammon Loss : 0.06334782391786575\n",
            "Batch 500 / 996 Sammon Loss : 0.07935891300439835\n",
            "Batch 510 / 996 Sammon Loss : 0.08470287919044495\n",
            "Batch 520 / 996 Sammon Loss : 0.23523087799549103\n",
            "Batch 530 / 996 Sammon Loss : 0.05122263729572296\n",
            "Batch 540 / 996 Sammon Loss : 0.021481750532984734\n",
            "Batch 550 / 996 Sammon Loss : 0.05017951503396034\n",
            "Batch 560 / 996 Sammon Loss : 0.48329320549964905\n",
            "Batch 570 / 996 Sammon Loss : 0.1966220587491989\n",
            "Batch 580 / 996 Sammon Loss : 0.12861011922359467\n",
            "Batch 590 / 996 Sammon Loss : 0.11202364414930344\n",
            "Batch 600 / 996 Sammon Loss : 0.009871350601315498\n",
            "Batch 610 / 996 Sammon Loss : 0.010869883000850677\n",
            "Batch 620 / 996 Sammon Loss : 0.0026678089052438736\n",
            "Batch 630 / 996 Sammon Loss : 0.048140350729227066\n",
            "Batch 640 / 996 Sammon Loss : 0.03548266738653183\n",
            "Batch 650 / 996 Sammon Loss : 0.039931777864694595\n",
            "Batch 660 / 996 Sammon Loss : 0.012310219928622246\n",
            "Batch 670 / 996 Sammon Loss : 0.008603288792073727\n",
            "Batch 680 / 996 Sammon Loss : 0.0358506441116333\n",
            "Batch 690 / 996 Sammon Loss : 0.1296980232000351\n",
            "Batch 700 / 996 Sammon Loss : 0.46019411087036133\n",
            "Batch 710 / 996 Sammon Loss : 0.057425301522016525\n",
            "Batch 720 / 996 Sammon Loss : 0.03581381216645241\n",
            "Batch 730 / 996 Sammon Loss : 0.012726493179798126\n",
            "Batch 740 / 996 Sammon Loss : 0.04381942003965378\n",
            "Batch 750 / 996 Sammon Loss : 0.2641753554344177\n",
            "Batch 760 / 996 Sammon Loss : 0.6067297458648682\n",
            "Batch 770 / 996 Sammon Loss : 0.5238189101219177\n",
            "Batch 780 / 996 Sammon Loss : 0.08815564960241318\n",
            "Batch 790 / 996 Sammon Loss : 0.0663556158542633\n",
            "Batch 800 / 996 Sammon Loss : 0.040857598185539246\n",
            "Batch 810 / 996 Sammon Loss : 0.09558994323015213\n",
            "Batch 820 / 996 Sammon Loss : 0.03156011551618576\n",
            "Batch 830 / 996 Sammon Loss : 0.07995663583278656\n",
            "Batch 840 / 996 Sammon Loss : 0.08731354773044586\n",
            "Batch 850 / 996 Sammon Loss : 0.07282628864049911\n",
            "Batch 860 / 996 Sammon Loss : 0.0019145015394315124\n",
            "Batch 870 / 996 Sammon Loss : 0.0565665028989315\n",
            "Batch 880 / 996 Sammon Loss : 0.01355814840644598\n",
            "Batch 890 / 996 Sammon Loss : 0.01601545698940754\n",
            "Batch 900 / 996 Sammon Loss : 0.01964319497346878\n",
            "Batch 910 / 996 Sammon Loss : 0.5307192206382751\n",
            "Batch 920 / 996 Sammon Loss : 0.08494897186756134\n",
            "Batch 930 / 996 Sammon Loss : 0.11853787302970886\n",
            "Batch 940 / 996 Sammon Loss : 0.4571523368358612\n",
            "Batch 950 / 996 Sammon Loss : 0.031093256548047066\n",
            "Batch 960 / 996 Sammon Loss : 0.01143194641917944\n",
            "Batch 970 / 996 Sammon Loss : 0.16954004764556885\n",
            "Batch 980 / 996 Sammon Loss : 0.05211907625198364\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-93c6cfd547f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mloss2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mYm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mopt1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mbatch1\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9XdPuF1wTAH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "34cb9a52-83cf-4ece-e856-610de038cdc0"
      },
      "source": [
        "pairs[2:9,0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 3, 4, 5, 6, 7, 8])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    }
  ]
}