{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Siamese2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kkuntal990/CTW_2020/blob/master/Siamese2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjEwmtrg7sBN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "da91eb7d-1ccc-4545-84f9-55014a581e35"
      },
      "source": [
        "import psutil\n",
        "def get_size(bytes, suffix=\"B\"):\n",
        "    factor = 1024\n",
        "    for unit in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\"]:\n",
        "        if bytes < factor:\n",
        "            return f\"{bytes:.2f}{unit}{suffix}\"zz\n",
        "        bytes /= factor\n",
        "print(\"=\"*40, \"Memory Information\", \"=\"*40)\n",
        "svmem = psutil.virtual_memory()\n",
        "print(f\"Total: {get_size(svmem.total)}\") ; print(f\"Available: {get_size(svmem.available)}\")\n",
        "print(f\"Used: {get_size(svmem.used)}\") ; print(f\"Percentage: {svmem.percent}%\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======================================== Memory Information ========================================\n",
            "Total: 25.51GB\n",
            "Available: 24.54GB\n",
            "Used: 639.60MB\n",
            "Percentage: 3.8%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmZKU7tvakmQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import os\n",
        "from torchvision.utils import save_image\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL-9c6qz7zUQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "8d9f4e9b-eded-4898-a548-224d2a4cf9a9"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "! nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Jul 10 08:47:49 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqpFkedI8Kbu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "02f5d62f-75f1-448c-edae-3157956ed721"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fesmFnEKGskw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import h5py\n",
        "f = h5py.File(\"/content/gdrive/My Drive/CTW2020/Processed Data/udata2020.hdf5\",\"r\")\n",
        "import numpy as np\n",
        "_ , key = next(enumerate(f.keys()))\n",
        "X = f[key][:]\n",
        "f.close()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TZMmdAe-bDJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c2e42ccb-a2f0-461e-8e43-2ab244e89e8f"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(36192, 56, 924, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hos9jEq9cW6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.sqrt(X[:,:,:,0]**2 + X[:,:,:,1]**2)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYGpSuWAPoil",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "63ae9e24-37d7-4d08-c00a-7dec5a2d0641"
      },
      "source": [
        "import h5py\n",
        "f = h5py.File(\"/content/gdrive/My Drive/CTW2020/Processed Data/Labelled_1.hdf5\",\"r\")\n",
        "\n",
        "H = f[\"H_Est\"][:]\n",
        "Pos = f[\"Pos\"][:]\n",
        "SNR = f[\"SNR\"][:]\n",
        "f.close()\n",
        "\n",
        "print(H.shape)\n",
        "print(SNR.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4979, 56, 924, 2)\n",
            "(4979, 56)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOk-lnxv9MRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "H = np.sqrt(H[:,:,:,0]**2 + H[:,:,:,1]**2)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0U4rJbdh9nKb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "H_Train, H_Test , Pos_Train , Pos_Test = train_test_split(H,Pos,test_size=0.1, random_state=42)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCu8RSztap10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "H_Train, H_Val , Pos_Train , Pos_Val = train_test_split(H_Train,Pos_Train,test_size=0.1, random_state=99)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVmnIvvEVInY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "10ad69b0-25a3-470b-c86e-30713cbc94d8"
      },
      "source": [
        "print(H_Train.shape, H_Val.shape, H_Test.shape)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4032, 56, 924) (449, 56, 924) (498, 56, 924)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6el12NxbqAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "H_Train = torch.tensor(H_Train,dtype=torch.float)\n",
        "Pos_Train = torch.tensor(Pos_Train,dtype=torch.float)\n",
        "H_Val = torch.tensor(H_Val,dtype=torch.float)\n",
        "Pos_Val = torch.tensor(Pos_Val,dtype=torch.float)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3ue3vK55J8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sammon(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Sammon, self).__init__()\n",
        "        \n",
        "        \n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(1,2, (1,8), stride=(1,2)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(2, 4, (1,8), stride=(1,4)),\n",
        "            nn.ReLU())\n",
        "        \n",
        "        self.Project = nn.Sequential(nn.Linear(25312,12000),\n",
        "                                     nn.Tanh(),\n",
        "                                     nn.Dropout(p=0.2),\n",
        "                                     nn.Linear(12000,7500),\n",
        "                                     nn.Softplus(),\n",
        "                                     nn.Dropout(p=0.2),\n",
        "                                     nn.Linear(7500,4500),\n",
        "                                     nn.Tanh(),\n",
        "                                     nn.Dropout(p=0.2),\n",
        "                                     nn.Linear(4500,1024),\n",
        "                                     nn.Softplus(),\n",
        "                                     nn.Dropout(p=0.2),\n",
        "                                     nn.Linear(1024,256),\n",
        "                                     nn.Tanh(),\n",
        "                                     nn.Dropout(p=0.2),\n",
        "                                     nn.Linear(256,128),\n",
        "                                     nn.Tanh(),\n",
        "                                     nn.Dropout(p=0.1),\n",
        "                                     nn.Linear(128,64),\n",
        "                                     nn.Tanh(),\n",
        "                                     nn.Dropout(p=0.1),\n",
        "                                     nn.Linear(64,16),\n",
        "                                     nn.Tanh(),\n",
        "                                     nn.Dropout(p=0.1),\n",
        "                                     nn.Linear(16,8),\n",
        "                                     nn.Tanh(),\n",
        "                                     nn.Linear(8,3))\n",
        "        self.alpha = nn.Parameter(torch.rand(1))\n",
        "                                     \n",
        "          \n",
        "            \n",
        "    def forward(self, input):\n",
        "        return (self.aplha)*(self.Project(self.main(input).view(-1,25312)).view(-1,3))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7yfKTDJA-O7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
        "sammon = Sammon().to(device)\n",
        "#reg = Regressor().to(device)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkGd-RUyBxdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = optim.Adam([{'params': model.main.parameters()},{'params': model.Project.parameters()},{'params': model.alpha.parameters(),'lr':1e-2}], lr=1e-4)\n",
        "MSE = nn.MSELoss()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd77lbbE9EIg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "outputId": "7c81c973-cc93-44e6-d925-0a53e7d01f4d"
      },
      "source": [
        "summary(sammon, (1, 56,924))\n",
        "summary(reg, (4500,))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 2, 56, 459]              18\n",
            "              ReLU-2           [-1, 2, 56, 459]               0\n",
            "            Conv2d-3           [-1, 4, 56, 113]              68\n",
            "              ReLU-4           [-1, 4, 56, 113]               0\n",
            "            Linear-5                [-1, 12000]     303,756,000\n",
            "              Tanh-6                [-1, 12000]               0\n",
            "            Linear-7                 [-1, 7500]      90,007,500\n",
            "          Softplus-8                 [-1, 7500]               0\n",
            "            Linear-9                 [-1, 4500]      33,754,500\n",
            "             Tanh-10                 [-1, 4500]               0\n",
            "================================================================\n",
            "Total params: 427,518,086\n",
            "Trainable params: 427,518,086\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.20\n",
            "Forward/backward pass size (MB): 1.54\n",
            "Params size (MB): 1630.85\n",
            "Estimated Total Size (MB): 1632.59\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                 [-1, 2048]       9,218,048\n",
            "              Tanh-2                 [-1, 2048]               0\n",
            "            Linear-3                 [-1, 1024]       2,098,176\n",
            "          Softplus-4                 [-1, 1024]               0\n",
            "            Linear-5                  [-1, 512]         524,800\n",
            "              Tanh-6                  [-1, 512]               0\n",
            "            Linear-7                  [-1, 256]         131,328\n",
            "              Tanh-8                  [-1, 256]               0\n",
            "            Linear-9                  [-1, 128]          32,896\n",
            "             Tanh-10                  [-1, 128]               0\n",
            "           Linear-11                   [-1, 64]           8,256\n",
            "             Tanh-12                   [-1, 64]               0\n",
            "           Linear-13                   [-1, 32]           2,080\n",
            "         Softplus-14                   [-1, 32]               0\n",
            "           Linear-15                   [-1, 16]             528\n",
            "             Tanh-16                   [-1, 16]               0\n",
            "           Linear-17                    [-1, 8]             136\n",
            "             Tanh-18                    [-1, 8]               0\n",
            "           Linear-19                    [-1, 3]              27\n",
            "================================================================\n",
            "Total params: 12,016,275\n",
            "Trainable params: 12,016,275\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.02\n",
            "Forward/backward pass size (MB): 0.06\n",
            "Params size (MB): 45.84\n",
            "Estimated Total Size (MB): 45.92\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpK2iU41cZEL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "A = list(range(len(X)))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnxcS8sie2Ix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process(indices):\n",
        "    # data of size 500\n",
        "\n",
        "    pairs = []\n",
        "    for i in range(len(indices)):\n",
        "        for j in range(i+1,len(indices)):\n",
        "            pairs.append([i,j])\n",
        "\n",
        "    return pairs\n",
        "\n",
        "\n",
        "import random\n",
        "def chunk(xs, n):\n",
        "    ys = list(xs)\n",
        "    random.shuffle(ys)\n",
        "    chunk_length = len(ys) // n\n",
        "    needs_extra = len(ys) % n\n",
        "    start = 0\n",
        "    for i in range(n):\n",
        "        if i < needs_extra:\n",
        "            end = start + chunk_length + 1\n",
        "        else:\n",
        "            end = start + chunk_length\n",
        "        yield ys[start:end]\n",
        "        start = end\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J9Kb_KS7kHc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f99e1e6c-c1e6-4d28-ddaa-67c3e577709a"
      },
      "source": [
        "pairs.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(72384, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JNeMIPNcPVR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "569663e9-4cb2-4bf9-a7f2-1d354b613593"
      },
      "source": [
        "N_b2 = len(H_Train)\n",
        "N_b = 72\n",
        "N_val = len(H_Val)\n",
        "bs1= 512\n",
        "bs2 = 8\n",
        "epsilon = 1e-6\n",
        "\n",
        "print(N_b2, N_b, N_val)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4032 72384 449\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvqLGR37VRxI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1c008eeb-0dae-4505-f2cf-4c96c4d5d228"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(36192, 56, 924)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RB2wmMna1tFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_loss(XX, YY):\n",
        "  return torch.sum((1/XX)*(torch.square(XX-YY)))\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UKR9fGq5HOW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bfa5d549-e25a-4576-d968-18c5de547bd1"
      },
      "source": [
        "for i in range(100):\n",
        "  batch1=0\n",
        "  batch2=0\n",
        "  for ch in chunks(A,72):\n",
        "    pairs = process(ch)\n",
        "    for j in range(0,len(pairs),bs1):\n",
        "      sammon.zero_grad()\n",
        "      end = min(j+bs1,len(pairs))\n",
        "      Xn,Xm = torch.tensor(X[pairs[j:end,0]],dtype=torch.float),  torch.tensor(X[pairs[j:end,1]],dtype=torch.float)\n",
        "      Xn , Xm = Variable(Xn, requires_grad=True) , Variable(Xm, requires_grad=True)\n",
        "      Xn = Xn.to(device)\n",
        "      Xm = Xm.to(device)\n",
        "      Yn = sammon(Xn)\n",
        "      Ym = sammon(Xm)\n",
        "      \n",
        "      XX = torch.norm((Xn-Xm),p=2, dim = [1,2])\n",
        "      YY = torch.norm((Yn-Ym),p=2, dim = 1)\n",
        "      XX = torch.sqrt(torch.square(XX) + epsilon)\n",
        "      YY = torch.sqrt(torch.square(YY) + epsilon)\n",
        "      loss = custom_loss(XX, YY)\n",
        "      #print(loss)\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "\n",
        "\n",
        "    batch1+=1\n",
        "    print(\"Batch\",batch1,\"/\",(len(A)//72)+1,\"Sammon Loss :\" , loss.item())\n",
        "\n",
        "  for k in range(0,N_b2,bs2):\n",
        "    end = min(k+bs2,N_b2)\n",
        "    H1 = H_Train[k:end]\n",
        "    sammon.zero_grad()\n",
        "    reg.zero_grad()\n",
        "    H1 = Variable(H1,requires_grad=True)\n",
        "    H1 = H1.to(device)\n",
        "    Y1 = reg(sammon(H1.view(-1,1,56,924)))\n",
        "    loss_1 = MSE(Y1,Pos_Train[k:end].to(device)).view(-1)\n",
        "    loss_1.backward()\n",
        "    opt1.step()\n",
        "    opt2.step()\n",
        "    batch2+=1\n",
        "    if(batch2%10 == 0):\n",
        "      print(\"Batch\",batch2,\"/\",(N_b2//bs2)+1,\"Regressor Loss :\" , loss_1.item())\n",
        "  print(\"Epoch \",i,\"/\",100,\" Sammon Loss :\",loss.item() ,\" Regressor Loss :\" , loss_1.item())\n",
        "  for l in range(N_val):\n",
        "    H_val = torch.tensor(H_Val[l],dtype=torch.float)\n",
        "    H_val = H_val.to(device)\n",
        "    with torch.no_grad():\n",
        "      Y_Val = reg(sammon(H_val.view(-1,1,56,924)))\n",
        "      loss_Val = MSE(Y_Val,Pos_Val[l].to(device)).view(-1)\n",
        "  print(\"Val Loss :\" , loss_Val.item())\n",
        "\n",
        "    \n",
        "\n",
        "      \n",
        "  \n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch 10 / 1132 Sammon Loss : 1895.8427734375\n",
            "Batch 20 / 1132 Sammon Loss : 1176.537353515625\n",
            "Batch 30 / 1132 Sammon Loss : 1350.989501953125\n",
            "Batch 40 / 1132 Sammon Loss : 1379.025146484375\n",
            "Batch 50 / 1132 Sammon Loss : 1583.758056640625\n",
            "Batch 60 / 1132 Sammon Loss : 764.25634765625\n",
            "Batch 70 / 1132 Sammon Loss : 629.0015869140625\n",
            "Batch 80 / 1132 Sammon Loss : 591.71240234375\n",
            "Batch 90 / 1132 Sammon Loss : 598.542724609375\n",
            "Batch 100 / 1132 Sammon Loss : 461.2624816894531\n",
            "Batch 110 / 1132 Sammon Loss : 264.6654357910156\n",
            "Batch 120 / 1132 Sammon Loss : 155.77392578125\n",
            "Batch 130 / 1132 Sammon Loss : 419.4209899902344\n",
            "Batch 140 / 1132 Sammon Loss : 705.1890258789062\n",
            "Batch 150 / 1132 Sammon Loss : 582.5146484375\n",
            "Batch 160 / 1132 Sammon Loss : 469.21356201171875\n",
            "Batch 170 / 1132 Sammon Loss : 1361.4617919921875\n",
            "Batch 180 / 1132 Sammon Loss : 1397.240966796875\n",
            "Batch 190 / 1132 Sammon Loss : 1167.712890625\n",
            "Batch 200 / 1132 Sammon Loss : 1064.260986328125\n",
            "Batch 210 / 1132 Sammon Loss : 1087.994140625\n",
            "Batch 220 / 1132 Sammon Loss : 781.423583984375\n",
            "Batch 230 / 1132 Sammon Loss : 747.1761474609375\n",
            "Batch 240 / 1132 Sammon Loss : 458.517578125\n",
            "Batch 250 / 1132 Sammon Loss : 473.7816162109375\n",
            "Batch 260 / 1132 Sammon Loss : 374.1361083984375\n",
            "Batch 270 / 1132 Sammon Loss : 554.7432861328125\n",
            "Batch 280 / 1132 Sammon Loss : 504.7508544921875\n",
            "Batch 290 / 1132 Sammon Loss : 435.60491943359375\n",
            "Batch 300 / 1132 Sammon Loss : 1341.2381591796875\n",
            "Batch 310 / 1132 Sammon Loss : 547.75\n",
            "Batch 320 / 1132 Sammon Loss : 558.3423461914062\n",
            "Batch 330 / 1132 Sammon Loss : 728.68701171875\n",
            "Batch 340 / 1132 Sammon Loss : 518.4217529296875\n",
            "Batch 350 / 1132 Sammon Loss : 1143.73876953125\n",
            "Batch 360 / 1132 Sammon Loss : 1166.67822265625\n",
            "Batch 370 / 1132 Sammon Loss : 1316.58740234375\n",
            "Batch 380 / 1132 Sammon Loss : 1009.3970336914062\n",
            "Batch 390 / 1132 Sammon Loss : 1240.1158447265625\n",
            "Batch 400 / 1132 Sammon Loss : 1396.105224609375\n",
            "Batch 410 / 1132 Sammon Loss : 1168.9901123046875\n",
            "Batch 420 / 1132 Sammon Loss : 1223.302490234375\n",
            "Batch 430 / 1132 Sammon Loss : 924.4456176757812\n",
            "Batch 440 / 1132 Sammon Loss : 1960.3623046875\n",
            "Batch 450 / 1132 Sammon Loss : 1032.302490234375\n",
            "Batch 460 / 1132 Sammon Loss : 1435.964599609375\n",
            "Batch 470 / 1132 Sammon Loss : 1201.940673828125\n",
            "Batch 480 / 1132 Sammon Loss : 1665.9307861328125\n",
            "Batch 490 / 1132 Sammon Loss : 847.6507568359375\n",
            "Batch 500 / 1132 Sammon Loss : 4021.97705078125\n",
            "Batch 510 / 1132 Sammon Loss : 3851.19482421875\n",
            "Batch 520 / 1132 Sammon Loss : 2772.799072265625\n",
            "Batch 530 / 1132 Sammon Loss : 3538.69287109375\n",
            "Batch 540 / 1132 Sammon Loss : 635.9200439453125\n",
            "Batch 550 / 1132 Sammon Loss : 730.7985229492188\n",
            "Batch 560 / 1132 Sammon Loss : 816.876708984375\n",
            "Batch 570 / 1132 Sammon Loss : 863.1055908203125\n",
            "Batch 580 / 1132 Sammon Loss : 1152.875\n",
            "Batch 590 / 1132 Sammon Loss : 1283.3262939453125\n",
            "Batch 600 / 1132 Sammon Loss : 760.83154296875\n",
            "Batch 610 / 1132 Sammon Loss : 632.542724609375\n",
            "Batch 620 / 1132 Sammon Loss : 2082.247314453125\n",
            "Batch 630 / 1132 Sammon Loss : 2222.06787109375\n",
            "Batch 640 / 1132 Sammon Loss : 2032.326904296875\n",
            "Batch 650 / 1132 Sammon Loss : 2954.39208984375\n",
            "Batch 660 / 1132 Sammon Loss : 1046.0941162109375\n",
            "Batch 670 / 1132 Sammon Loss : 770.9832763671875\n",
            "Batch 680 / 1132 Sammon Loss : 1134.705322265625\n",
            "Batch 690 / 1132 Sammon Loss : 768.5509033203125\n",
            "Batch 700 / 1132 Sammon Loss : 1034.0323486328125\n",
            "Batch 710 / 1132 Sammon Loss : 731.23828125\n",
            "Batch 720 / 1132 Sammon Loss : 1272.222412109375\n",
            "Batch 730 / 1132 Sammon Loss : 477.1728820800781\n",
            "Batch 740 / 1132 Sammon Loss : 1041.673583984375\n",
            "Batch 750 / 1132 Sammon Loss : 764.4818115234375\n",
            "Batch 760 / 1132 Sammon Loss : 1034.2098388671875\n",
            "Batch 770 / 1132 Sammon Loss : 1752.8299560546875\n",
            "Batch 780 / 1132 Sammon Loss : 1452.8536376953125\n",
            "Batch 790 / 1132 Sammon Loss : 879.9752197265625\n",
            "Batch 800 / 1132 Sammon Loss : 1904.329833984375\n",
            "Batch 810 / 1132 Sammon Loss : 806.5601196289062\n",
            "Batch 820 / 1132 Sammon Loss : 929.016845703125\n",
            "Batch 830 / 1132 Sammon Loss : 1774.15966796875\n",
            "Batch 840 / 1132 Sammon Loss : 1380.074462890625\n",
            "Batch 850 / 1132 Sammon Loss : 2988.609375\n",
            "Batch 860 / 1132 Sammon Loss : 3310.8271484375\n",
            "Batch 870 / 1132 Sammon Loss : 3714.50927734375\n",
            "Batch 880 / 1132 Sammon Loss : 1325.4547119140625\n",
            "Batch 890 / 1132 Sammon Loss : 1002.2445068359375\n",
            "Batch 900 / 1132 Sammon Loss : 894.5858154296875\n",
            "Batch 910 / 1132 Sammon Loss : 1249.90771484375\n",
            "Batch 920 / 1132 Sammon Loss : 825.1966552734375\n",
            "Batch 930 / 1132 Sammon Loss : 1936.33154296875\n",
            "Batch 940 / 1132 Sammon Loss : 2705.338623046875\n",
            "Batch 950 / 1132 Sammon Loss : 2185.082763671875\n",
            "Batch 960 / 1132 Sammon Loss : 1566.371826171875\n",
            "Batch 970 / 1132 Sammon Loss : 1076.663330078125\n",
            "Batch 980 / 1132 Sammon Loss : 791.0274658203125\n",
            "Batch 990 / 1132 Sammon Loss : 1163.517578125\n",
            "Batch 1000 / 1132 Sammon Loss : 884.1470947265625\n",
            "Batch 1010 / 1132 Sammon Loss : 784.7677001953125\n",
            "Batch 1020 / 1132 Sammon Loss : 1488.106201171875\n",
            "Batch 1030 / 1132 Sammon Loss : 1318.106201171875\n",
            "Batch 1040 / 1132 Sammon Loss : 1548.440185546875\n",
            "Batch 1050 / 1132 Sammon Loss : 1064.81689453125\n",
            "Batch 1060 / 1132 Sammon Loss : 943.8399658203125\n",
            "Batch 1070 / 1132 Sammon Loss : 2818.8974609375\n",
            "Batch 1080 / 1132 Sammon Loss : 1377.4169921875\n",
            "Batch 1090 / 1132 Sammon Loss : 870.457275390625\n",
            "Batch 1100 / 1132 Sammon Loss : 3367.97216796875\n",
            "Batch 1110 / 1132 Sammon Loss : 948.1974487304688\n",
            "Batch 1120 / 1132 Sammon Loss : 1400.220703125\n",
            "Batch 1130 / 1132 Sammon Loss : 936.76123046875\n",
            "Batch 10 / 505 Regressor Loss : 123783.109375\n",
            "Batch 20 / 505 Regressor Loss : 135228.515625\n",
            "Batch 30 / 505 Regressor Loss : 118368.171875\n",
            "Batch 40 / 505 Regressor Loss : 119442.921875\n",
            "Batch 50 / 505 Regressor Loss : 153827.4375\n",
            "Batch 60 / 505 Regressor Loss : 146440.34375\n",
            "Batch 70 / 505 Regressor Loss : 145284.515625\n",
            "Batch 80 / 505 Regressor Loss : 148976.4375\n",
            "Batch 90 / 505 Regressor Loss : 150484.53125\n",
            "Batch 100 / 505 Regressor Loss : 124905.390625\n",
            "Batch 110 / 505 Regressor Loss : 142795.53125\n",
            "Batch 120 / 505 Regressor Loss : 130785.3671875\n",
            "Batch 130 / 505 Regressor Loss : 146470.90625\n",
            "Batch 140 / 505 Regressor Loss : 153740.125\n",
            "Batch 150 / 505 Regressor Loss : 137053.171875\n",
            "Batch 160 / 505 Regressor Loss : 111662.28125\n",
            "Batch 170 / 505 Regressor Loss : 121054.265625\n",
            "Batch 180 / 505 Regressor Loss : 151929.84375\n",
            "Batch 190 / 505 Regressor Loss : 147308.5\n",
            "Batch 200 / 505 Regressor Loss : 120465.421875\n",
            "Batch 210 / 505 Regressor Loss : 129977.8984375\n",
            "Batch 220 / 505 Regressor Loss : 143318.515625\n",
            "Batch 230 / 505 Regressor Loss : 135108.53125\n",
            "Batch 240 / 505 Regressor Loss : 125515.046875\n",
            "Batch 250 / 505 Regressor Loss : 170802.609375\n",
            "Batch 260 / 505 Regressor Loss : 146835.546875\n",
            "Batch 270 / 505 Regressor Loss : 124842.171875\n",
            "Batch 280 / 505 Regressor Loss : 144473.15625\n",
            "Batch 290 / 505 Regressor Loss : 142005.0625\n",
            "Batch 300 / 505 Regressor Loss : 134784.03125\n",
            "Batch 310 / 505 Regressor Loss : 138706.46875\n",
            "Batch 320 / 505 Regressor Loss : 167596.8125\n",
            "Batch 330 / 505 Regressor Loss : 131846.78125\n",
            "Batch 340 / 505 Regressor Loss : 129662.8046875\n",
            "Batch 350 / 505 Regressor Loss : 110599.890625\n",
            "Batch 360 / 505 Regressor Loss : 116758.296875\n",
            "Batch 370 / 505 Regressor Loss : 139577.0\n",
            "Batch 380 / 505 Regressor Loss : 121307.4609375\n",
            "Batch 390 / 505 Regressor Loss : 134150.859375\n",
            "Batch 400 / 505 Regressor Loss : 160946.96875\n",
            "Batch 410 / 505 Regressor Loss : 122413.375\n",
            "Batch 420 / 505 Regressor Loss : 144626.125\n",
            "Batch 430 / 505 Regressor Loss : 128287.375\n",
            "Batch 440 / 505 Regressor Loss : 130252.875\n",
            "Batch 450 / 505 Regressor Loss : 116192.734375\n",
            "Batch 460 / 505 Regressor Loss : 146542.484375\n",
            "Batch 470 / 505 Regressor Loss : 137382.734375\n",
            "Batch 480 / 505 Regressor Loss : 139242.75\n",
            "Batch 490 / 505 Regressor Loss : 139890.890625\n",
            "Batch 500 / 505 Regressor Loss : 136356.109375\n",
            "Epoch  0 / 100  Sammon Loss : 562.22509765625  Regressor Loss : 119032.15625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([1, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Batch 690 / 1132 Sammon Loss : 1096.6151123046875\n",
            "Batch 700 / 1132 Sammon Loss : 1517.276611328125\n",
            "Batch 710 / 1132 Sammon Loss : 1067.60107421875\n",
            "Batch 720 / 1132 Sammon Loss : 1655.429931640625\n",
            "Batch 730 / 1132 Sammon Loss : 700.5896606445312\n",
            "Batch 740 / 1132 Sammon Loss : 1344.64501953125\n",
            "Batch 750 / 1132 Sammon Loss : 1028.8076171875\n",
            "Batch 760 / 1132 Sammon Loss : 1475.412353515625\n",
            "Batch 770 / 1132 Sammon Loss : 2211.4658203125\n",
            "Batch 780 / 1132 Sammon Loss : 1908.04638671875\n",
            "Batch 790 / 1132 Sammon Loss : 1220.09521484375\n",
            "Batch 800 / 1132 Sammon Loss : 2269.52392578125\n",
            "Batch 810 / 1132 Sammon Loss : 1015.3395385742188\n",
            "Batch 820 / 1132 Sammon Loss : 1203.2666015625\n",
            "Batch 830 / 1132 Sammon Loss : 2103.70556640625\n",
            "Batch 840 / 1132 Sammon Loss : 1715.426025390625\n",
            "Batch 850 / 1132 Sammon Loss : 3473.804931640625\n",
            "Batch 860 / 1132 Sammon Loss : 3790.21875\n",
            "Batch 870 / 1132 Sammon Loss : 4177.44921875\n",
            "Batch 880 / 1132 Sammon Loss : 1763.414306640625\n",
            "Batch 890 / 1132 Sammon Loss : 1553.5364990234375\n",
            "Batch 900 / 1132 Sammon Loss : 1425.37646484375\n",
            "Batch 910 / 1132 Sammon Loss : 1747.247314453125\n",
            "Batch 920 / 1132 Sammon Loss : 1237.6478271484375\n",
            "Batch 930 / 1132 Sammon Loss : 2695.14453125\n",
            "Batch 940 / 1132 Sammon Loss : 3595.95751953125\n",
            "Batch 950 / 1132 Sammon Loss : 2997.37255859375\n",
            "Batch 960 / 1132 Sammon Loss : 2194.23291015625\n",
            "Batch 970 / 1132 Sammon Loss : 1488.2001953125\n",
            "Batch 980 / 1132 Sammon Loss : 1180.583740234375\n",
            "Batch 990 / 1132 Sammon Loss : 1653.614501953125\n",
            "Batch 1000 / 1132 Sammon Loss : 1271.9248046875\n",
            "Batch 1010 / 1132 Sammon Loss : 974.8606567382812\n",
            "Batch 1020 / 1132 Sammon Loss : 1976.4617919921875\n",
            "Batch 1030 / 1132 Sammon Loss : 1840.416748046875\n",
            "Batch 1040 / 1132 Sammon Loss : 2159.299560546875\n",
            "Batch 1050 / 1132 Sammon Loss : 1461.657958984375\n",
            "Batch 1060 / 1132 Sammon Loss : 1378.26806640625\n",
            "Batch 1070 / 1132 Sammon Loss : 3502.646484375\n",
            "Batch 1080 / 1132 Sammon Loss : 1936.427978515625\n",
            "Batch 1090 / 1132 Sammon Loss : 1270.876708984375\n",
            "Batch 1100 / 1132 Sammon Loss : 4197.4638671875\n",
            "Batch 1110 / 1132 Sammon Loss : 1341.581298828125\n",
            "Batch 1120 / 1132 Sammon Loss : 1685.478271484375\n",
            "Batch 1130 / 1132 Sammon Loss : 1505.565673828125\n",
            "Batch 10 / 505 Regressor Loss : 110147.3359375\n",
            "Batch 20 / 505 Regressor Loss : 121359.671875\n",
            "Batch 30 / 505 Regressor Loss : 104400.65625\n",
            "Batch 40 / 505 Regressor Loss : 105399.578125\n",
            "Batch 50 / 505 Regressor Loss : 137049.21875\n",
            "Batch 60 / 505 Regressor Loss : 131107.15625\n",
            "Batch 70 / 505 Regressor Loss : 130540.015625\n",
            "Batch 80 / 505 Regressor Loss : 132944.46875\n",
            "Batch 90 / 505 Regressor Loss : 135914.625\n",
            "Batch 100 / 505 Regressor Loss : 110531.265625\n",
            "Batch 110 / 505 Regressor Loss : 128588.5\n",
            "Batch 120 / 505 Regressor Loss : 117528.203125\n",
            "Batch 130 / 505 Regressor Loss : 132260.546875\n",
            "Batch 140 / 505 Regressor Loss : 138567.375\n",
            "Batch 150 / 505 Regressor Loss : 122878.109375\n",
            "Batch 160 / 505 Regressor Loss : 98462.6875\n",
            "Batch 170 / 505 Regressor Loss : 107409.2109375\n",
            "Batch 180 / 505 Regressor Loss : 136761.8125\n",
            "Batch 190 / 505 Regressor Loss : 131546.1875\n",
            "Batch 200 / 505 Regressor Loss : 107793.4921875\n",
            "Batch 210 / 505 Regressor Loss : 115837.5\n",
            "Batch 220 / 505 Regressor Loss : 128631.125\n",
            "Batch 230 / 505 Regressor Loss : 121219.0859375\n",
            "Batch 240 / 505 Regressor Loss : 112446.453125\n",
            "Batch 250 / 505 Regressor Loss : 154716.984375\n",
            "Batch 260 / 505 Regressor Loss : 131472.09375\n",
            "Batch 270 / 505 Regressor Loss : 111502.1171875\n",
            "Batch 280 / 505 Regressor Loss : 129683.6171875\n",
            "Batch 290 / 505 Regressor Loss : 128194.125\n",
            "Batch 300 / 505 Regressor Loss : 120283.8359375\n",
            "Batch 310 / 505 Regressor Loss : 124443.5859375\n",
            "Batch 320 / 505 Regressor Loss : 151306.5\n",
            "Batch 330 / 505 Regressor Loss : 117424.4375\n",
            "Batch 340 / 505 Regressor Loss : 114732.4609375\n",
            "Batch 350 / 505 Regressor Loss : 97400.921875\n",
            "Batch 360 / 505 Regressor Loss : 104397.2421875\n",
            "Batch 370 / 505 Regressor Loss : 124808.1875\n",
            "Batch 380 / 505 Regressor Loss : 108022.859375\n",
            "Batch 390 / 505 Regressor Loss : 120178.6796875\n",
            "Batch 400 / 505 Regressor Loss : 144190.09375\n",
            "Batch 410 / 505 Regressor Loss : 109004.28125\n",
            "Batch 420 / 505 Regressor Loss : 130247.59375\n",
            "Batch 430 / 505 Regressor Loss : 114540.796875\n",
            "Batch 440 / 505 Regressor Loss : 116114.765625\n",
            "Batch 450 / 505 Regressor Loss : 103542.8125\n",
            "Batch 460 / 505 Regressor Loss : 132011.546875\n",
            "Batch 470 / 505 Regressor Loss : 124143.53125\n",
            "Batch 480 / 505 Regressor Loss : 125213.0625\n",
            "Batch 490 / 505 Regressor Loss : 125453.1171875\n",
            "Batch 500 / 505 Regressor Loss : 122870.453125\n",
            "Epoch  56 / 100  Sammon Loss : 957.541748046875  Regressor Loss : 106476.0\n",
            "Val Loss : 106161.6171875\n",
            "Batch 10 / 1132 Sammon Loss : 3276.252197265625\n",
            "Batch 20 / 1132 Sammon Loss : 3658.815185546875\n",
            "Batch 30 / 1132 Sammon Loss : 3712.695556640625\n",
            "Batch 40 / 1132 Sammon Loss : 3828.498046875\n",
            "Batch 50 / 1132 Sammon Loss : 4041.886474609375\n",
            "Batch 60 / 1132 Sammon Loss : 2016.74658203125\n",
            "Batch 70 / 1132 Sammon Loss : 2012.9759521484375\n",
            "Batch 80 / 1132 Sammon Loss : 1724.833984375\n",
            "Batch 90 / 1132 Sammon Loss : 1766.2623291015625\n",
            "Batch 100 / 1132 Sammon Loss : 1582.7147216796875\n",
            "Batch 110 / 1132 Sammon Loss : 1199.9794921875\n",
            "Batch 120 / 1132 Sammon Loss : 643.394775390625\n",
            "Batch 130 / 1132 Sammon Loss : 1205.789794921875\n",
            "Batch 140 / 1132 Sammon Loss : 1367.480712890625\n",
            "Batch 150 / 1132 Sammon Loss : 1570.518310546875\n",
            "Batch 160 / 1132 Sammon Loss : 1416.600341796875\n",
            "Batch 170 / 1132 Sammon Loss : 3414.656494140625\n",
            "Batch 180 / 1132 Sammon Loss : 3871.2236328125\n",
            "Batch 190 / 1132 Sammon Loss : 2881.59521484375\n",
            "Batch 200 / 1132 Sammon Loss : 2638.65625\n",
            "Batch 210 / 1132 Sammon Loss : 2484.387939453125\n",
            "Batch 220 / 1132 Sammon Loss : 1870.398681640625\n",
            "Batch 230 / 1132 Sammon Loss : 1617.13818359375\n",
            "Batch 240 / 1132 Sammon Loss : 1096.705078125\n",
            "Batch 250 / 1132 Sammon Loss : 966.0458984375\n",
            "Batch 260 / 1132 Sammon Loss : 1175.018310546875\n",
            "Batch 270 / 1132 Sammon Loss : 1338.1453857421875\n",
            "Batch 280 / 1132 Sammon Loss : 995.3779907226562\n",
            "Batch 290 / 1132 Sammon Loss : 1101.4410400390625\n",
            "Batch 300 / 1132 Sammon Loss : 2400.180908203125\n",
            "Batch 310 / 1132 Sammon Loss : 1117.6480712890625\n",
            "Batch 320 / 1132 Sammon Loss : 932.82666015625\n",
            "Batch 330 / 1132 Sammon Loss : 1348.28076171875\n",
            "Batch 340 / 1132 Sammon Loss : 1124.58642578125\n",
            "Batch 350 / 1132 Sammon Loss : 1963.571533203125\n",
            "Batch 360 / 1132 Sammon Loss : 2122.101318359375\n",
            "Batch 370 / 1132 Sammon Loss : 2150.494140625\n",
            "Batch 380 / 1132 Sammon Loss : 1703.012451171875\n",
            "Batch 390 / 1132 Sammon Loss : 2112.96240234375\n",
            "Batch 400 / 1132 Sammon Loss : 2522.9404296875\n",
            "Batch 410 / 1132 Sammon Loss : 1882.462158203125\n",
            "Batch 420 / 1132 Sammon Loss : 1858.9044189453125\n",
            "Batch 430 / 1132 Sammon Loss : 1903.140625\n",
            "Batch 440 / 1132 Sammon Loss : 2811.83642578125\n",
            "Batch 450 / 1132 Sammon Loss : 1764.35693359375\n",
            "Batch 460 / 1132 Sammon Loss : 2187.63134765625\n",
            "Batch 470 / 1132 Sammon Loss : 1755.085205078125\n",
            "Batch 480 / 1132 Sammon Loss : 2355.25341796875\n",
            "Batch 490 / 1132 Sammon Loss : 1340.098388671875\n",
            "Batch 500 / 1132 Sammon Loss : 5170.7294921875\n",
            "Batch 510 / 1132 Sammon Loss : 5056.2265625\n",
            "Batch 520 / 1132 Sammon Loss : 3496.2080078125\n",
            "Batch 530 / 1132 Sammon Loss : 4510.3525390625\n",
            "Batch 540 / 1132 Sammon Loss : 1139.312255859375\n",
            "Batch 550 / 1132 Sammon Loss : 1292.884033203125\n",
            "Batch 560 / 1132 Sammon Loss : 1396.452880859375\n",
            "Batch 570 / 1132 Sammon Loss : 1398.485595703125\n",
            "Batch 580 / 1132 Sammon Loss : 1618.475341796875\n",
            "Batch 590 / 1132 Sammon Loss : 1732.1962890625\n",
            "Batch 600 / 1132 Sammon Loss : 1239.9700927734375\n",
            "Batch 610 / 1132 Sammon Loss : 1121.4544677734375\n",
            "Batch 620 / 1132 Sammon Loss : 2827.66796875\n",
            "Batch 630 / 1132 Sammon Loss : 3011.31201171875\n",
            "Batch 640 / 1132 Sammon Loss : 2747.6240234375\n",
            "Batch 650 / 1132 Sammon Loss : 3507.1162109375\n",
            "Batch 660 / 1132 Sammon Loss : 1501.936279296875\n",
            "Batch 670 / 1132 Sammon Loss : 1145.7938232421875\n",
            "Batch 680 / 1132 Sammon Loss : 1635.9896240234375\n",
            "Batch 690 / 1132 Sammon Loss : 1088.692138671875\n",
            "Batch 700 / 1132 Sammon Loss : 1481.209716796875\n",
            "Batch 710 / 1132 Sammon Loss : 1085.6513671875\n",
            "Batch 720 / 1132 Sammon Loss : 1656.6746826171875\n",
            "Batch 730 / 1132 Sammon Loss : 747.9337158203125\n",
            "Batch 740 / 1132 Sammon Loss : 1431.34228515625\n",
            "Batch 750 / 1132 Sammon Loss : 1119.396728515625\n",
            "Batch 760 / 1132 Sammon Loss : 1486.568115234375\n",
            "Batch 770 / 1132 Sammon Loss : 2239.349609375\n",
            "Batch 780 / 1132 Sammon Loss : 1968.262451171875\n",
            "Batch 790 / 1132 Sammon Loss : 1259.491455078125\n",
            "Batch 800 / 1132 Sammon Loss : 2330.53271484375\n",
            "Batch 810 / 1132 Sammon Loss : 1059.721435546875\n",
            "Batch 820 / 1132 Sammon Loss : 1255.821044921875\n",
            "Batch 830 / 1132 Sammon Loss : 2128.517333984375\n",
            "Batch 840 / 1132 Sammon Loss : 1766.31005859375\n",
            "Batch 850 / 1132 Sammon Loss : 3459.6552734375\n",
            "Batch 860 / 1132 Sammon Loss : 3811.036376953125\n",
            "Batch 870 / 1132 Sammon Loss : 4201.2646484375\n",
            "Batch 880 / 1132 Sammon Loss : 1819.15478515625\n",
            "Batch 890 / 1132 Sammon Loss : 1551.688720703125\n",
            "Batch 900 / 1132 Sammon Loss : 1453.8311767578125\n",
            "Batch 910 / 1132 Sammon Loss : 1769.8895263671875\n",
            "Batch 920 / 1132 Sammon Loss : 1270.3873291015625\n",
            "Batch 930 / 1132 Sammon Loss : 2660.69189453125\n",
            "Batch 940 / 1132 Sammon Loss : 3608.91748046875\n",
            "Batch 950 / 1132 Sammon Loss : 2971.916015625\n",
            "Batch 960 / 1132 Sammon Loss : 2147.4091796875\n",
            "Batch 970 / 1132 Sammon Loss : 1492.310302734375\n",
            "Batch 980 / 1132 Sammon Loss : 1247.822509765625\n",
            "Batch 990 / 1132 Sammon Loss : 1651.63525390625\n",
            "Batch 1000 / 1132 Sammon Loss : 1301.80078125\n",
            "Batch 1010 / 1132 Sammon Loss : 998.9620971679688\n",
            "Batch 1020 / 1132 Sammon Loss : 2001.48095703125\n",
            "Batch 1030 / 1132 Sammon Loss : 1856.9825439453125\n",
            "Batch 1040 / 1132 Sammon Loss : 2165.568603515625\n",
            "Batch 1050 / 1132 Sammon Loss : 1497.2777099609375\n",
            "Batch 1060 / 1132 Sammon Loss : 1382.5963134765625\n",
            "Batch 1070 / 1132 Sammon Loss : 3532.461669921875\n",
            "Batch 1080 / 1132 Sammon Loss : 1905.985107421875\n",
            "Batch 1090 / 1132 Sammon Loss : 1280.9345703125\n",
            "Batch 1100 / 1132 Sammon Loss : 4221.5537109375\n",
            "Batch 1110 / 1132 Sammon Loss : 1380.71435546875\n",
            "Batch 1120 / 1132 Sammon Loss : 1737.443115234375\n",
            "Batch 1130 / 1132 Sammon Loss : 1499.137451171875\n",
            "Batch 10 / 505 Regressor Loss : 109926.5\n",
            "Batch 20 / 505 Regressor Loss : 121134.6484375\n",
            "Batch 30 / 505 Regressor Loss : 104174.46875\n",
            "Batch 40 / 505 Regressor Loss : 105170.7421875\n",
            "Batch 50 / 505 Regressor Loss : 136773.578125\n",
            "Batch 60 / 505 Regressor Loss : 130855.4609375\n",
            "Batch 70 / 505 Regressor Loss : 130297.3671875\n",
            "Batch 80 / 505 Regressor Loss : 132681.59375\n",
            "Batch 90 / 505 Regressor Loss : 135674.421875\n",
            "Batch 100 / 505 Regressor Loss : 110294.890625\n",
            "Batch 110 / 505 Regressor Loss : 128353.390625\n",
            "Batch 120 / 505 Regressor Loss : 117310.890625\n",
            "Batch 130 / 505 Regressor Loss : 132025.25\n",
            "Batch 140 / 505 Regressor Loss : 138316.6875\n",
            "Batch 150 / 505 Regressor Loss : 122644.453125\n",
            "Batch 160 / 505 Regressor Loss : 98246.984375\n",
            "Batch 170 / 505 Regressor Loss : 107185.2734375\n",
            "Batch 180 / 505 Regressor Loss : 136511.0\n",
            "Batch 190 / 505 Regressor Loss : 131285.21875\n",
            "Batch 200 / 505 Regressor Loss : 107585.171875\n",
            "Batch 210 / 505 Regressor Loss : 115604.3125\n",
            "Batch 220 / 505 Regressor Loss : 128389.125\n",
            "Batch 230 / 505 Regressor Loss : 120989.7734375\n",
            "Batch 240 / 505 Regressor Loss : 112231.0859375\n",
            "Batch 250 / 505 Regressor Loss : 154448.84375\n",
            "Batch 260 / 505 Regressor Loss : 131218.6875\n",
            "Batch 270 / 505 Regressor Loss : 111282.8359375\n",
            "Batch 280 / 505 Regressor Loss : 129438.7109375\n",
            "Batch 290 / 505 Regressor Loss : 127965.484375\n",
            "Batch 300 / 505 Regressor Loss : 120043.21875\n",
            "Batch 310 / 505 Regressor Loss : 124206.9296875\n",
            "Batch 320 / 505 Regressor Loss : 151035.4375\n",
            "Batch 330 / 505 Regressor Loss : 117184.8984375\n",
            "Batch 340 / 505 Regressor Loss : 114484.5234375\n",
            "Batch 350 / 505 Regressor Loss : 97183.359375\n",
            "Batch 360 / 505 Regressor Loss : 104193.4296875\n",
            "Batch 370 / 505 Regressor Loss : 124563.203125\n",
            "Batch 380 / 505 Regressor Loss : 107803.578125\n",
            "Batch 390 / 505 Regressor Loss : 119946.3125\n",
            "Batch 400 / 505 Regressor Loss : 143911.359375\n",
            "Batch 410 / 505 Regressor Loss : 108783.328125\n",
            "Batch 420 / 505 Regressor Loss : 130007.984375\n",
            "Batch 430 / 505 Regressor Loss : 114312.1875\n",
            "Batch 440 / 505 Regressor Loss : 115879.5546875\n",
            "Batch 450 / 505 Regressor Loss : 103333.96875\n",
            "Batch 460 / 505 Regressor Loss : 131769.1875\n",
            "Batch 470 / 505 Regressor Loss : 123922.6875\n",
            "Batch 480 / 505 Regressor Loss : 124979.234375\n",
            "Batch 490 / 505 Regressor Loss : 125212.515625\n",
            "Batch 500 / 505 Regressor Loss : 122646.40625\n",
            "Epoch  57 / 100  Sammon Loss : 969.4978637695312  Regressor Loss : 106267.9609375\n",
            "Val Loss : 105900.3359375\n",
            "Batch 10 / 1132 Sammon Loss : 3307.518798828125\n",
            "Batch 20 / 1132 Sammon Loss : 3643.0322265625\n",
            "Batch 30 / 1132 Sammon Loss : 3719.571044921875\n",
            "Batch 40 / 1132 Sammon Loss : 3875.561767578125\n",
            "Batch 50 / 1132 Sammon Loss : 4097.62109375\n",
            "Batch 60 / 1132 Sammon Loss : 2062.22265625\n",
            "Batch 70 / 1132 Sammon Loss : 2040.531494140625\n",
            "Batch 80 / 1132 Sammon Loss : 1738.0623779296875\n",
            "Batch 90 / 1132 Sammon Loss : 1769.302490234375\n",
            "Batch 100 / 1132 Sammon Loss : 1637.8404541015625\n",
            "Batch 110 / 1132 Sammon Loss : 1235.0264892578125\n",
            "Batch 120 / 1132 Sammon Loss : 630.1303100585938\n",
            "Batch 130 / 1132 Sammon Loss : 1223.942626953125\n",
            "Batch 140 / 1132 Sammon Loss : 1437.4554443359375\n",
            "Batch 150 / 1132 Sammon Loss : 1600.14111328125\n",
            "Batch 160 / 1132 Sammon Loss : 1475.26171875\n",
            "Batch 170 / 1132 Sammon Loss : 3441.10400390625\n",
            "Batch 180 / 1132 Sammon Loss : 3890.068359375\n",
            "Batch 190 / 1132 Sammon Loss : 2907.21875\n",
            "Batch 200 / 1132 Sammon Loss : 2703.500732421875\n",
            "Batch 210 / 1132 Sammon Loss : 2523.214599609375\n",
            "Batch 220 / 1132 Sammon Loss : 1899.2366943359375\n",
            "Batch 230 / 1132 Sammon Loss : 1652.55810546875\n",
            "Batch 240 / 1132 Sammon Loss : 1120.745361328125\n",
            "Batch 250 / 1132 Sammon Loss : 1027.82861328125\n",
            "Batch 260 / 1132 Sammon Loss : 1211.54736328125\n",
            "Batch 270 / 1132 Sammon Loss : 1325.964599609375\n",
            "Batch 280 / 1132 Sammon Loss : 1041.205078125\n",
            "Batch 290 / 1132 Sammon Loss : 1115.590087890625\n",
            "Batch 300 / 1132 Sammon Loss : 2407.64306640625\n",
            "Batch 310 / 1132 Sammon Loss : 1145.3568115234375\n",
            "Batch 320 / 1132 Sammon Loss : 969.1190795898438\n",
            "Batch 330 / 1132 Sammon Loss : 1363.671630859375\n",
            "Batch 340 / 1132 Sammon Loss : 1155.783935546875\n",
            "Batch 350 / 1132 Sammon Loss : 1985.18701171875\n",
            "Batch 360 / 1132 Sammon Loss : 2122.43408203125\n",
            "Batch 370 / 1132 Sammon Loss : 2199.935546875\n",
            "Batch 380 / 1132 Sammon Loss : 1753.572021484375\n",
            "Batch 390 / 1132 Sammon Loss : 2132.99560546875\n",
            "Batch 400 / 1132 Sammon Loss : 2554.107421875\n",
            "Batch 410 / 1132 Sammon Loss : 1885.543701171875\n",
            "Batch 420 / 1132 Sammon Loss : 1855.8280029296875\n",
            "Batch 430 / 1132 Sammon Loss : 1957.7783203125\n",
            "Batch 440 / 1132 Sammon Loss : 2850.85009765625\n",
            "Batch 450 / 1132 Sammon Loss : 1782.869873046875\n",
            "Batch 460 / 1132 Sammon Loss : 2232.053955078125\n",
            "Batch 470 / 1132 Sammon Loss : 1802.601318359375\n",
            "Batch 480 / 1132 Sammon Loss : 2370.33837890625\n",
            "Batch 490 / 1132 Sammon Loss : 1345.1058349609375\n",
            "Batch 500 / 1132 Sammon Loss : 5265.802734375\n",
            "Batch 510 / 1132 Sammon Loss : 5109.2392578125\n",
            "Batch 520 / 1132 Sammon Loss : 3539.181884765625\n",
            "Batch 530 / 1132 Sammon Loss : 4562.59619140625\n",
            "Batch 540 / 1132 Sammon Loss : 1134.59765625\n",
            "Batch 550 / 1132 Sammon Loss : 1263.3321533203125\n",
            "Batch 560 / 1132 Sammon Loss : 1363.5546875\n",
            "Batch 570 / 1132 Sammon Loss : 1356.378662109375\n",
            "Batch 580 / 1132 Sammon Loss : 1598.0614013671875\n",
            "Batch 590 / 1132 Sammon Loss : 1710.8326416015625\n",
            "Batch 600 / 1132 Sammon Loss : 1204.4447021484375\n",
            "Batch 610 / 1132 Sammon Loss : 1060.8099365234375\n",
            "Batch 620 / 1132 Sammon Loss : 2965.955078125\n",
            "Batch 630 / 1132 Sammon Loss : 3101.42431640625\n",
            "Batch 640 / 1132 Sammon Loss : 2798.623046875\n",
            "Batch 650 / 1132 Sammon Loss : 3612.4599609375\n",
            "Batch 660 / 1132 Sammon Loss : 1528.898681640625\n",
            "Batch 670 / 1132 Sammon Loss : 1153.568359375\n",
            "Batch 680 / 1132 Sammon Loss : 1651.48583984375\n",
            "Batch 690 / 1132 Sammon Loss : 1108.0335693359375\n",
            "Batch 700 / 1132 Sammon Loss : 1516.7989501953125\n",
            "Batch 710 / 1132 Sammon Loss : 1073.857421875\n",
            "Batch 720 / 1132 Sammon Loss : 1695.855712890625\n",
            "Batch 730 / 1132 Sammon Loss : 733.15234375\n",
            "Batch 740 / 1132 Sammon Loss : 1420.681396484375\n",
            "Batch 750 / 1132 Sammon Loss : 1089.771728515625\n",
            "Batch 760 / 1132 Sammon Loss : 1498.1494140625\n",
            "Batch 770 / 1132 Sammon Loss : 2277.52294921875\n",
            "Batch 780 / 1132 Sammon Loss : 1976.77880859375\n",
            "Batch 790 / 1132 Sammon Loss : 1273.7757568359375\n",
            "Batch 800 / 1132 Sammon Loss : 2329.92138671875\n",
            "Batch 810 / 1132 Sammon Loss : 1039.3173828125\n",
            "Batch 820 / 1132 Sammon Loss : 1197.3106689453125\n",
            "Batch 830 / 1132 Sammon Loss : 2139.78662109375\n",
            "Batch 840 / 1132 Sammon Loss : 1766.7264404296875\n",
            "Batch 850 / 1132 Sammon Loss : 3537.68603515625\n",
            "Batch 860 / 1132 Sammon Loss : 3860.327392578125\n",
            "Batch 870 / 1132 Sammon Loss : 4258.16015625\n",
            "Batch 880 / 1132 Sammon Loss : 1795.205322265625\n",
            "Batch 890 / 1132 Sammon Loss : 1557.736572265625\n",
            "Batch 900 / 1132 Sammon Loss : 1478.3768310546875\n",
            "Batch 910 / 1132 Sammon Loss : 1775.239501953125\n",
            "Batch 920 / 1132 Sammon Loss : 1269.340576171875\n",
            "Batch 930 / 1132 Sammon Loss : 2780.822509765625\n",
            "Batch 940 / 1132 Sammon Loss : 3663.072265625\n",
            "Batch 950 / 1132 Sammon Loss : 3043.274169921875\n",
            "Batch 960 / 1132 Sammon Loss : 2197.21875\n",
            "Batch 970 / 1132 Sammon Loss : 1510.4984130859375\n",
            "Batch 980 / 1132 Sammon Loss : 1222.011474609375\n",
            "Batch 990 / 1132 Sammon Loss : 1686.6513671875\n",
            "Batch 1000 / 1132 Sammon Loss : 1307.47900390625\n",
            "Batch 1010 / 1132 Sammon Loss : 974.818603515625\n",
            "Batch 1020 / 1132 Sammon Loss : 2010.478271484375\n",
            "Batch 1030 / 1132 Sammon Loss : 1866.7081298828125\n",
            "Batch 1040 / 1132 Sammon Loss : 2165.90380859375\n",
            "Batch 1050 / 1132 Sammon Loss : 1517.96142578125\n",
            "Batch 1060 / 1132 Sammon Loss : 1396.128662109375\n",
            "Batch 1070 / 1132 Sammon Loss : 3542.2744140625\n",
            "Batch 1080 / 1132 Sammon Loss : 1959.8936767578125\n",
            "Batch 1090 / 1132 Sammon Loss : 1298.0341796875\n",
            "Batch 1100 / 1132 Sammon Loss : 4264.79296875\n",
            "Batch 1110 / 1132 Sammon Loss : 1387.143310546875\n",
            "Batch 1120 / 1132 Sammon Loss : 1733.0638427734375\n",
            "Batch 1130 / 1132 Sammon Loss : 1485.427001953125\n",
            "Batch 10 / 505 Regressor Loss : 109706.078125\n",
            "Batch 20 / 505 Regressor Loss : 120910.0234375\n",
            "Batch 30 / 505 Regressor Loss : 103948.703125\n",
            "Batch 40 / 505 Regressor Loss : 104942.3125\n",
            "Batch 50 / 505 Regressor Loss : 136498.359375\n",
            "Batch 60 / 505 Regressor Loss : 130604.203125\n",
            "Batch 70 / 505 Regressor Loss : 130055.125\n",
            "Batch 80 / 505 Regressor Loss : 132419.1875\n",
            "Batch 90 / 505 Regressor Loss : 135434.609375\n",
            "Batch 100 / 505 Regressor Loss : 110058.921875\n",
            "Batch 110 / 505 Regressor Loss : 128118.6484375\n",
            "Batch 120 / 505 Regressor Loss : 117093.9609375\n",
            "Batch 130 / 505 Regressor Loss : 131790.3125\n",
            "Batch 140 / 505 Regressor Loss : 138066.40625\n",
            "Batch 150 / 505 Regressor Loss : 122411.1875\n",
            "Batch 160 / 505 Regressor Loss : 98031.6875\n",
            "Batch 170 / 505 Regressor Loss : 106961.7421875\n",
            "Batch 180 / 505 Regressor Loss : 136260.609375\n",
            "Batch 190 / 505 Regressor Loss : 131024.6796875\n",
            "Batch 200 / 505 Regressor Loss : 107377.2421875\n",
            "Batch 210 / 505 Regressor Loss : 115371.515625\n",
            "Batch 220 / 505 Regressor Loss : 128147.53125\n",
            "Batch 230 / 505 Regressor Loss : 120760.859375\n",
            "Batch 240 / 505 Regressor Loss : 112016.109375\n",
            "Batch 250 / 505 Regressor Loss : 154181.109375\n",
            "Batch 260 / 505 Regressor Loss : 130965.7109375\n",
            "Batch 270 / 505 Regressor Loss : 111063.9609375\n",
            "Batch 280 / 505 Regressor Loss : 129194.234375\n",
            "Batch 290 / 505 Regressor Loss : 127737.2109375\n",
            "Batch 300 / 505 Regressor Loss : 119803.0\n",
            "Batch 310 / 505 Regressor Loss : 123970.6796875\n",
            "Batch 320 / 505 Regressor Loss : 150764.8125\n",
            "Batch 330 / 505 Regressor Loss : 116945.765625\n",
            "Batch 340 / 505 Regressor Loss : 114237.0\n",
            "Batch 350 / 505 Regressor Loss : 96966.1796875\n",
            "Batch 360 / 505 Regressor Loss : 103989.96875\n",
            "Batch 370 / 505 Regressor Loss : 124318.609375\n",
            "Batch 380 / 505 Regressor Loss : 107584.6796875\n",
            "Batch 390 / 505 Regressor Loss : 119714.3359375\n",
            "Batch 400 / 505 Regressor Loss : 143633.046875\n",
            "Batch 410 / 505 Regressor Loss : 108562.7734375\n",
            "Batch 420 / 505 Regressor Loss : 129768.796875\n",
            "Batch 430 / 505 Regressor Loss : 114083.953125\n",
            "Batch 440 / 505 Regressor Loss : 115644.75\n",
            "Batch 450 / 505 Regressor Loss : 103125.515625\n",
            "Batch 460 / 505 Regressor Loss : 131527.25\n",
            "Batch 470 / 505 Regressor Loss : 123702.2109375\n",
            "Batch 480 / 505 Regressor Loss : 124745.8046875\n",
            "Batch 490 / 505 Regressor Loss : 124972.296875\n",
            "Batch 500 / 505 Regressor Loss : 122422.75\n",
            "Epoch  58 / 100  Sammon Loss : 940.50634765625  Regressor Loss : 106060.296875\n",
            "Val Loss : 105639.484375\n",
            "Batch 10 / 1132 Sammon Loss : 3323.6484375\n",
            "Batch 20 / 1132 Sammon Loss : 3668.0810546875\n",
            "Batch 30 / 1132 Sammon Loss : 3747.19482421875\n",
            "Batch 40 / 1132 Sammon Loss : 3870.782958984375\n",
            "Batch 50 / 1132 Sammon Loss : 4111.8740234375\n",
            "Batch 60 / 1132 Sammon Loss : 2040.892822265625\n",
            "Batch 70 / 1132 Sammon Loss : 2042.593994140625\n",
            "Batch 80 / 1132 Sammon Loss : 1735.91015625\n",
            "Batch 90 / 1132 Sammon Loss : 1783.54296875\n",
            "Batch 100 / 1132 Sammon Loss : 1608.455078125\n",
            "Batch 110 / 1132 Sammon Loss : 1219.2781982421875\n",
            "Batch 120 / 1132 Sammon Loss : 648.822265625\n",
            "Batch 130 / 1132 Sammon Loss : 1240.7469482421875\n",
            "Batch 140 / 1132 Sammon Loss : 1406.5859375\n",
            "Batch 150 / 1132 Sammon Loss : 1583.2640380859375\n",
            "Batch 160 / 1132 Sammon Loss : 1438.961669921875\n",
            "Batch 170 / 1132 Sammon Loss : 3482.770263671875\n",
            "Batch 180 / 1132 Sammon Loss : 3946.828125\n",
            "Batch 190 / 1132 Sammon Loss : 2936.95556640625\n",
            "Batch 200 / 1132 Sammon Loss : 2710.43603515625\n",
            "Batch 210 / 1132 Sammon Loss : 2547.51318359375\n",
            "Batch 220 / 1132 Sammon Loss : 1924.6048583984375\n",
            "Batch 230 / 1132 Sammon Loss : 1684.0703125\n",
            "Batch 240 / 1132 Sammon Loss : 1112.32958984375\n",
            "Batch 250 / 1132 Sammon Loss : 1029.61669921875\n",
            "Batch 260 / 1132 Sammon Loss : 1224.480224609375\n",
            "Batch 270 / 1132 Sammon Loss : 1349.9345703125\n",
            "Batch 280 / 1132 Sammon Loss : 1045.7392578125\n",
            "Batch 290 / 1132 Sammon Loss : 1111.416259765625\n",
            "Batch 300 / 1132 Sammon Loss : 2437.078125\n",
            "Batch 310 / 1132 Sammon Loss : 1147.1376953125\n",
            "Batch 320 / 1132 Sammon Loss : 1013.97802734375\n",
            "Batch 330 / 1132 Sammon Loss : 1369.002685546875\n",
            "Batch 340 / 1132 Sammon Loss : 1178.74658203125\n",
            "Batch 350 / 1132 Sammon Loss : 2020.4481201171875\n",
            "Batch 360 / 1132 Sammon Loss : 2164.507568359375\n",
            "Batch 370 / 1132 Sammon Loss : 2216.88330078125\n",
            "Batch 380 / 1132 Sammon Loss : 1778.48876953125\n",
            "Batch 390 / 1132 Sammon Loss : 2166.625244140625\n",
            "Batch 400 / 1132 Sammon Loss : 2607.280517578125\n",
            "Batch 410 / 1132 Sammon Loss : 1899.3524169921875\n",
            "Batch 420 / 1132 Sammon Loss : 1879.0806884765625\n",
            "Batch 430 / 1132 Sammon Loss : 1995.183349609375\n",
            "Batch 440 / 1132 Sammon Loss : 2864.89306640625\n",
            "Batch 450 / 1132 Sammon Loss : 1812.310791015625\n",
            "Batch 460 / 1132 Sammon Loss : 2243.5234375\n",
            "Batch 470 / 1132 Sammon Loss : 1809.965087890625\n",
            "Batch 480 / 1132 Sammon Loss : 2380.49951171875\n",
            "Batch 490 / 1132 Sammon Loss : 1356.6458740234375\n",
            "Batch 500 / 1132 Sammon Loss : 5285.5361328125\n",
            "Batch 510 / 1132 Sammon Loss : 5143.4931640625\n",
            "Batch 520 / 1132 Sammon Loss : 3568.824462890625\n",
            "Batch 530 / 1132 Sammon Loss : 4610.4169921875\n",
            "Batch 540 / 1132 Sammon Loss : 1125.600341796875\n",
            "Batch 550 / 1132 Sammon Loss : 1262.08349609375\n",
            "Batch 560 / 1132 Sammon Loss : 1382.1944580078125\n",
            "Batch 570 / 1132 Sammon Loss : 1399.1826171875\n",
            "Batch 580 / 1132 Sammon Loss : 1603.104736328125\n",
            "Batch 590 / 1132 Sammon Loss : 1724.042724609375\n",
            "Batch 600 / 1132 Sammon Loss : 1235.990478515625\n",
            "Batch 610 / 1132 Sammon Loss : 1089.4266357421875\n",
            "Batch 620 / 1132 Sammon Loss : 2963.64111328125\n",
            "Batch 630 / 1132 Sammon Loss : 3128.857666015625\n",
            "Batch 640 / 1132 Sammon Loss : 2832.54052734375\n",
            "Batch 650 / 1132 Sammon Loss : 3605.8994140625\n",
            "Batch 660 / 1132 Sammon Loss : 1567.66845703125\n",
            "Batch 670 / 1132 Sammon Loss : 1175.572265625\n",
            "Batch 680 / 1132 Sammon Loss : 1695.814453125\n",
            "Batch 690 / 1132 Sammon Loss : 1126.51025390625\n",
            "Batch 700 / 1132 Sammon Loss : 1564.4808349609375\n",
            "Batch 710 / 1132 Sammon Loss : 1097.4796142578125\n",
            "Batch 720 / 1132 Sammon Loss : 1749.98828125\n",
            "Batch 730 / 1132 Sammon Loss : 753.63232421875\n",
            "Batch 740 / 1132 Sammon Loss : 1422.6229248046875\n",
            "Batch 750 / 1132 Sammon Loss : 1076.1947021484375\n",
            "Batch 760 / 1132 Sammon Loss : 1586.0438232421875\n",
            "Batch 770 / 1132 Sammon Loss : 2317.958740234375\n",
            "Batch 780 / 1132 Sammon Loss : 2029.459228515625\n",
            "Batch 790 / 1132 Sammon Loss : 1341.138916015625\n",
            "Batch 800 / 1132 Sammon Loss : 2369.607666015625\n",
            "Batch 810 / 1132 Sammon Loss : 1088.16162109375\n",
            "Batch 820 / 1132 Sammon Loss : 1250.3052978515625\n",
            "Batch 830 / 1132 Sammon Loss : 2166.190673828125\n",
            "Batch 840 / 1132 Sammon Loss : 1785.622314453125\n",
            "Batch 850 / 1132 Sammon Loss : 3565.749267578125\n",
            "Batch 860 / 1132 Sammon Loss : 3854.4833984375\n",
            "Batch 870 / 1132 Sammon Loss : 4252.89013671875\n",
            "Batch 880 / 1132 Sammon Loss : 1803.2379150390625\n",
            "Batch 890 / 1132 Sammon Loss : 1591.9327392578125\n",
            "Batch 900 / 1132 Sammon Loss : 1499.37451171875\n",
            "Batch 910 / 1132 Sammon Loss : 1802.3900146484375\n",
            "Batch 920 / 1132 Sammon Loss : 1295.50390625\n",
            "Batch 930 / 1132 Sammon Loss : 2786.419189453125\n",
            "Batch 940 / 1132 Sammon Loss : 3685.11474609375\n",
            "Batch 950 / 1132 Sammon Loss : 3074.759765625\n",
            "Batch 960 / 1132 Sammon Loss : 2232.055419921875\n",
            "Batch 970 / 1132 Sammon Loss : 1529.735595703125\n",
            "Batch 980 / 1132 Sammon Loss : 1256.30078125\n",
            "Batch 990 / 1132 Sammon Loss : 1707.281494140625\n",
            "Batch 1000 / 1132 Sammon Loss : 1312.299072265625\n",
            "Batch 1010 / 1132 Sammon Loss : 977.2423095703125\n",
            "Batch 1020 / 1132 Sammon Loss : 2038.36181640625\n",
            "Batch 1030 / 1132 Sammon Loss : 1933.615966796875\n",
            "Batch 1040 / 1132 Sammon Loss : 2233.2685546875\n",
            "Batch 1050 / 1132 Sammon Loss : 1521.276611328125\n",
            "Batch 1060 / 1132 Sammon Loss : 1418.741455078125\n",
            "Batch 1070 / 1132 Sammon Loss : 3626.064697265625\n",
            "Batch 1080 / 1132 Sammon Loss : 1992.1226806640625\n",
            "Batch 1090 / 1132 Sammon Loss : 1303.774658203125\n",
            "Batch 1100 / 1132 Sammon Loss : 4332.14990234375\n",
            "Batch 1110 / 1132 Sammon Loss : 1394.824951171875\n",
            "Batch 1120 / 1132 Sammon Loss : 1737.894775390625\n",
            "Batch 1130 / 1132 Sammon Loss : 1550.83203125\n",
            "Batch 10 / 505 Regressor Loss : 109486.0234375\n",
            "Batch 20 / 505 Regressor Loss : 120685.796875\n",
            "Batch 30 / 505 Regressor Loss : 103723.3125\n",
            "Batch 40 / 505 Regressor Loss : 104714.2734375\n",
            "Batch 50 / 505 Regressor Loss : 136223.578125\n",
            "Batch 60 / 505 Regressor Loss : 130353.34375\n",
            "Batch 70 / 505 Regressor Loss : 129813.2734375\n",
            "Batch 80 / 505 Regressor Loss : 132157.1875\n",
            "Batch 90 / 505 Regressor Loss : 135195.1875\n",
            "Batch 100 / 505 Regressor Loss : 109823.328125\n",
            "Batch 110 / 505 Regressor Loss : 127884.296875\n",
            "Batch 120 / 505 Regressor Loss : 116877.421875\n",
            "Batch 130 / 505 Regressor Loss : 131555.78125\n",
            "Batch 140 / 505 Regressor Loss : 137816.5\n",
            "Batch 150 / 505 Regressor Loss : 122178.3125\n",
            "Batch 160 / 505 Regressor Loss : 97816.796875\n",
            "Batch 170 / 505 Regressor Loss : 106738.609375\n",
            "Batch 180 / 505 Regressor Loss : 136010.59375\n",
            "Batch 190 / 505 Regressor Loss : 130764.546875\n",
            "Batch 200 / 505 Regressor Loss : 107169.6796875\n",
            "Batch 210 / 505 Regressor Loss : 115139.1171875\n",
            "Batch 220 / 505 Regressor Loss : 127906.359375\n",
            "Batch 230 / 505 Regressor Loss : 120532.328125\n",
            "Batch 240 / 505 Regressor Loss : 111801.5\n",
            "Batch 250 / 505 Regressor Loss : 153913.75\n",
            "Batch 260 / 505 Regressor Loss : 130713.1875\n",
            "Batch 270 / 505 Regressor Loss : 110845.46875\n",
            "Batch 280 / 505 Regressor Loss : 128950.109375\n",
            "Batch 290 / 505 Regressor Loss : 127509.359375\n",
            "Batch 300 / 505 Regressor Loss : 119563.1796875\n",
            "Batch 310 / 505 Regressor Loss : 123734.796875\n",
            "Batch 320 / 505 Regressor Loss : 150494.59375\n",
            "Batch 330 / 505 Regressor Loss : 116707.015625\n",
            "Batch 340 / 505 Regressor Loss : 113989.875\n",
            "Batch 350 / 505 Regressor Loss : 96749.3984375\n",
            "Batch 360 / 505 Regressor Loss : 103786.921875\n",
            "Batch 370 / 505 Regressor Loss : 124074.4375\n",
            "Batch 380 / 505 Regressor Loss : 107366.1796875\n",
            "Batch 390 / 505 Regressor Loss : 119482.75\n",
            "Batch 400 / 505 Regressor Loss : 143355.171875\n",
            "Batch 410 / 505 Regressor Loss : 108342.625\n",
            "Batch 420 / 505 Regressor Loss : 129529.984375\n",
            "Batch 430 / 505 Regressor Loss : 113856.1171875\n",
            "Batch 440 / 505 Regressor Loss : 115410.3359375\n",
            "Batch 450 / 505 Regressor Loss : 102917.4375\n",
            "Batch 460 / 505 Regressor Loss : 131285.71875\n",
            "Batch 470 / 505 Regressor Loss : 123482.125\n",
            "Batch 480 / 505 Regressor Loss : 124512.7734375\n",
            "Batch 490 / 505 Regressor Loss : 124732.4921875\n",
            "Batch 500 / 505 Regressor Loss : 122199.5\n",
            "Epoch  59 / 100  Sammon Loss : 951.9105834960938  Regressor Loss : 105853.0234375\n",
            "Val Loss : 105379.0234375\n",
            "Batch 10 / 1132 Sammon Loss : 3394.7568359375\n",
            "Batch 20 / 1132 Sammon Loss : 3763.17041015625\n",
            "Batch 30 / 1132 Sammon Loss : 3806.71875\n",
            "Batch 40 / 1132 Sammon Loss : 3957.8017578125\n",
            "Batch 50 / 1132 Sammon Loss : 4171.8818359375\n",
            "Batch 60 / 1132 Sammon Loss : 2076.28271484375\n",
            "Batch 70 / 1132 Sammon Loss : 2111.6982421875\n",
            "Batch 80 / 1132 Sammon Loss : 1813.5211181640625\n",
            "Batch 90 / 1132 Sammon Loss : 1813.7686767578125\n",
            "Batch 100 / 1132 Sammon Loss : 1699.392578125\n",
            "Batch 110 / 1132 Sammon Loss : 1305.33349609375\n",
            "Batch 120 / 1132 Sammon Loss : 732.0308227539062\n",
            "Batch 130 / 1132 Sammon Loss : 1304.6353759765625\n",
            "Batch 140 / 1132 Sammon Loss : 1515.060546875\n",
            "Batch 150 / 1132 Sammon Loss : 1657.40283203125\n",
            "Batch 160 / 1132 Sammon Loss : 1488.660888671875\n",
            "Batch 170 / 1132 Sammon Loss : 3511.968505859375\n",
            "Batch 180 / 1132 Sammon Loss : 3956.218017578125\n",
            "Batch 190 / 1132 Sammon Loss : 2967.892578125\n",
            "Batch 200 / 1132 Sammon Loss : 2765.216796875\n",
            "Batch 210 / 1132 Sammon Loss : 2578.198974609375\n",
            "Batch 220 / 1132 Sammon Loss : 1973.507080078125\n",
            "Batch 230 / 1132 Sammon Loss : 1749.3521728515625\n",
            "Batch 240 / 1132 Sammon Loss : 1139.385986328125\n",
            "Batch 250 / 1132 Sammon Loss : 1039.7978515625\n",
            "Batch 260 / 1132 Sammon Loss : 1257.698486328125\n",
            "Batch 270 / 1132 Sammon Loss : 1392.4371337890625\n",
            "Batch 280 / 1132 Sammon Loss : 1070.400390625\n",
            "Batch 290 / 1132 Sammon Loss : 1169.9229736328125\n",
            "Batch 300 / 1132 Sammon Loss : 2494.123046875\n",
            "Batch 310 / 1132 Sammon Loss : 1174.533203125\n",
            "Batch 320 / 1132 Sammon Loss : 1017.7530517578125\n",
            "Batch 330 / 1132 Sammon Loss : 1408.9072265625\n",
            "Batch 340 / 1132 Sammon Loss : 1193.138916015625\n",
            "Batch 350 / 1132 Sammon Loss : 2061.417236328125\n",
            "Batch 360 / 1132 Sammon Loss : 2233.229736328125\n",
            "Batch 370 / 1132 Sammon Loss : 2258.672119140625\n",
            "Batch 380 / 1132 Sammon Loss : 1823.64697265625\n",
            "Batch 390 / 1132 Sammon Loss : 2237.814453125\n",
            "Batch 400 / 1132 Sammon Loss : 2628.845458984375\n",
            "Batch 410 / 1132 Sammon Loss : 1934.08203125\n",
            "Batch 420 / 1132 Sammon Loss : 1930.306640625\n",
            "Batch 430 / 1132 Sammon Loss : 2034.4857177734375\n",
            "Batch 440 / 1132 Sammon Loss : 2911.00341796875\n",
            "Batch 450 / 1132 Sammon Loss : 1855.75048828125\n",
            "Batch 460 / 1132 Sammon Loss : 2302.83837890625\n",
            "Batch 470 / 1132 Sammon Loss : 1843.525634765625\n",
            "Batch 480 / 1132 Sammon Loss : 2407.787109375\n",
            "Batch 490 / 1132 Sammon Loss : 1393.7757568359375\n",
            "Batch 500 / 1132 Sammon Loss : 5374.431640625\n",
            "Batch 510 / 1132 Sammon Loss : 5240.0439453125\n",
            "Batch 520 / 1132 Sammon Loss : 3642.293701171875\n",
            "Batch 530 / 1132 Sammon Loss : 4664.921875\n",
            "Batch 540 / 1132 Sammon Loss : 1160.84912109375\n",
            "Batch 550 / 1132 Sammon Loss : 1296.722412109375\n",
            "Batch 560 / 1132 Sammon Loss : 1428.372802734375\n",
            "Batch 570 / 1132 Sammon Loss : 1404.95703125\n",
            "Batch 580 / 1132 Sammon Loss : 1655.0369873046875\n",
            "Batch 590 / 1132 Sammon Loss : 1774.8336181640625\n",
            "Batch 600 / 1132 Sammon Loss : 1257.12890625\n",
            "Batch 610 / 1132 Sammon Loss : 1117.6025390625\n",
            "Batch 620 / 1132 Sammon Loss : 3094.625\n",
            "Batch 630 / 1132 Sammon Loss : 3204.4150390625\n",
            "Batch 640 / 1132 Sammon Loss : 2903.903564453125\n",
            "Batch 650 / 1132 Sammon Loss : 3681.2021484375\n",
            "Batch 660 / 1132 Sammon Loss : 1602.81396484375\n",
            "Batch 670 / 1132 Sammon Loss : 1200.1561279296875\n",
            "Batch 680 / 1132 Sammon Loss : 1747.503662109375\n",
            "Batch 690 / 1132 Sammon Loss : 1177.955322265625\n",
            "Batch 700 / 1132 Sammon Loss : 1581.4732666015625\n",
            "Batch 710 / 1132 Sammon Loss : 1117.953369140625\n",
            "Batch 720 / 1132 Sammon Loss : 1758.34033203125\n",
            "Batch 730 / 1132 Sammon Loss : 754.2967529296875\n",
            "Batch 740 / 1132 Sammon Loss : 1447.4921875\n",
            "Batch 750 / 1132 Sammon Loss : 1106.8480224609375\n",
            "Batch 760 / 1132 Sammon Loss : 1598.53564453125\n",
            "Batch 770 / 1132 Sammon Loss : 2373.50146484375\n",
            "Batch 780 / 1132 Sammon Loss : 2060.7568359375\n",
            "Batch 790 / 1132 Sammon Loss : 1333.7935791015625\n",
            "Batch 800 / 1132 Sammon Loss : 2389.759765625\n",
            "Batch 810 / 1132 Sammon Loss : 1080.39990234375\n",
            "Batch 820 / 1132 Sammon Loss : 1250.4892578125\n",
            "Batch 830 / 1132 Sammon Loss : 2203.54736328125\n",
            "Batch 840 / 1132 Sammon Loss : 1810.353515625\n",
            "Batch 850 / 1132 Sammon Loss : 3658.373291015625\n",
            "Batch 860 / 1132 Sammon Loss : 3930.147216796875\n",
            "Batch 870 / 1132 Sammon Loss : 4341.18359375\n",
            "Batch 880 / 1132 Sammon Loss : 1852.49267578125\n",
            "Batch 890 / 1132 Sammon Loss : 1609.8768310546875\n",
            "Batch 900 / 1132 Sammon Loss : 1556.264892578125\n",
            "Batch 910 / 1132 Sammon Loss : 1790.63330078125\n",
            "Batch 920 / 1132 Sammon Loss : 1299.017822265625\n",
            "Batch 930 / 1132 Sammon Loss : 2870.185302734375\n",
            "Batch 940 / 1132 Sammon Loss : 3750.32080078125\n",
            "Batch 950 / 1132 Sammon Loss : 3106.852783203125\n",
            "Batch 960 / 1132 Sammon Loss : 2240.59814453125\n",
            "Batch 970 / 1132 Sammon Loss : 1545.4715576171875\n",
            "Batch 980 / 1132 Sammon Loss : 1275.86767578125\n",
            "Batch 990 / 1132 Sammon Loss : 1719.5225830078125\n",
            "Batch 1000 / 1132 Sammon Loss : 1322.369384765625\n",
            "Batch 1010 / 1132 Sammon Loss : 974.7510375976562\n",
            "Batch 1020 / 1132 Sammon Loss : 2013.30908203125\n",
            "Batch 1030 / 1132 Sammon Loss : 1873.718505859375\n",
            "Batch 1040 / 1132 Sammon Loss : 2218.616943359375\n",
            "Batch 1050 / 1132 Sammon Loss : 1491.1322021484375\n",
            "Batch 1060 / 1132 Sammon Loss : 1398.864990234375\n",
            "Batch 1070 / 1132 Sammon Loss : 3594.293701171875\n",
            "Batch 1080 / 1132 Sammon Loss : 1931.0609130859375\n",
            "Batch 1090 / 1132 Sammon Loss : 1269.3994140625\n",
            "Batch 1100 / 1132 Sammon Loss : 4320.88427734375\n",
            "Batch 1110 / 1132 Sammon Loss : 1399.676025390625\n",
            "Batch 1120 / 1132 Sammon Loss : 1714.15478515625\n",
            "Batch 1130 / 1132 Sammon Loss : 1517.239013671875\n",
            "Batch 10 / 505 Regressor Loss : 109266.359375\n",
            "Batch 20 / 505 Regressor Loss : 120461.953125\n",
            "Batch 30 / 505 Regressor Loss : 103498.3359375\n",
            "Batch 40 / 505 Regressor Loss : 104486.625\n",
            "Batch 50 / 505 Regressor Loss : 135949.203125\n",
            "Batch 60 / 505 Regressor Loss : 130102.890625\n",
            "Batch 70 / 505 Regressor Loss : 129571.828125\n",
            "Batch 80 / 505 Regressor Loss : 131895.609375\n",
            "Batch 90 / 505 Regressor Loss : 134956.171875\n",
            "Batch 100 / 505 Regressor Loss : 109588.1484375\n",
            "Batch 110 / 505 Regressor Loss : 127650.3125\n",
            "Batch 120 / 505 Regressor Loss : 116661.2734375\n",
            "Batch 130 / 505 Regressor Loss : 131321.609375\n",
            "Batch 140 / 505 Regressor Loss : 137567.046875\n",
            "Batch 150 / 505 Regressor Loss : 121945.8359375\n",
            "Batch 160 / 505 Regressor Loss : 97602.296875\n",
            "Batch 170 / 505 Regressor Loss : 106515.875\n",
            "Batch 180 / 505 Regressor Loss : 135761.015625\n",
            "Batch 190 / 505 Regressor Loss : 130504.8359375\n",
            "Batch 200 / 505 Regressor Loss : 106962.5\n",
            "Batch 210 / 505 Regressor Loss : 114907.1171875\n",
            "Batch 220 / 505 Regressor Loss : 127665.609375\n",
            "Batch 230 / 505 Regressor Loss : 120304.1875\n",
            "Batch 240 / 505 Regressor Loss : 111587.296875\n",
            "Batch 250 / 505 Regressor Loss : 153646.859375\n",
            "Batch 260 / 505 Regressor Loss : 130461.0625\n",
            "Batch 270 / 505 Regressor Loss : 110627.3984375\n",
            "Batch 280 / 505 Regressor Loss : 128706.4375\n",
            "Batch 290 / 505 Regressor Loss : 127281.875\n",
            "Batch 300 / 505 Regressor Loss : 119323.75\n",
            "Batch 310 / 505 Regressor Loss : 123499.34375\n",
            "Batch 320 / 505 Regressor Loss : 150224.828125\n",
            "Batch 330 / 505 Regressor Loss : 116468.671875\n",
            "Batch 340 / 505 Regressor Loss : 113743.171875\n",
            "Batch 350 / 505 Regressor Loss : 96533.046875\n",
            "Batch 360 / 505 Regressor Loss : 103584.2421875\n",
            "Batch 370 / 505 Regressor Loss : 123830.6875\n",
            "Batch 380 / 505 Regressor Loss : 107148.0859375\n",
            "Batch 390 / 505 Regressor Loss : 119251.578125\n",
            "Batch 400 / 505 Regressor Loss : 143077.765625\n",
            "Batch 410 / 505 Regressor Loss : 108122.890625\n",
            "Batch 420 / 505 Regressor Loss : 129291.578125\n",
            "Batch 430 / 505 Regressor Loss : 113628.6796875\n",
            "Batch 440 / 505 Regressor Loss : 115176.328125\n",
            "Batch 450 / 505 Regressor Loss : 102709.7734375\n",
            "Batch 460 / 505 Regressor Loss : 131044.578125\n",
            "Batch 470 / 505 Regressor Loss : 123262.421875\n",
            "Batch 480 / 505 Regressor Loss : 124280.140625\n",
            "Batch 490 / 505 Regressor Loss : 124493.09375\n",
            "Batch 500 / 505 Regressor Loss : 121976.640625\n",
            "Epoch  60 / 100  Sammon Loss : 937.5801391601562  Regressor Loss : 105646.140625\n",
            "Val Loss : 105118.9921875\n",
            "Batch 10 / 1132 Sammon Loss : 3360.005615234375\n",
            "Batch 20 / 1132 Sammon Loss : 3722.83740234375\n",
            "Batch 30 / 1132 Sammon Loss : 3752.83740234375\n",
            "Batch 40 / 1132 Sammon Loss : 3899.1630859375\n",
            "Batch 50 / 1132 Sammon Loss : 4110.2353515625\n",
            "Batch 60 / 1132 Sammon Loss : 2037.219970703125\n",
            "Batch 70 / 1132 Sammon Loss : 2077.5224609375\n",
            "Batch 80 / 1132 Sammon Loss : 1759.531005859375\n",
            "Batch 90 / 1132 Sammon Loss : 1765.887939453125\n",
            "Batch 100 / 1132 Sammon Loss : 1626.6573486328125\n",
            "Batch 110 / 1132 Sammon Loss : 1218.6707763671875\n",
            "Batch 120 / 1132 Sammon Loss : 647.3307495117188\n",
            "Batch 130 / 1132 Sammon Loss : 1252.7042236328125\n",
            "Batch 140 / 1132 Sammon Loss : 1403.64404296875\n",
            "Batch 150 / 1132 Sammon Loss : 1611.0975341796875\n",
            "Batch 160 / 1132 Sammon Loss : 1468.3721923828125\n",
            "Batch 170 / 1132 Sammon Loss : 3408.0947265625\n",
            "Batch 180 / 1132 Sammon Loss : 3832.468505859375\n",
            "Batch 190 / 1132 Sammon Loss : 2907.654296875\n",
            "Batch 200 / 1132 Sammon Loss : 2717.63671875\n",
            "Batch 210 / 1132 Sammon Loss : 2543.3134765625\n",
            "Batch 220 / 1132 Sammon Loss : 1915.796875\n",
            "Batch 230 / 1132 Sammon Loss : 1679.48095703125\n",
            "Batch 240 / 1132 Sammon Loss : 1103.3009033203125\n",
            "Batch 250 / 1132 Sammon Loss : 1023.3425903320312\n",
            "Batch 260 / 1132 Sammon Loss : 1221.7479248046875\n",
            "Batch 270 / 1132 Sammon Loss : 1346.27001953125\n",
            "Batch 280 / 1132 Sammon Loss : 1061.805908203125\n",
            "Batch 290 / 1132 Sammon Loss : 1128.960693359375\n",
            "Batch 300 / 1132 Sammon Loss : 2430.86669921875\n",
            "Batch 310 / 1132 Sammon Loss : 1140.244140625\n",
            "Batch 320 / 1132 Sammon Loss : 976.54248046875\n",
            "Batch 330 / 1132 Sammon Loss : 1372.299072265625\n",
            "Batch 340 / 1132 Sammon Loss : 1157.097900390625\n",
            "Batch 350 / 1132 Sammon Loss : 1978.702392578125\n",
            "Batch 360 / 1132 Sammon Loss : 2125.953369140625\n",
            "Batch 370 / 1132 Sammon Loss : 2164.704833984375\n",
            "Batch 380 / 1132 Sammon Loss : 1730.2208251953125\n",
            "Batch 390 / 1132 Sammon Loss : 2127.793701171875\n",
            "Batch 400 / 1132 Sammon Loss : 2560.5830078125\n",
            "Batch 410 / 1132 Sammon Loss : 1859.27587890625\n",
            "Batch 420 / 1132 Sammon Loss : 1854.8515625\n",
            "Batch 430 / 1132 Sammon Loss : 1921.305908203125\n",
            "Batch 440 / 1132 Sammon Loss : 2847.40185546875\n",
            "Batch 450 / 1132 Sammon Loss : 1774.4110107421875\n",
            "Batch 460 / 1132 Sammon Loss : 2239.94384765625\n",
            "Batch 470 / 1132 Sammon Loss : 1804.970703125\n",
            "Batch 480 / 1132 Sammon Loss : 2369.0146484375\n",
            "Batch 490 / 1132 Sammon Loss : 1351.396240234375\n",
            "Batch 500 / 1132 Sammon Loss : 5236.7373046875\n",
            "Batch 510 / 1132 Sammon Loss : 5133.1708984375\n",
            "Batch 520 / 1132 Sammon Loss : 3540.886962890625\n",
            "Batch 530 / 1132 Sammon Loss : 4591.9599609375\n",
            "Batch 540 / 1132 Sammon Loss : 1130.4866943359375\n",
            "Batch 550 / 1132 Sammon Loss : 1258.8768310546875\n",
            "Batch 560 / 1132 Sammon Loss : 1387.7657470703125\n",
            "Batch 570 / 1132 Sammon Loss : 1426.70263671875\n",
            "Batch 580 / 1132 Sammon Loss : 1615.680908203125\n",
            "Batch 590 / 1132 Sammon Loss : 1746.132080078125\n",
            "Batch 600 / 1132 Sammon Loss : 1212.0255126953125\n",
            "Batch 610 / 1132 Sammon Loss : 1116.143310546875\n",
            "Batch 620 / 1132 Sammon Loss : 2898.060302734375\n",
            "Batch 630 / 1132 Sammon Loss : 3061.779541015625\n",
            "Batch 640 / 1132 Sammon Loss : 2748.53564453125\n",
            "Batch 650 / 1132 Sammon Loss : 3521.83251953125\n",
            "Batch 660 / 1132 Sammon Loss : 1569.2139892578125\n",
            "Batch 670 / 1132 Sammon Loss : 1155.4505615234375\n",
            "Batch 680 / 1132 Sammon Loss : 1677.48193359375\n",
            "Batch 690 / 1132 Sammon Loss : 1130.065673828125\n",
            "Batch 700 / 1132 Sammon Loss : 1549.397705078125\n",
            "Batch 710 / 1132 Sammon Loss : 1100.96728515625\n",
            "Batch 720 / 1132 Sammon Loss : 1683.312744140625\n",
            "Batch 730 / 1132 Sammon Loss : 750.760986328125\n",
            "Batch 740 / 1132 Sammon Loss : 1423.9959716796875\n",
            "Batch 750 / 1132 Sammon Loss : 1086.34619140625\n",
            "Batch 760 / 1132 Sammon Loss : 1513.4140625\n",
            "Batch 770 / 1132 Sammon Loss : 2285.012451171875\n",
            "Batch 780 / 1132 Sammon Loss : 2002.5782470703125\n",
            "Batch 790 / 1132 Sammon Loss : 1285.9376220703125\n",
            "Batch 800 / 1132 Sammon Loss : 2320.051513671875\n",
            "Batch 810 / 1132 Sammon Loss : 1059.545654296875\n",
            "Batch 820 / 1132 Sammon Loss : 1226.0908203125\n",
            "Batch 830 / 1132 Sammon Loss : 2136.0966796875\n",
            "Batch 840 / 1132 Sammon Loss : 1759.21728515625\n",
            "Batch 850 / 1132 Sammon Loss : 3533.940185546875\n",
            "Batch 860 / 1132 Sammon Loss : 3817.470458984375\n",
            "Batch 870 / 1132 Sammon Loss : 4193.2568359375\n",
            "Batch 880 / 1132 Sammon Loss : 1784.8154296875\n",
            "Batch 890 / 1132 Sammon Loss : 1544.571533203125\n",
            "Batch 900 / 1132 Sammon Loss : 1479.187744140625\n",
            "Batch 910 / 1132 Sammon Loss : 1796.025390625\n",
            "Batch 920 / 1132 Sammon Loss : 1277.9014892578125\n",
            "Batch 930 / 1132 Sammon Loss : 2732.045166015625\n",
            "Batch 940 / 1132 Sammon Loss : 3624.72314453125\n",
            "Batch 950 / 1132 Sammon Loss : 2997.68115234375\n",
            "Batch 960 / 1132 Sammon Loss : 2177.013671875\n",
            "Batch 970 / 1132 Sammon Loss : 1496.95458984375\n",
            "Batch 980 / 1132 Sammon Loss : 1232.038818359375\n",
            "Batch 990 / 1132 Sammon Loss : 1708.4932861328125\n",
            "Batch 1000 / 1132 Sammon Loss : 1335.25732421875\n",
            "Batch 1010 / 1132 Sammon Loss : 1003.834716796875\n",
            "Batch 1020 / 1132 Sammon Loss : 1998.67041015625\n",
            "Batch 1030 / 1132 Sammon Loss : 1867.96875\n",
            "Batch 1040 / 1132 Sammon Loss : 2203.9716796875\n",
            "Batch 1050 / 1132 Sammon Loss : 1535.642822265625\n",
            "Batch 1060 / 1132 Sammon Loss : 1418.600830078125\n",
            "Batch 1070 / 1132 Sammon Loss : 3546.45361328125\n",
            "Batch 1080 / 1132 Sammon Loss : 1949.665771484375\n",
            "Batch 1090 / 1132 Sammon Loss : 1306.7728271484375\n",
            "Batch 1100 / 1132 Sammon Loss : 4232.626953125\n",
            "Batch 1110 / 1132 Sammon Loss : 1373.4927978515625\n",
            "Batch 1120 / 1132 Sammon Loss : 1727.7679443359375\n",
            "Batch 1130 / 1132 Sammon Loss : 1541.2392578125\n",
            "Batch 10 / 505 Regressor Loss : 109047.0859375\n",
            "Batch 20 / 505 Regressor Loss : 120238.4921875\n",
            "Batch 30 / 505 Regressor Loss : 103273.7734375\n",
            "Batch 40 / 505 Regressor Loss : 104259.375\n",
            "Batch 50 / 505 Regressor Loss : 135675.28125\n",
            "Batch 60 / 505 Regressor Loss : 129852.859375\n",
            "Batch 70 / 505 Regressor Loss : 129330.7734375\n",
            "Batch 80 / 505 Regressor Loss : 131634.484375\n",
            "Batch 90 / 505 Regressor Loss : 134717.53125\n",
            "Batch 100 / 505 Regressor Loss : 109353.375\n",
            "Batch 110 / 505 Regressor Loss : 127416.734375\n",
            "Batch 120 / 505 Regressor Loss : 116445.5\n",
            "Batch 130 / 505 Regressor Loss : 131087.8125\n",
            "Batch 140 / 505 Regressor Loss : 137317.96875\n",
            "Batch 150 / 505 Regressor Loss : 121713.75\n",
            "Batch 160 / 505 Regressor Loss : 97388.203125\n",
            "Batch 170 / 505 Regressor Loss : 106293.5234375\n",
            "Batch 180 / 505 Regressor Loss : 135511.84375\n",
            "Batch 190 / 505 Regressor Loss : 130245.546875\n",
            "Batch 200 / 505 Regressor Loss : 106755.703125\n",
            "Batch 210 / 505 Regressor Loss : 114675.546875\n",
            "Batch 220 / 505 Regressor Loss : 127425.2734375\n",
            "Batch 230 / 505 Regressor Loss : 120076.4609375\n",
            "Batch 240 / 505 Regressor Loss : 111373.484375\n",
            "Batch 250 / 505 Regressor Loss : 153380.34375\n",
            "Batch 260 / 505 Regressor Loss : 130209.3984375\n",
            "Batch 270 / 505 Regressor Loss : 110409.734375\n",
            "Batch 280 / 505 Regressor Loss : 128463.15625\n",
            "Batch 290 / 505 Regressor Loss : 127054.8046875\n",
            "Batch 300 / 505 Regressor Loss : 119084.7421875\n",
            "Batch 310 / 505 Regressor Loss : 123264.2734375\n",
            "Batch 320 / 505 Regressor Loss : 149955.484375\n",
            "Batch 330 / 505 Regressor Loss : 116230.7109375\n",
            "Batch 340 / 505 Regressor Loss : 113496.8671875\n",
            "Batch 350 / 505 Regressor Loss : 96317.0625\n",
            "Batch 360 / 505 Regressor Loss : 103381.953125\n",
            "Batch 370 / 505 Regressor Loss : 123587.3359375\n",
            "Batch 380 / 505 Regressor Loss : 106930.3984375\n",
            "Batch 390 / 505 Regressor Loss : 119020.796875\n",
            "Batch 400 / 505 Regressor Loss : 142800.78125\n",
            "Batch 410 / 505 Regressor Loss : 107903.578125\n",
            "Batch 420 / 505 Regressor Loss : 129053.578125\n",
            "Batch 430 / 505 Regressor Loss : 113401.625\n",
            "Batch 440 / 505 Regressor Loss : 114942.71875\n",
            "Batch 450 / 505 Regressor Loss : 102502.484375\n",
            "Batch 460 / 505 Regressor Loss : 130803.8359375\n",
            "Batch 470 / 505 Regressor Loss : 123043.0859375\n",
            "Batch 480 / 505 Regressor Loss : 124047.890625\n",
            "Batch 490 / 505 Regressor Loss : 124254.1171875\n",
            "Batch 500 / 505 Regressor Loss : 121754.1796875\n",
            "Epoch  61 / 100  Sammon Loss : 973.5684204101562  Regressor Loss : 105439.625\n",
            "Val Loss : 104859.375\n",
            "Batch 10 / 1132 Sammon Loss : 3342.1259765625\n",
            "Batch 20 / 1132 Sammon Loss : 3720.365234375\n",
            "Batch 30 / 1132 Sammon Loss : 3769.9375\n",
            "Batch 40 / 1132 Sammon Loss : 3896.05029296875\n",
            "Batch 50 / 1132 Sammon Loss : 4093.21240234375\n",
            "Batch 60 / 1132 Sammon Loss : 2023.95947265625\n",
            "Batch 70 / 1132 Sammon Loss : 2049.08447265625\n",
            "Batch 80 / 1132 Sammon Loss : 1747.6712646484375\n",
            "Batch 90 / 1132 Sammon Loss : 1762.6048583984375\n",
            "Batch 100 / 1132 Sammon Loss : 1592.264404296875\n",
            "Batch 110 / 1132 Sammon Loss : 1212.9287109375\n",
            "Batch 120 / 1132 Sammon Loss : 681.7366943359375\n",
            "Batch 130 / 1132 Sammon Loss : 1276.6041259765625\n",
            "Batch 140 / 1132 Sammon Loss : 1406.55712890625\n",
            "Batch 150 / 1132 Sammon Loss : 1644.8521728515625\n",
            "Batch 160 / 1132 Sammon Loss : 1487.90087890625\n",
            "Batch 170 / 1132 Sammon Loss : 3482.05908203125\n",
            "Batch 180 / 1132 Sammon Loss : 3884.864501953125\n",
            "Batch 190 / 1132 Sammon Loss : 2929.87353515625\n",
            "Batch 200 / 1132 Sammon Loss : 2727.1826171875\n",
            "Batch 210 / 1132 Sammon Loss : 2548.3583984375\n",
            "Batch 220 / 1132 Sammon Loss : 1949.957275390625\n",
            "Batch 230 / 1132 Sammon Loss : 1720.442138671875\n",
            "Batch 240 / 1132 Sammon Loss : 1105.3779296875\n",
            "Batch 250 / 1132 Sammon Loss : 1075.3087158203125\n",
            "Batch 260 / 1132 Sammon Loss : 1240.3411865234375\n",
            "Batch 270 / 1132 Sammon Loss : 1369.893310546875\n",
            "Batch 280 / 1132 Sammon Loss : 1085.512451171875\n",
            "Batch 290 / 1132 Sammon Loss : 1187.2091064453125\n",
            "Batch 300 / 1132 Sammon Loss : 2491.93896484375\n",
            "Batch 310 / 1132 Sammon Loss : 1176.32373046875\n",
            "Batch 320 / 1132 Sammon Loss : 1031.47802734375\n",
            "Batch 330 / 1132 Sammon Loss : 1386.940185546875\n",
            "Batch 340 / 1132 Sammon Loss : 1167.173095703125\n",
            "Batch 350 / 1132 Sammon Loss : 2032.1639404296875\n",
            "Batch 360 / 1132 Sammon Loss : 2186.76318359375\n",
            "Batch 370 / 1132 Sammon Loss : 2217.17626953125\n",
            "Batch 380 / 1132 Sammon Loss : 1767.512451171875\n",
            "Batch 390 / 1132 Sammon Loss : 2164.93505859375\n",
            "Batch 400 / 1132 Sammon Loss : 2591.89501953125\n",
            "Batch 410 / 1132 Sammon Loss : 1880.368408203125\n",
            "Batch 420 / 1132 Sammon Loss : 1856.4298095703125\n",
            "Batch 430 / 1132 Sammon Loss : 1997.9940185546875\n",
            "Batch 440 / 1132 Sammon Loss : 2828.33544921875\n",
            "Batch 450 / 1132 Sammon Loss : 1855.9443359375\n",
            "Batch 460 / 1132 Sammon Loss : 2251.339599609375\n",
            "Batch 470 / 1132 Sammon Loss : 1808.31787109375\n",
            "Batch 480 / 1132 Sammon Loss : 2380.6328125\n",
            "Batch 490 / 1132 Sammon Loss : 1391.1180419921875\n",
            "Batch 500 / 1132 Sammon Loss : 5270.2294921875\n",
            "Batch 510 / 1132 Sammon Loss : 5145.06640625\n",
            "Batch 520 / 1132 Sammon Loss : 3601.6328125\n",
            "Batch 530 / 1132 Sammon Loss : 4612.48291015625\n",
            "Batch 540 / 1132 Sammon Loss : 1194.323974609375\n",
            "Batch 550 / 1132 Sammon Loss : 1287.9326171875\n",
            "Batch 560 / 1132 Sammon Loss : 1416.619873046875\n",
            "Batch 570 / 1132 Sammon Loss : 1422.6114501953125\n",
            "Batch 580 / 1132 Sammon Loss : 1656.762939453125\n",
            "Batch 590 / 1132 Sammon Loss : 1773.5966796875\n",
            "Batch 600 / 1132 Sammon Loss : 1269.62744140625\n",
            "Batch 610 / 1132 Sammon Loss : 1139.046142578125\n",
            "Batch 620 / 1132 Sammon Loss : 2964.864990234375\n",
            "Batch 630 / 1132 Sammon Loss : 3122.435791015625\n",
            "Batch 640 / 1132 Sammon Loss : 2818.52490234375\n",
            "Batch 650 / 1132 Sammon Loss : 3610.056640625\n",
            "Batch 660 / 1132 Sammon Loss : 1585.5494384765625\n",
            "Batch 670 / 1132 Sammon Loss : 1192.585693359375\n",
            "Batch 680 / 1132 Sammon Loss : 1703.208984375\n",
            "Batch 690 / 1132 Sammon Loss : 1184.688720703125\n",
            "Batch 700 / 1132 Sammon Loss : 1587.01025390625\n",
            "Batch 710 / 1132 Sammon Loss : 1126.191162109375\n",
            "Batch 720 / 1132 Sammon Loss : 1721.8482666015625\n",
            "Batch 730 / 1132 Sammon Loss : 793.1259765625\n",
            "Batch 740 / 1132 Sammon Loss : 1475.068603515625\n",
            "Batch 750 / 1132 Sammon Loss : 1112.1864013671875\n",
            "Batch 760 / 1132 Sammon Loss : 1543.4866943359375\n",
            "Batch 770 / 1132 Sammon Loss : 2350.2119140625\n",
            "Batch 780 / 1132 Sammon Loss : 2046.148193359375\n",
            "Batch 790 / 1132 Sammon Loss : 1336.5863037109375\n",
            "Batch 800 / 1132 Sammon Loss : 2368.9814453125\n",
            "Batch 810 / 1132 Sammon Loss : 1111.560546875\n",
            "Batch 820 / 1132 Sammon Loss : 1297.13427734375\n",
            "Batch 830 / 1132 Sammon Loss : 2200.88525390625\n",
            "Batch 840 / 1132 Sammon Loss : 1811.5369873046875\n",
            "Batch 850 / 1132 Sammon Loss : 3556.890625\n",
            "Batch 860 / 1132 Sammon Loss : 3852.845703125\n",
            "Batch 870 / 1132 Sammon Loss : 4241.2685546875\n",
            "Batch 880 / 1132 Sammon Loss : 1853.9833984375\n",
            "Batch 890 / 1132 Sammon Loss : 1600.225830078125\n",
            "Batch 900 / 1132 Sammon Loss : 1511.095458984375\n",
            "Batch 910 / 1132 Sammon Loss : 1800.5311279296875\n",
            "Batch 920 / 1132 Sammon Loss : 1303.170166015625\n",
            "Batch 930 / 1132 Sammon Loss : 2748.701171875\n",
            "Batch 940 / 1132 Sammon Loss : 3669.298828125\n",
            "Batch 950 / 1132 Sammon Loss : 3054.77734375\n",
            "Batch 960 / 1132 Sammon Loss : 2233.67578125\n",
            "Batch 970 / 1132 Sammon Loss : 1529.474365234375\n",
            "Batch 980 / 1132 Sammon Loss : 1218.411865234375\n",
            "Batch 990 / 1132 Sammon Loss : 1676.537353515625\n",
            "Batch 1000 / 1132 Sammon Loss : 1312.916748046875\n",
            "Batch 1010 / 1132 Sammon Loss : 991.603271484375\n",
            "Batch 1020 / 1132 Sammon Loss : 2012.42236328125\n",
            "Batch 1030 / 1132 Sammon Loss : 1901.001708984375\n",
            "Batch 1040 / 1132 Sammon Loss : 2231.302978515625\n",
            "Batch 1050 / 1132 Sammon Loss : 1527.41845703125\n",
            "Batch 1060 / 1132 Sammon Loss : 1430.5296630859375\n",
            "Batch 1070 / 1132 Sammon Loss : 3619.3828125\n",
            "Batch 1080 / 1132 Sammon Loss : 1931.6619873046875\n",
            "Batch 1090 / 1132 Sammon Loss : 1272.4395751953125\n",
            "Batch 1100 / 1132 Sammon Loss : 4242.84814453125\n",
            "Batch 1110 / 1132 Sammon Loss : 1397.801513671875\n",
            "Batch 1120 / 1132 Sammon Loss : 1731.0638427734375\n",
            "Batch 1130 / 1132 Sammon Loss : 1527.806884765625\n",
            "Batch 10 / 505 Regressor Loss : 108828.203125\n",
            "Batch 20 / 505 Regressor Loss : 120015.4609375\n",
            "Batch 30 / 505 Regressor Loss : 103049.609375\n",
            "Batch 40 / 505 Regressor Loss : 104032.53125\n",
            "Batch 50 / 505 Regressor Loss : 135401.78125\n",
            "Batch 60 / 505 Regressor Loss : 129603.21875\n",
            "Batch 70 / 505 Regressor Loss : 129090.109375\n",
            "Batch 80 / 505 Regressor Loss : 131373.78125\n",
            "Batch 90 / 505 Regressor Loss : 134479.296875\n",
            "Batch 100 / 505 Regressor Loss : 109118.96875\n",
            "Batch 110 / 505 Regressor Loss : 127183.5234375\n",
            "Batch 120 / 505 Regressor Loss : 116230.125\n",
            "Batch 130 / 505 Regressor Loss : 130854.421875\n",
            "Batch 140 / 505 Regressor Loss : 137069.296875\n",
            "Batch 150 / 505 Regressor Loss : 121482.0625\n",
            "Batch 160 / 505 Regressor Loss : 97174.5\n",
            "Batch 170 / 505 Regressor Loss : 106071.59375\n",
            "Batch 180 / 505 Regressor Loss : 135263.09375\n",
            "Batch 190 / 505 Regressor Loss : 129986.6875\n",
            "Batch 200 / 505 Regressor Loss : 106549.296875\n",
            "Batch 210 / 505 Regressor Loss : 114444.359375\n",
            "Batch 220 / 505 Regressor Loss : 127185.34375\n",
            "Batch 230 / 505 Regressor Loss : 119849.125\n",
            "Batch 240 / 505 Regressor Loss : 111160.0546875\n",
            "Batch 250 / 505 Regressor Loss : 153114.25\n",
            "Batch 260 / 505 Regressor Loss : 129958.15625\n",
            "Batch 270 / 505 Regressor Loss : 110192.4609375\n",
            "Batch 280 / 505 Regressor Loss : 128220.3046875\n",
            "Batch 290 / 505 Regressor Loss : 126828.125\n",
            "Batch 300 / 505 Regressor Loss : 118846.125\n",
            "Batch 310 / 505 Regressor Loss : 123029.609375\n",
            "Batch 320 / 505 Regressor Loss : 149686.546875\n",
            "Batch 330 / 505 Regressor Loss : 115993.171875\n",
            "Batch 340 / 505 Regressor Loss : 113250.984375\n",
            "Batch 350 / 505 Regressor Loss : 96101.5\n",
            "Batch 360 / 505 Regressor Loss : 103180.046875\n",
            "Batch 370 / 505 Regressor Loss : 123344.421875\n",
            "Batch 380 / 505 Regressor Loss : 106713.109375\n",
            "Batch 390 / 505 Regressor Loss : 118790.390625\n",
            "Batch 400 / 505 Regressor Loss : 142524.234375\n",
            "Batch 410 / 505 Regressor Loss : 107684.65625\n",
            "Batch 420 / 505 Regressor Loss : 128815.9609375\n",
            "Batch 430 / 505 Regressor Loss : 113174.984375\n",
            "Batch 440 / 505 Regressor Loss : 114709.5234375\n",
            "Batch 450 / 505 Regressor Loss : 102295.609375\n",
            "Batch 460 / 505 Regressor Loss : 130563.5\n",
            "Batch 470 / 505 Regressor Loss : 122824.125\n",
            "Batch 480 / 505 Regressor Loss : 123816.046875\n",
            "Batch 490 / 505 Regressor Loss : 124015.5\n",
            "Batch 500 / 505 Regressor Loss : 121532.125\n",
            "Epoch  62 / 100  Sammon Loss : 985.129638671875  Regressor Loss : 105233.5234375\n",
            "Val Loss : 104600.15625\n",
            "Batch 10 / 1132 Sammon Loss : 3286.579833984375\n",
            "Batch 20 / 1132 Sammon Loss : 3715.6591796875\n",
            "Batch 30 / 1132 Sammon Loss : 3742.25048828125\n",
            "Batch 40 / 1132 Sammon Loss : 3905.148681640625\n",
            "Batch 50 / 1132 Sammon Loss : 4077.90869140625\n",
            "Batch 60 / 1132 Sammon Loss : 2041.74853515625\n",
            "Batch 70 / 1132 Sammon Loss : 2069.401611328125\n",
            "Batch 80 / 1132 Sammon Loss : 1774.8782958984375\n",
            "Batch 90 / 1132 Sammon Loss : 1792.8154296875\n",
            "Batch 100 / 1132 Sammon Loss : 1632.2099609375\n",
            "Batch 110 / 1132 Sammon Loss : 1240.76708984375\n",
            "Batch 120 / 1132 Sammon Loss : 661.02099609375\n",
            "Batch 130 / 1132 Sammon Loss : 1298.89111328125\n",
            "Batch 140 / 1132 Sammon Loss : 1431.07421875\n",
            "Batch 150 / 1132 Sammon Loss : 1625.9560546875\n",
            "Batch 160 / 1132 Sammon Loss : 1506.500244140625\n",
            "Batch 170 / 1132 Sammon Loss : 3469.68115234375\n",
            "Batch 180 / 1132 Sammon Loss : 3926.9931640625\n",
            "Batch 190 / 1132 Sammon Loss : 2966.42578125\n",
            "Batch 200 / 1132 Sammon Loss : 2760.28271484375\n",
            "Batch 210 / 1132 Sammon Loss : 2569.39794921875\n",
            "Batch 220 / 1132 Sammon Loss : 1955.879638671875\n",
            "Batch 230 / 1132 Sammon Loss : 1712.895263671875\n",
            "Batch 240 / 1132 Sammon Loss : 1129.01806640625\n",
            "Batch 250 / 1132 Sammon Loss : 1072.02734375\n",
            "Batch 260 / 1132 Sammon Loss : 1265.90185546875\n",
            "Batch 270 / 1132 Sammon Loss : 1384.8203125\n",
            "Batch 280 / 1132 Sammon Loss : 1041.816162109375\n",
            "Batch 290 / 1132 Sammon Loss : 1154.846923828125\n",
            "Batch 300 / 1132 Sammon Loss : 2507.219482421875\n",
            "Batch 310 / 1132 Sammon Loss : 1182.101318359375\n",
            "Batch 320 / 1132 Sammon Loss : 1032.189208984375\n",
            "Batch 330 / 1132 Sammon Loss : 1391.690673828125\n",
            "Batch 340 / 1132 Sammon Loss : 1183.0369873046875\n",
            "Batch 350 / 1132 Sammon Loss : 2047.5340576171875\n",
            "Batch 360 / 1132 Sammon Loss : 2206.72607421875\n",
            "Batch 370 / 1132 Sammon Loss : 2241.730224609375\n",
            "Batch 380 / 1132 Sammon Loss : 1787.734130859375\n",
            "Batch 390 / 1132 Sammon Loss : 2174.065673828125\n",
            "Batch 400 / 1132 Sammon Loss : 2623.517578125\n",
            "Batch 410 / 1132 Sammon Loss : 1923.478515625\n",
            "Batch 420 / 1132 Sammon Loss : 1894.51953125\n",
            "Batch 430 / 1132 Sammon Loss : 2048.246826171875\n",
            "Batch 440 / 1132 Sammon Loss : 3005.1025390625\n",
            "Batch 450 / 1132 Sammon Loss : 1834.311767578125\n",
            "Batch 460 / 1132 Sammon Loss : 2306.083984375\n",
            "Batch 470 / 1132 Sammon Loss : 1842.049560546875\n",
            "Batch 480 / 1132 Sammon Loss : 2400.78271484375\n",
            "Batch 490 / 1132 Sammon Loss : 1348.597412109375\n",
            "Batch 500 / 1132 Sammon Loss : 5411.75537109375\n",
            "Batch 510 / 1132 Sammon Loss : 5225.40966796875\n",
            "Batch 520 / 1132 Sammon Loss : 3687.03125\n",
            "Batch 530 / 1132 Sammon Loss : 4681.2177734375\n",
            "Batch 540 / 1132 Sammon Loss : 1115.1640625\n",
            "Batch 550 / 1132 Sammon Loss : 1265.7841796875\n",
            "Batch 560 / 1132 Sammon Loss : 1397.365234375\n",
            "Batch 570 / 1132 Sammon Loss : 1394.54150390625\n",
            "Batch 580 / 1132 Sammon Loss : 1594.322998046875\n",
            "Batch 590 / 1132 Sammon Loss : 1735.957275390625\n",
            "Batch 600 / 1132 Sammon Loss : 1300.98828125\n",
            "Batch 610 / 1132 Sammon Loss : 1098.2176513671875\n",
            "Batch 620 / 1132 Sammon Loss : 3028.8056640625\n",
            "Batch 630 / 1132 Sammon Loss : 3155.72900390625\n",
            "Batch 640 / 1132 Sammon Loss : 2841.717529296875\n",
            "Batch 650 / 1132 Sammon Loss : 3651.312255859375\n",
            "Batch 660 / 1132 Sammon Loss : 1568.509033203125\n",
            "Batch 670 / 1132 Sammon Loss : 1226.87841796875\n",
            "Batch 680 / 1132 Sammon Loss : 1714.681884765625\n",
            "Batch 690 / 1132 Sammon Loss : 1146.168701171875\n",
            "Batch 700 / 1132 Sammon Loss : 1551.6849365234375\n",
            "Batch 710 / 1132 Sammon Loss : 1100.556884765625\n",
            "Batch 720 / 1132 Sammon Loss : 1743.64697265625\n",
            "Batch 730 / 1132 Sammon Loss : 761.008056640625\n",
            "Batch 740 / 1132 Sammon Loss : 1431.22119140625\n",
            "Batch 750 / 1132 Sammon Loss : 1084.7442626953125\n",
            "Batch 760 / 1132 Sammon Loss : 1535.00048828125\n",
            "Batch 770 / 1132 Sammon Loss : 2344.92333984375\n",
            "Batch 780 / 1132 Sammon Loss : 2059.88525390625\n",
            "Batch 790 / 1132 Sammon Loss : 1328.633056640625\n",
            "Batch 800 / 1132 Sammon Loss : 2351.330078125\n",
            "Batch 810 / 1132 Sammon Loss : 1097.4647216796875\n",
            "Batch 820 / 1132 Sammon Loss : 1244.32861328125\n",
            "Batch 830 / 1132 Sammon Loss : 2186.3251953125\n",
            "Batch 840 / 1132 Sammon Loss : 1797.65576171875\n",
            "Batch 850 / 1132 Sammon Loss : 3617.67138671875\n",
            "Batch 860 / 1132 Sammon Loss : 3914.965576171875\n",
            "Batch 870 / 1132 Sammon Loss : 4317.05322265625\n",
            "Batch 880 / 1132 Sammon Loss : 1842.944580078125\n",
            "Batch 890 / 1132 Sammon Loss : 1602.650634765625\n",
            "Batch 900 / 1132 Sammon Loss : 1543.73486328125\n",
            "Batch 910 / 1132 Sammon Loss : 1785.5745849609375\n",
            "Batch 920 / 1132 Sammon Loss : 1277.2244873046875\n",
            "Batch 930 / 1132 Sammon Loss : 2868.16650390625\n",
            "Batch 940 / 1132 Sammon Loss : 3724.07470703125\n",
            "Batch 950 / 1132 Sammon Loss : 3084.767822265625\n",
            "Batch 960 / 1132 Sammon Loss : 2232.342041015625\n",
            "Batch 970 / 1132 Sammon Loss : 1529.1826171875\n",
            "Batch 980 / 1132 Sammon Loss : 1277.647216796875\n",
            "Batch 990 / 1132 Sammon Loss : 1727.2841796875\n",
            "Batch 1000 / 1132 Sammon Loss : 1337.8453369140625\n",
            "Batch 1010 / 1132 Sammon Loss : 1030.0274658203125\n",
            "Batch 1020 / 1132 Sammon Loss : 2070.5166015625\n",
            "Batch 1030 / 1132 Sammon Loss : 1917.62353515625\n",
            "Batch 1040 / 1132 Sammon Loss : 2246.80859375\n",
            "Batch 1050 / 1132 Sammon Loss : 1536.640625\n",
            "Batch 1060 / 1132 Sammon Loss : 1423.85986328125\n",
            "Batch 1070 / 1132 Sammon Loss : 3633.1494140625\n",
            "Batch 1080 / 1132 Sammon Loss : 1994.052734375\n",
            "Batch 1090 / 1132 Sammon Loss : 1306.5218505859375\n",
            "Batch 1100 / 1132 Sammon Loss : 4311.7646484375\n",
            "Batch 1110 / 1132 Sammon Loss : 1405.220703125\n",
            "Batch 1120 / 1132 Sammon Loss : 1753.1240234375\n",
            "Batch 1130 / 1132 Sammon Loss : 1538.1351318359375\n",
            "Batch 10 / 505 Regressor Loss : 108609.7109375\n",
            "Batch 20 / 505 Regressor Loss : 119792.796875\n",
            "Batch 30 / 505 Regressor Loss : 102825.8359375\n",
            "Batch 40 / 505 Regressor Loss : 103806.0859375\n",
            "Batch 50 / 505 Regressor Loss : 135128.6875\n",
            "Batch 60 / 505 Regressor Loss : 129353.984375\n",
            "Batch 70 / 505 Regressor Loss : 128849.84375\n",
            "Batch 80 / 505 Regressor Loss : 131113.5\n",
            "Batch 90 / 505 Regressor Loss : 134241.453125\n",
            "Batch 100 / 505 Regressor Loss : 108884.9921875\n",
            "Batch 110 / 505 Regressor Loss : 126950.671875\n",
            "Batch 120 / 505 Regressor Loss : 116015.125\n",
            "Batch 130 / 505 Regressor Loss : 130621.3984375\n",
            "Batch 140 / 505 Regressor Loss : 136821.046875\n",
            "Batch 150 / 505 Regressor Loss : 121250.78125\n",
            "Batch 160 / 505 Regressor Loss : 96961.203125\n",
            "Batch 170 / 505 Regressor Loss : 105850.0625\n",
            "Batch 180 / 505 Regressor Loss : 135014.734375\n",
            "Batch 190 / 505 Regressor Loss : 129728.21875\n",
            "Batch 200 / 505 Regressor Loss : 106343.25\n",
            "Batch 210 / 505 Regressor Loss : 114213.578125\n",
            "Batch 220 / 505 Regressor Loss : 126945.8359375\n",
            "Batch 230 / 505 Regressor Loss : 119622.1875\n",
            "Batch 240 / 505 Regressor Loss : 110947.0\n",
            "Batch 250 / 505 Regressor Loss : 152848.5625\n",
            "Batch 260 / 505 Regressor Loss : 129707.3359375\n",
            "Batch 270 / 505 Regressor Loss : 109975.59375\n",
            "Batch 280 / 505 Regressor Loss : 127977.859375\n",
            "Batch 290 / 505 Regressor Loss : 126601.84375\n",
            "Batch 300 / 505 Regressor Loss : 118607.90625\n",
            "Batch 310 / 505 Regressor Loss : 122795.3359375\n",
            "Batch 320 / 505 Regressor Loss : 149418.0625\n",
            "Batch 330 / 505 Regressor Loss : 115756.0234375\n",
            "Batch 340 / 505 Regressor Loss : 113005.515625\n",
            "Batch 350 / 505 Regressor Loss : 95886.3359375\n",
            "Batch 360 / 505 Regressor Loss : 102978.5234375\n",
            "Batch 370 / 505 Regressor Loss : 123101.8984375\n",
            "Batch 380 / 505 Regressor Loss : 106496.21875\n",
            "Batch 390 / 505 Regressor Loss : 118560.390625\n",
            "Batch 400 / 505 Regressor Loss : 142248.140625\n",
            "Batch 410 / 505 Regressor Loss : 107466.15625\n",
            "Batch 420 / 505 Regressor Loss : 128578.75\n",
            "Batch 430 / 505 Regressor Loss : 112948.7109375\n",
            "Batch 440 / 505 Regressor Loss : 114476.703125\n",
            "Batch 450 / 505 Regressor Loss : 102089.1171875\n",
            "Batch 460 / 505 Regressor Loss : 130323.5625\n",
            "Batch 470 / 505 Regressor Loss : 122605.5546875\n",
            "Batch 480 / 505 Regressor Loss : 123584.59375\n",
            "Batch 490 / 505 Regressor Loss : 123777.3125\n",
            "Batch 500 / 505 Regressor Loss : 121310.4609375\n",
            "Epoch  63 / 100  Sammon Loss : 988.8436889648438  Regressor Loss : 105027.796875\n",
            "Val Loss : 104341.34375\n",
            "Batch 10 / 1132 Sammon Loss : 3328.684814453125\n",
            "Batch 20 / 1132 Sammon Loss : 3737.78271484375\n",
            "Batch 30 / 1132 Sammon Loss : 3775.888671875\n",
            "Batch 40 / 1132 Sammon Loss : 3943.469482421875\n",
            "Batch 50 / 1132 Sammon Loss : 4120.74462890625\n",
            "Batch 60 / 1132 Sammon Loss : 2069.583251953125\n",
            "Batch 70 / 1132 Sammon Loss : 2097.212646484375\n",
            "Batch 80 / 1132 Sammon Loss : 1804.274169921875\n",
            "Batch 90 / 1132 Sammon Loss : 1801.2044677734375\n",
            "Batch 100 / 1132 Sammon Loss : 1662.20947265625\n",
            "Batch 110 / 1132 Sammon Loss : 1257.7020263671875\n",
            "Batch 120 / 1132 Sammon Loss : 642.4448852539062\n",
            "Batch 130 / 1132 Sammon Loss : 1273.1630859375\n",
            "Batch 140 / 1132 Sammon Loss : 1449.22998046875\n",
            "Batch 150 / 1132 Sammon Loss : 1619.4442138671875\n",
            "Batch 160 / 1132 Sammon Loss : 1489.438720703125\n",
            "Batch 170 / 1132 Sammon Loss : 3484.42529296875\n",
            "Batch 180 / 1132 Sammon Loss : 3939.983154296875\n",
            "Batch 190 / 1132 Sammon Loss : 2936.0283203125\n",
            "Batch 200 / 1132 Sammon Loss : 2758.58349609375\n",
            "Batch 210 / 1132 Sammon Loss : 2573.71923828125\n",
            "Batch 220 / 1132 Sammon Loss : 1957.259521484375\n",
            "Batch 230 / 1132 Sammon Loss : 1729.5289306640625\n",
            "Batch 240 / 1132 Sammon Loss : 1133.0029296875\n",
            "Batch 250 / 1132 Sammon Loss : 1049.3865966796875\n",
            "Batch 260 / 1132 Sammon Loss : 1246.744384765625\n",
            "Batch 270 / 1132 Sammon Loss : 1378.76220703125\n",
            "Batch 280 / 1132 Sammon Loss : 1074.257568359375\n",
            "Batch 290 / 1132 Sammon Loss : 1149.9830322265625\n",
            "Batch 300 / 1132 Sammon Loss : 2466.180419921875\n",
            "Batch 310 / 1132 Sammon Loss : 1166.36962890625\n",
            "Batch 320 / 1132 Sammon Loss : 1003.5277099609375\n",
            "Batch 330 / 1132 Sammon Loss : 1401.7132568359375\n",
            "Batch 340 / 1132 Sammon Loss : 1177.920166015625\n",
            "Batch 350 / 1132 Sammon Loss : 2042.19140625\n",
            "Batch 360 / 1132 Sammon Loss : 2205.433349609375\n",
            "Batch 370 / 1132 Sammon Loss : 2273.21875\n",
            "Batch 380 / 1132 Sammon Loss : 1793.4505615234375\n",
            "Batch 390 / 1132 Sammon Loss : 2179.459228515625\n",
            "Batch 400 / 1132 Sammon Loss : 2626.71435546875\n",
            "Batch 410 / 1132 Sammon Loss : 1906.823486328125\n",
            "Batch 420 / 1132 Sammon Loss : 1902.3719482421875\n",
            "Batch 430 / 1132 Sammon Loss : 2023.72314453125\n",
            "Batch 440 / 1132 Sammon Loss : 2939.37353515625\n",
            "Batch 450 / 1132 Sammon Loss : 1832.05419921875\n",
            "Batch 460 / 1132 Sammon Loss : 2267.96240234375\n",
            "Batch 470 / 1132 Sammon Loss : 1823.6141357421875\n",
            "Batch 480 / 1132 Sammon Loss : 2408.728515625\n",
            "Batch 490 / 1132 Sammon Loss : 1377.220703125\n",
            "Batch 500 / 1132 Sammon Loss : 5363.9765625\n",
            "Batch 510 / 1132 Sammon Loss : 5206.7177734375\n",
            "Batch 520 / 1132 Sammon Loss : 3636.860595703125\n",
            "Batch 530 / 1132 Sammon Loss : 4693.7314453125\n",
            "Batch 540 / 1132 Sammon Loss : 1150.4822998046875\n",
            "Batch 550 / 1132 Sammon Loss : 1285.886962890625\n",
            "Batch 560 / 1132 Sammon Loss : 1415.85009765625\n",
            "Batch 570 / 1132 Sammon Loss : 1427.9183349609375\n",
            "Batch 580 / 1132 Sammon Loss : 1616.9830322265625\n",
            "Batch 590 / 1132 Sammon Loss : 1744.859375\n",
            "Batch 600 / 1132 Sammon Loss : 1300.934326171875\n",
            "Batch 610 / 1132 Sammon Loss : 1129.458251953125\n",
            "Batch 620 / 1132 Sammon Loss : 3024.241943359375\n",
            "Batch 630 / 1132 Sammon Loss : 3154.7236328125\n",
            "Batch 640 / 1132 Sammon Loss : 2850.8427734375\n",
            "Batch 650 / 1132 Sammon Loss : 3676.718505859375\n",
            "Batch 660 / 1132 Sammon Loss : 1587.77783203125\n",
            "Batch 670 / 1132 Sammon Loss : 1245.78955078125\n",
            "Batch 680 / 1132 Sammon Loss : 1739.5615234375\n",
            "Batch 690 / 1132 Sammon Loss : 1158.459716796875\n",
            "Batch 700 / 1132 Sammon Loss : 1563.3076171875\n",
            "Batch 710 / 1132 Sammon Loss : 1122.1094970703125\n",
            "Batch 720 / 1132 Sammon Loss : 1729.724365234375\n",
            "Batch 730 / 1132 Sammon Loss : 794.8212890625\n",
            "Batch 740 / 1132 Sammon Loss : 1475.2193603515625\n",
            "Batch 750 / 1132 Sammon Loss : 1143.8724365234375\n",
            "Batch 760 / 1132 Sammon Loss : 1548.066650390625\n",
            "Batch 770 / 1132 Sammon Loss : 2351.225341796875\n",
            "Batch 780 / 1132 Sammon Loss : 2079.35205078125\n",
            "Batch 790 / 1132 Sammon Loss : 1349.1748046875\n",
            "Batch 800 / 1132 Sammon Loss : 2372.454345703125\n",
            "Batch 810 / 1132 Sammon Loss : 1116.9423828125\n",
            "Batch 820 / 1132 Sammon Loss : 1324.16357421875\n",
            "Batch 830 / 1132 Sammon Loss : 2241.638427734375\n",
            "Batch 840 / 1132 Sammon Loss : 1884.2047119140625\n",
            "Batch 850 / 1132 Sammon Loss : 3608.47412109375\n",
            "Batch 860 / 1132 Sammon Loss : 3921.79443359375\n",
            "Batch 870 / 1132 Sammon Loss : 4301.45361328125\n",
            "Batch 880 / 1132 Sammon Loss : 1854.1263427734375\n",
            "Batch 890 / 1132 Sammon Loss : 1608.009521484375\n",
            "Batch 900 / 1132 Sammon Loss : 1541.0164794921875\n",
            "Batch 910 / 1132 Sammon Loss : 1848.6304931640625\n",
            "Batch 920 / 1132 Sammon Loss : 1317.264892578125\n",
            "Batch 930 / 1132 Sammon Loss : 2850.07470703125\n",
            "Batch 940 / 1132 Sammon Loss : 3721.4072265625\n",
            "Batch 950 / 1132 Sammon Loss : 3097.0078125\n",
            "Batch 960 / 1132 Sammon Loss : 2257.43408203125\n",
            "Batch 970 / 1132 Sammon Loss : 1558.51708984375\n",
            "Batch 980 / 1132 Sammon Loss : 1305.52685546875\n",
            "Batch 990 / 1132 Sammon Loss : 1734.5751953125\n",
            "Batch 1000 / 1132 Sammon Loss : 1354.091796875\n",
            "Batch 1010 / 1132 Sammon Loss : 1010.584716796875\n",
            "Batch 1020 / 1132 Sammon Loss : 2048.0361328125\n",
            "Batch 1030 / 1132 Sammon Loss : 1937.54150390625\n",
            "Batch 1040 / 1132 Sammon Loss : 2244.66455078125\n",
            "Batch 1050 / 1132 Sammon Loss : 1553.2275390625\n",
            "Batch 1060 / 1132 Sammon Loss : 1447.94140625\n",
            "Batch 1070 / 1132 Sammon Loss : 3642.90234375\n",
            "Batch 1080 / 1132 Sammon Loss : 2028.6962890625\n",
            "Batch 1090 / 1132 Sammon Loss : 1356.53466796875\n",
            "Batch 1100 / 1132 Sammon Loss : 4362.93310546875\n",
            "Batch 1110 / 1132 Sammon Loss : 1424.20068359375\n",
            "Batch 1120 / 1132 Sammon Loss : 1798.61669921875\n",
            "Batch 1130 / 1132 Sammon Loss : 1569.3544921875\n",
            "Batch 10 / 505 Regressor Loss : 108391.609375\n",
            "Batch 20 / 505 Regressor Loss : 119570.546875\n",
            "Batch 30 / 505 Regressor Loss : 102602.484375\n",
            "Batch 40 / 505 Regressor Loss : 103580.0234375\n",
            "Batch 50 / 505 Regressor Loss : 134856.03125\n",
            "Batch 60 / 505 Regressor Loss : 129105.171875\n",
            "Batch 70 / 505 Regressor Loss : 128609.96875\n",
            "Batch 80 / 505 Regressor Loss : 130853.6484375\n",
            "Batch 90 / 505 Regressor Loss : 134003.984375\n",
            "Batch 100 / 505 Regressor Loss : 108651.3984375\n",
            "Batch 110 / 505 Regressor Loss : 126718.234375\n",
            "Batch 120 / 505 Regressor Loss : 115800.5234375\n",
            "Batch 130 / 505 Regressor Loss : 130388.75\n",
            "Batch 140 / 505 Regressor Loss : 136573.203125\n",
            "Batch 150 / 505 Regressor Loss : 121019.890625\n",
            "Batch 160 / 505 Regressor Loss : 96748.3125\n",
            "Batch 170 / 505 Regressor Loss : 105628.9296875\n",
            "Batch 180 / 505 Regressor Loss : 134766.78125\n",
            "Batch 190 / 505 Regressor Loss : 129470.1875\n",
            "Batch 200 / 505 Regressor Loss : 106137.609375\n",
            "Batch 210 / 505 Regressor Loss : 113983.1875\n",
            "Batch 220 / 505 Regressor Loss : 126706.734375\n",
            "Batch 230 / 505 Regressor Loss : 119395.6484375\n",
            "Batch 240 / 505 Regressor Loss : 110734.3359375\n",
            "Batch 250 / 505 Regressor Loss : 152583.28125\n",
            "Batch 260 / 505 Regressor Loss : 129456.9609375\n",
            "Batch 270 / 505 Regressor Loss : 109759.1171875\n",
            "Batch 280 / 505 Regressor Loss : 127735.8125\n",
            "Batch 290 / 505 Regressor Loss : 126375.953125\n",
            "Batch 300 / 505 Regressor Loss : 118370.09375\n",
            "Batch 310 / 505 Regressor Loss : 122561.4609375\n",
            "Batch 320 / 505 Regressor Loss : 149150.0\n",
            "Batch 330 / 505 Regressor Loss : 115519.2734375\n",
            "Batch 340 / 505 Regressor Loss : 112760.453125\n",
            "Batch 350 / 505 Regressor Loss : 95671.5625\n",
            "Batch 360 / 505 Regressor Loss : 102777.375\n",
            "Batch 370 / 505 Regressor Loss : 122859.8046875\n",
            "Batch 380 / 505 Regressor Loss : 106279.734375\n",
            "Batch 390 / 505 Regressor Loss : 118330.7734375\n",
            "Batch 400 / 505 Regressor Loss : 141972.5\n",
            "Batch 410 / 505 Regressor Loss : 107248.078125\n",
            "Batch 420 / 505 Regressor Loss : 128341.921875\n",
            "Batch 430 / 505 Regressor Loss : 112722.84375\n",
            "Batch 440 / 505 Regressor Loss : 114244.296875\n",
            "Batch 450 / 505 Regressor Loss : 101883.0234375\n",
            "Batch 460 / 505 Regressor Loss : 130084.0234375\n",
            "Batch 470 / 505 Regressor Loss : 122387.359375\n",
            "Batch 480 / 505 Regressor Loss : 123353.5234375\n",
            "Batch 490 / 505 Regressor Loss : 123539.546875\n",
            "Batch 500 / 505 Regressor Loss : 121089.1875\n",
            "Epoch  64 / 100  Sammon Loss : 1049.4573974609375  Regressor Loss : 104822.4609375\n",
            "Val Loss : 104082.9609375\n",
            "Batch 10 / 1132 Sammon Loss : 3389.16064453125\n",
            "Batch 20 / 1132 Sammon Loss : 3767.00390625\n",
            "Batch 30 / 1132 Sammon Loss : 3835.0478515625\n",
            "Batch 40 / 1132 Sammon Loss : 3964.28515625\n",
            "Batch 50 / 1132 Sammon Loss : 4194.92724609375\n",
            "Batch 60 / 1132 Sammon Loss : 2105.543701171875\n",
            "Batch 70 / 1132 Sammon Loss : 2111.437255859375\n",
            "Batch 80 / 1132 Sammon Loss : 1811.611572265625\n",
            "Batch 90 / 1132 Sammon Loss : 1843.6134033203125\n",
            "Batch 100 / 1132 Sammon Loss : 1682.001953125\n",
            "Batch 110 / 1132 Sammon Loss : 1301.410400390625\n",
            "Batch 120 / 1132 Sammon Loss : 681.980224609375\n",
            "Batch 130 / 1132 Sammon Loss : 1307.969482421875\n",
            "Batch 140 / 1132 Sammon Loss : 1514.075927734375\n",
            "Batch 150 / 1132 Sammon Loss : 1680.6827392578125\n",
            "Batch 160 / 1132 Sammon Loss : 1531.382080078125\n",
            "Batch 170 / 1132 Sammon Loss : 3526.86181640625\n",
            "Batch 180 / 1132 Sammon Loss : 3982.685302734375\n",
            "Batch 190 / 1132 Sammon Loss : 2992.26708984375\n",
            "Batch 200 / 1132 Sammon Loss : 2797.070556640625\n",
            "Batch 210 / 1132 Sammon Loss : 2650.189453125\n",
            "Batch 220 / 1132 Sammon Loss : 2011.6134033203125\n",
            "Batch 230 / 1132 Sammon Loss : 1743.7978515625\n",
            "Batch 240 / 1132 Sammon Loss : 1162.826904296875\n",
            "Batch 250 / 1132 Sammon Loss : 1097.056396484375\n",
            "Batch 260 / 1132 Sammon Loss : 1294.608154296875\n",
            "Batch 270 / 1132 Sammon Loss : 1428.39404296875\n",
            "Batch 280 / 1132 Sammon Loss : 1099.4520263671875\n",
            "Batch 290 / 1132 Sammon Loss : 1196.9443359375\n",
            "Batch 300 / 1132 Sammon Loss : 2530.37353515625\n",
            "Batch 310 / 1132 Sammon Loss : 1216.385498046875\n",
            "Batch 320 / 1132 Sammon Loss : 1043.1492919921875\n",
            "Batch 330 / 1132 Sammon Loss : 1413.223876953125\n",
            "Batch 340 / 1132 Sammon Loss : 1204.5838623046875\n",
            "Batch 350 / 1132 Sammon Loss : 2072.22021484375\n",
            "Batch 360 / 1132 Sammon Loss : 2245.298828125\n",
            "Batch 370 / 1132 Sammon Loss : 2268.3125\n",
            "Batch 380 / 1132 Sammon Loss : 1841.6019287109375\n",
            "Batch 390 / 1132 Sammon Loss : 2253.10791015625\n",
            "Batch 400 / 1132 Sammon Loss : 2734.195068359375\n",
            "Batch 410 / 1132 Sammon Loss : 1989.9310302734375\n",
            "Batch 420 / 1132 Sammon Loss : 1983.516845703125\n",
            "Batch 430 / 1132 Sammon Loss : 2098.153076171875\n",
            "Batch 440 / 1132 Sammon Loss : 3021.8291015625\n",
            "Batch 450 / 1132 Sammon Loss : 1858.7857666015625\n",
            "Batch 460 / 1132 Sammon Loss : 2311.821044921875\n",
            "Batch 470 / 1132 Sammon Loss : 1872.9571533203125\n",
            "Batch 480 / 1132 Sammon Loss : 2459.388671875\n",
            "Batch 490 / 1132 Sammon Loss : 1394.7958984375\n",
            "Batch 500 / 1132 Sammon Loss : 5451.5126953125\n",
            "Batch 510 / 1132 Sammon Loss : 5278.4423828125\n",
            "Batch 520 / 1132 Sammon Loss : 3682.63525390625\n",
            "Batch 530 / 1132 Sammon Loss : 4738.8984375\n",
            "Batch 540 / 1132 Sammon Loss : 1175.5333251953125\n",
            "Batch 550 / 1132 Sammon Loss : 1315.923583984375\n",
            "Batch 560 / 1132 Sammon Loss : 1459.0494384765625\n",
            "Batch 570 / 1132 Sammon Loss : 1446.823486328125\n",
            "Batch 580 / 1132 Sammon Loss : 1647.3494873046875\n",
            "Batch 590 / 1132 Sammon Loss : 1750.7978515625\n",
            "Batch 600 / 1132 Sammon Loss : 1283.69580078125\n",
            "Batch 610 / 1132 Sammon Loss : 1093.340576171875\n",
            "Batch 620 / 1132 Sammon Loss : 3131.2060546875\n",
            "Batch 630 / 1132 Sammon Loss : 3252.2587890625\n",
            "Batch 640 / 1132 Sammon Loss : 2911.8544921875\n",
            "Batch 650 / 1132 Sammon Loss : 3694.60791015625\n",
            "Batch 660 / 1132 Sammon Loss : 1599.129150390625\n",
            "Batch 670 / 1132 Sammon Loss : 1205.12548828125\n",
            "Batch 680 / 1132 Sammon Loss : 1733.8909912109375\n",
            "Batch 690 / 1132 Sammon Loss : 1168.048828125\n",
            "Batch 700 / 1132 Sammon Loss : 1589.06591796875\n",
            "Batch 710 / 1132 Sammon Loss : 1133.9539794921875\n",
            "Batch 720 / 1132 Sammon Loss : 1750.8985595703125\n",
            "Batch 730 / 1132 Sammon Loss : 766.625\n",
            "Batch 740 / 1132 Sammon Loss : 1475.331787109375\n",
            "Batch 750 / 1132 Sammon Loss : 1130.9993896484375\n",
            "Batch 760 / 1132 Sammon Loss : 1602.677490234375\n",
            "Batch 770 / 1132 Sammon Loss : 2376.127685546875\n",
            "Batch 780 / 1132 Sammon Loss : 2085.033203125\n",
            "Batch 790 / 1132 Sammon Loss : 1365.776611328125\n",
            "Batch 800 / 1132 Sammon Loss : 2418.45068359375\n",
            "Batch 810 / 1132 Sammon Loss : 1130.3514404296875\n",
            "Batch 820 / 1132 Sammon Loss : 1304.6865234375\n",
            "Batch 830 / 1132 Sammon Loss : 2223.1416015625\n",
            "Batch 840 / 1132 Sammon Loss : 1861.7333984375\n",
            "Batch 850 / 1132 Sammon Loss : 3620.689697265625\n",
            "Batch 860 / 1132 Sammon Loss : 3941.42041015625\n",
            "Batch 870 / 1132 Sammon Loss : 4341.6064453125\n",
            "Batch 880 / 1132 Sammon Loss : 1881.8922119140625\n",
            "Batch 890 / 1132 Sammon Loss : 1676.6416015625\n",
            "Batch 900 / 1132 Sammon Loss : 1572.611083984375\n",
            "Batch 910 / 1132 Sammon Loss : 1848.5018310546875\n",
            "Batch 920 / 1132 Sammon Loss : 1348.94091796875\n",
            "Batch 930 / 1132 Sammon Loss : 2817.321044921875\n",
            "Batch 940 / 1132 Sammon Loss : 3766.08056640625\n",
            "Batch 950 / 1132 Sammon Loss : 3132.2470703125\n",
            "Batch 960 / 1132 Sammon Loss : 2278.97607421875\n",
            "Batch 970 / 1132 Sammon Loss : 1569.6649169921875\n",
            "Batch 980 / 1132 Sammon Loss : 1294.975830078125\n",
            "Batch 990 / 1132 Sammon Loss : 1760.22509765625\n",
            "Batch 1000 / 1132 Sammon Loss : 1369.435302734375\n",
            "Batch 1010 / 1132 Sammon Loss : 1007.20703125\n",
            "Batch 1020 / 1132 Sammon Loss : 2071.76318359375\n",
            "Batch 1030 / 1132 Sammon Loss : 1952.522216796875\n",
            "Batch 1040 / 1132 Sammon Loss : 2284.10107421875\n",
            "Batch 1050 / 1132 Sammon Loss : 1551.055908203125\n",
            "Batch 1060 / 1132 Sammon Loss : 1471.92529296875\n",
            "Batch 1070 / 1132 Sammon Loss : 3693.28515625\n",
            "Batch 1080 / 1132 Sammon Loss : 2030.3463134765625\n",
            "Batch 1090 / 1132 Sammon Loss : 1353.4237060546875\n",
            "Batch 1100 / 1132 Sammon Loss : 4409.046875\n",
            "Batch 1110 / 1132 Sammon Loss : 1464.8277587890625\n",
            "Batch 1120 / 1132 Sammon Loss : 1779.7498779296875\n",
            "Batch 1130 / 1132 Sammon Loss : 1602.624755859375\n",
            "Batch 10 / 505 Regressor Loss : 108173.8984375\n",
            "Batch 20 / 505 Regressor Loss : 119348.671875\n",
            "Batch 30 / 505 Regressor Loss : 102379.5234375\n",
            "Batch 40 / 505 Regressor Loss : 103354.3671875\n",
            "Batch 50 / 505 Regressor Loss : 134583.796875\n",
            "Batch 60 / 505 Regressor Loss : 128856.765625\n",
            "Batch 70 / 505 Regressor Loss : 128370.5\n",
            "Batch 80 / 505 Regressor Loss : 130594.2421875\n",
            "Batch 90 / 505 Regressor Loss : 133766.921875\n",
            "Batch 100 / 505 Regressor Loss : 108418.1875\n",
            "Batch 110 / 505 Regressor Loss : 126486.15625\n",
            "Batch 120 / 505 Regressor Loss : 115586.3125\n",
            "Batch 130 / 505 Regressor Loss : 130156.4921875\n",
            "Batch 140 / 505 Regressor Loss : 136325.75\n",
            "Batch 150 / 505 Regressor Loss : 120789.390625\n",
            "Batch 160 / 505 Regressor Loss : 96535.8125\n",
            "Batch 170 / 505 Regressor Loss : 105408.1875\n",
            "Batch 180 / 505 Regressor Loss : 134519.25\n",
            "Batch 190 / 505 Regressor Loss : 129212.578125\n",
            "Batch 200 / 505 Regressor Loss : 105932.328125\n",
            "Batch 210 / 505 Regressor Loss : 113753.2109375\n",
            "Batch 220 / 505 Regressor Loss : 126468.0625\n",
            "Batch 230 / 505 Regressor Loss : 119169.5\n",
            "Batch 240 / 505 Regressor Loss : 110522.0625\n",
            "Batch 250 / 505 Regressor Loss : 152318.40625\n",
            "Batch 260 / 505 Regressor Loss : 129207.0\n",
            "Batch 270 / 505 Regressor Loss : 109543.046875\n",
            "Batch 280 / 505 Regressor Loss : 127494.1875\n",
            "Batch 290 / 505 Regressor Loss : 126150.4375\n",
            "Batch 300 / 505 Regressor Loss : 118132.6875\n",
            "Batch 310 / 505 Regressor Loss : 122328.0\n",
            "Batch 320 / 505 Regressor Loss : 148882.359375\n",
            "Batch 330 / 505 Regressor Loss : 115282.9296875\n",
            "Batch 340 / 505 Regressor Loss : 112515.796875\n",
            "Batch 350 / 505 Regressor Loss : 95457.1875\n",
            "Batch 360 / 505 Regressor Loss : 102576.625\n",
            "Batch 370 / 505 Regressor Loss : 122618.1171875\n",
            "Batch 380 / 505 Regressor Loss : 106063.6484375\n",
            "Batch 390 / 505 Regressor Loss : 118101.5546875\n",
            "Batch 400 / 505 Regressor Loss : 141697.28125\n",
            "Batch 410 / 505 Regressor Loss : 107030.390625\n",
            "Batch 420 / 505 Regressor Loss : 128105.5\n",
            "Batch 430 / 505 Regressor Loss : 112497.3671875\n",
            "Batch 440 / 505 Regressor Loss : 114012.28125\n",
            "Batch 450 / 505 Regressor Loss : 101677.3125\n",
            "Batch 460 / 505 Regressor Loss : 129844.8984375\n",
            "Batch 470 / 505 Regressor Loss : 122169.53125\n",
            "Batch 480 / 505 Regressor Loss : 123122.8671875\n",
            "Batch 490 / 505 Regressor Loss : 123302.1484375\n",
            "Batch 500 / 505 Regressor Loss : 120868.3125\n",
            "Epoch  65 / 100  Sammon Loss : 991.38525390625  Regressor Loss : 104617.5\n",
            "Val Loss : 103824.984375\n",
            "Batch 10 / 1132 Sammon Loss : 3433.09619140625\n",
            "Batch 20 / 1132 Sammon Loss : 3888.4775390625\n",
            "Batch 30 / 1132 Sammon Loss : 3855.7607421875\n",
            "Batch 40 / 1132 Sammon Loss : 4061.01171875\n",
            "Batch 50 / 1132 Sammon Loss : 4195.91796875\n",
            "Batch 60 / 1132 Sammon Loss : 2105.59033203125\n",
            "Batch 70 / 1132 Sammon Loss : 2144.108642578125\n",
            "Batch 80 / 1132 Sammon Loss : 1858.595703125\n",
            "Batch 90 / 1132 Sammon Loss : 1867.0830078125\n",
            "Batch 100 / 1132 Sammon Loss : 1681.259765625\n",
            "Batch 110 / 1132 Sammon Loss : 1271.761474609375\n",
            "Batch 120 / 1132 Sammon Loss : 645.3247680664062\n",
            "Batch 130 / 1132 Sammon Loss : 1301.960205078125\n",
            "Batch 140 / 1132 Sammon Loss : 1488.3856201171875\n",
            "Batch 150 / 1132 Sammon Loss : 1667.542724609375\n",
            "Batch 160 / 1132 Sammon Loss : 1523.1767578125\n",
            "Batch 170 / 1132 Sammon Loss : 3575.7109375\n",
            "Batch 180 / 1132 Sammon Loss : 4019.36669921875\n",
            "Batch 190 / 1132 Sammon Loss : 3045.86474609375\n",
            "Batch 200 / 1132 Sammon Loss : 2821.828125\n",
            "Batch 210 / 1132 Sammon Loss : 2634.34912109375\n",
            "Batch 220 / 1132 Sammon Loss : 2008.74169921875\n",
            "Batch 230 / 1132 Sammon Loss : 1771.5992431640625\n",
            "Batch 240 / 1132 Sammon Loss : 1200.40673828125\n",
            "Batch 250 / 1132 Sammon Loss : 1092.9984130859375\n",
            "Batch 260 / 1132 Sammon Loss : 1310.280517578125\n",
            "Batch 270 / 1132 Sammon Loss : 1393.8306884765625\n",
            "Batch 280 / 1132 Sammon Loss : 1131.479248046875\n",
            "Batch 290 / 1132 Sammon Loss : 1179.6318359375\n",
            "Batch 300 / 1132 Sammon Loss : 2489.384521484375\n",
            "Batch 310 / 1132 Sammon Loss : 1202.4287109375\n",
            "Batch 320 / 1132 Sammon Loss : 1030.5361328125\n",
            "Batch 330 / 1132 Sammon Loss : 1412.85595703125\n",
            "Batch 340 / 1132 Sammon Loss : 1214.6737060546875\n",
            "Batch 350 / 1132 Sammon Loss : 2059.518310546875\n",
            "Batch 360 / 1132 Sammon Loss : 2207.6845703125\n",
            "Batch 370 / 1132 Sammon Loss : 2270.787109375\n",
            "Batch 380 / 1132 Sammon Loss : 1839.48388671875\n",
            "Batch 390 / 1132 Sammon Loss : 2244.78466796875\n",
            "Batch 400 / 1132 Sammon Loss : 2706.176025390625\n",
            "Batch 410 / 1132 Sammon Loss : 1952.7593994140625\n",
            "Batch 420 / 1132 Sammon Loss : 1953.0592041015625\n",
            "Batch 430 / 1132 Sammon Loss : 2054.0078125\n",
            "Batch 440 / 1132 Sammon Loss : 2956.335205078125\n",
            "Batch 450 / 1132 Sammon Loss : 1857.65771484375\n",
            "Batch 460 / 1132 Sammon Loss : 2305.117431640625\n",
            "Batch 470 / 1132 Sammon Loss : 1850.263427734375\n",
            "Batch 480 / 1132 Sammon Loss : 2464.197265625\n",
            "Batch 490 / 1132 Sammon Loss : 1375.7227783203125\n",
            "Batch 500 / 1132 Sammon Loss : 5368.12939453125\n",
            "Batch 510 / 1132 Sammon Loss : 5259.7265625\n",
            "Batch 520 / 1132 Sammon Loss : 3666.8740234375\n",
            "Batch 530 / 1132 Sammon Loss : 4681.8759765625\n",
            "Batch 540 / 1132 Sammon Loss : 1141.891357421875\n",
            "Batch 550 / 1132 Sammon Loss : 1297.905029296875\n",
            "Batch 560 / 1132 Sammon Loss : 1433.84375\n",
            "Batch 570 / 1132 Sammon Loss : 1430.86328125\n",
            "Batch 580 / 1132 Sammon Loss : 1637.336669921875\n",
            "Batch 590 / 1132 Sammon Loss : 1768.9034423828125\n",
            "Batch 600 / 1132 Sammon Loss : 1302.89697265625\n",
            "Batch 610 / 1132 Sammon Loss : 1103.206787109375\n",
            "Batch 620 / 1132 Sammon Loss : 3149.92578125\n",
            "Batch 630 / 1132 Sammon Loss : 3258.52685546875\n",
            "Batch 640 / 1132 Sammon Loss : 2923.19482421875\n",
            "Batch 650 / 1132 Sammon Loss : 3759.323486328125\n",
            "Batch 660 / 1132 Sammon Loss : 1640.502685546875\n",
            "Batch 670 / 1132 Sammon Loss : 1249.73046875\n",
            "Batch 680 / 1132 Sammon Loss : 1763.8929443359375\n",
            "Batch 690 / 1132 Sammon Loss : 1197.94091796875\n",
            "Batch 700 / 1132 Sammon Loss : 1606.1475830078125\n",
            "Batch 710 / 1132 Sammon Loss : 1122.6640625\n",
            "Batch 720 / 1132 Sammon Loss : 1776.197265625\n",
            "Batch 730 / 1132 Sammon Loss : 762.2734985351562\n",
            "Batch 740 / 1132 Sammon Loss : 1439.50390625\n",
            "Batch 750 / 1132 Sammon Loss : 1087.5567626953125\n",
            "Batch 760 / 1132 Sammon Loss : 1632.985595703125\n",
            "Batch 770 / 1132 Sammon Loss : 2405.45654296875\n",
            "Batch 780 / 1132 Sammon Loss : 2105.72412109375\n",
            "Batch 790 / 1132 Sammon Loss : 1371.895751953125\n",
            "Batch 800 / 1132 Sammon Loss : 2433.83935546875\n",
            "Batch 810 / 1132 Sammon Loss : 1104.9771728515625\n",
            "Batch 820 / 1132 Sammon Loss : 1261.4608154296875\n",
            "Batch 830 / 1132 Sammon Loss : 2249.7470703125\n",
            "Batch 840 / 1132 Sammon Loss : 1839.179931640625\n",
            "Batch 850 / 1132 Sammon Loss : 3716.928466796875\n",
            "Batch 860 / 1132 Sammon Loss : 4004.3828125\n",
            "Batch 870 / 1132 Sammon Loss : 4398.33837890625\n",
            "Batch 880 / 1132 Sammon Loss : 1929.131591796875\n",
            "Batch 890 / 1132 Sammon Loss : 1648.13623046875\n",
            "Batch 900 / 1132 Sammon Loss : 1566.998291015625\n",
            "Batch 910 / 1132 Sammon Loss : 1836.07080078125\n",
            "Batch 920 / 1132 Sammon Loss : 1325.176513671875\n",
            "Batch 930 / 1132 Sammon Loss : 2920.216796875\n",
            "Batch 940 / 1132 Sammon Loss : 3798.0390625\n",
            "Batch 950 / 1132 Sammon Loss : 3167.5810546875\n",
            "Batch 960 / 1132 Sammon Loss : 2306.438720703125\n",
            "Batch 970 / 1132 Sammon Loss : 1580.264892578125\n",
            "Batch 980 / 1132 Sammon Loss : 1300.176025390625\n",
            "Batch 990 / 1132 Sammon Loss : 1760.5810546875\n",
            "Batch 1000 / 1132 Sammon Loss : 1394.1513671875\n",
            "Batch 1010 / 1132 Sammon Loss : 1054.721435546875\n",
            "Batch 1020 / 1132 Sammon Loss : 2101.103515625\n",
            "Batch 1030 / 1132 Sammon Loss : 1948.928955078125\n",
            "Batch 1040 / 1132 Sammon Loss : 2244.86474609375\n",
            "Batch 1050 / 1132 Sammon Loss : 1570.5538330078125\n",
            "Batch 1060 / 1132 Sammon Loss : 1480.9842529296875\n",
            "Batch 1070 / 1132 Sammon Loss : 3668.646484375\n",
            "Batch 1080 / 1132 Sammon Loss : 2066.764404296875\n",
            "Batch 1090 / 1132 Sammon Loss : 1406.79052734375\n",
            "Batch 1100 / 1132 Sammon Loss : 4394.02734375\n",
            "Batch 1110 / 1132 Sammon Loss : 1470.31884765625\n",
            "Batch 1120 / 1132 Sammon Loss : 1809.620849609375\n",
            "Batch 1130 / 1132 Sammon Loss : 1595.04541015625\n",
            "Batch 10 / 505 Regressor Loss : 107956.578125\n",
            "Batch 20 / 505 Regressor Loss : 119127.1875\n",
            "Batch 30 / 505 Regressor Loss : 102156.96875\n",
            "Batch 40 / 505 Regressor Loss : 103129.109375\n",
            "Batch 50 / 505 Regressor Loss : 134312.03125\n",
            "Batch 60 / 505 Regressor Loss : 128608.765625\n",
            "Batch 70 / 505 Regressor Loss : 128131.421875\n",
            "Batch 80 / 505 Regressor Loss : 130335.265625\n",
            "Batch 90 / 505 Regressor Loss : 133530.25\n",
            "Batch 100 / 505 Regressor Loss : 108185.3984375\n",
            "Batch 110 / 505 Regressor Loss : 126254.4609375\n",
            "Batch 120 / 505 Regressor Loss : 115372.484375\n",
            "Batch 130 / 505 Regressor Loss : 129924.625\n",
            "Batch 140 / 505 Regressor Loss : 136078.71875\n",
            "Batch 150 / 505 Regressor Loss : 120559.296875\n",
            "Batch 160 / 505 Regressor Loss : 96323.7109375\n",
            "Batch 170 / 505 Regressor Loss : 105187.859375\n",
            "Batch 180 / 505 Regressor Loss : 134272.125\n",
            "Batch 190 / 505 Regressor Loss : 128955.375\n",
            "Batch 200 / 505 Regressor Loss : 105727.4296875\n",
            "Batch 210 / 505 Regressor Loss : 113523.640625\n",
            "Batch 220 / 505 Regressor Loss : 126229.8125\n",
            "Batch 230 / 505 Regressor Loss : 118943.75\n",
            "Batch 240 / 505 Regressor Loss : 110310.1875\n",
            "Batch 250 / 505 Regressor Loss : 152053.953125\n",
            "Batch 260 / 505 Regressor Loss : 128957.4921875\n",
            "Batch 270 / 505 Regressor Loss : 109327.390625\n",
            "Batch 280 / 505 Regressor Loss : 127252.96875\n",
            "Batch 290 / 505 Regressor Loss : 125925.3359375\n",
            "Batch 300 / 505 Regressor Loss : 117895.671875\n",
            "Batch 310 / 505 Regressor Loss : 122094.921875\n",
            "Batch 320 / 505 Regressor Loss : 148615.15625\n",
            "Batch 330 / 505 Regressor Loss : 115046.984375\n",
            "Batch 340 / 505 Regressor Loss : 112271.5546875\n",
            "Batch 350 / 505 Regressor Loss : 95243.234375\n",
            "Batch 360 / 505 Regressor Loss : 102376.25\n",
            "Batch 370 / 505 Regressor Loss : 122376.8359375\n",
            "Batch 380 / 505 Regressor Loss : 105847.9609375\n",
            "Batch 390 / 505 Regressor Loss : 117872.7421875\n",
            "Batch 400 / 505 Regressor Loss : 141422.53125\n",
            "Batch 410 / 505 Regressor Loss : 106813.125\n",
            "Batch 420 / 505 Regressor Loss : 127869.484375\n",
            "Batch 430 / 505 Regressor Loss : 112272.2734375\n",
            "Batch 440 / 505 Regressor Loss : 113780.671875\n",
            "Batch 450 / 505 Regressor Loss : 101472.0\n",
            "Batch 460 / 505 Regressor Loss : 129606.171875\n",
            "Batch 470 / 505 Regressor Loss : 121952.0859375\n",
            "Batch 480 / 505 Regressor Loss : 122892.609375\n",
            "Batch 490 / 505 Regressor Loss : 123065.1875\n",
            "Batch 500 / 505 Regressor Loss : 120647.8359375\n",
            "Epoch  66 / 100  Sammon Loss : 1008.3567504882812  Regressor Loss : 104412.9375\n",
            "Val Loss : 103567.421875\n",
            "Batch 10 / 1132 Sammon Loss : 3465.2080078125\n",
            "Batch 20 / 1132 Sammon Loss : 3875.00732421875\n",
            "Batch 30 / 1132 Sammon Loss : 3880.41796875\n",
            "Batch 40 / 1132 Sammon Loss : 4062.380126953125\n",
            "Batch 50 / 1132 Sammon Loss : 4173.107421875\n",
            "Batch 60 / 1132 Sammon Loss : 2118.21630859375\n",
            "Batch 70 / 1132 Sammon Loss : 2155.4365234375\n",
            "Batch 80 / 1132 Sammon Loss : 1857.862060546875\n",
            "Batch 90 / 1132 Sammon Loss : 1846.6219482421875\n",
            "Batch 100 / 1132 Sammon Loss : 1664.40625\n",
            "Batch 110 / 1132 Sammon Loss : 1285.603515625\n",
            "Batch 120 / 1132 Sammon Loss : 690.3916015625\n",
            "Batch 130 / 1132 Sammon Loss : 1318.84521484375\n",
            "Batch 140 / 1132 Sammon Loss : 1476.10546875\n",
            "Batch 150 / 1132 Sammon Loss : 1682.41455078125\n",
            "Batch 160 / 1132 Sammon Loss : 1526.935302734375\n",
            "Batch 170 / 1132 Sammon Loss : 3619.316162109375\n",
            "Batch 180 / 1132 Sammon Loss : 4139.419921875\n",
            "Batch 190 / 1132 Sammon Loss : 3085.2890625\n",
            "Batch 200 / 1132 Sammon Loss : 2844.38671875\n",
            "Batch 210 / 1132 Sammon Loss : 2629.455078125\n",
            "Batch 220 / 1132 Sammon Loss : 2011.5799560546875\n",
            "Batch 230 / 1132 Sammon Loss : 1758.1517333984375\n",
            "Batch 240 / 1132 Sammon Loss : 1171.3154296875\n",
            "Batch 250 / 1132 Sammon Loss : 1107.6014404296875\n",
            "Batch 260 / 1132 Sammon Loss : 1292.130615234375\n",
            "Batch 270 / 1132 Sammon Loss : 1404.549072265625\n",
            "Batch 280 / 1132 Sammon Loss : 1095.713134765625\n",
            "Batch 290 / 1132 Sammon Loss : 1176.41064453125\n",
            "Batch 300 / 1132 Sammon Loss : 2527.228515625\n",
            "Batch 310 / 1132 Sammon Loss : 1180.238525390625\n",
            "Batch 320 / 1132 Sammon Loss : 1008.5332641601562\n",
            "Batch 330 / 1132 Sammon Loss : 1381.880859375\n",
            "Batch 340 / 1132 Sammon Loss : 1181.88232421875\n",
            "Batch 350 / 1132 Sammon Loss : 2044.460205078125\n",
            "Batch 360 / 1132 Sammon Loss : 2180.154541015625\n",
            "Batch 370 / 1132 Sammon Loss : 2229.21826171875\n",
            "Batch 380 / 1132 Sammon Loss : 1788.384521484375\n",
            "Batch 390 / 1132 Sammon Loss : 2175.459228515625\n",
            "Batch 400 / 1132 Sammon Loss : 2594.091552734375\n",
            "Batch 410 / 1132 Sammon Loss : 1902.833251953125\n",
            "Batch 420 / 1132 Sammon Loss : 1884.05078125\n",
            "Batch 430 / 1132 Sammon Loss : 1984.0513916015625\n",
            "Batch 440 / 1132 Sammon Loss : 2849.919921875\n",
            "Batch 450 / 1132 Sammon Loss : 1813.060791015625\n",
            "Batch 460 / 1132 Sammon Loss : 2257.9052734375\n",
            "Batch 470 / 1132 Sammon Loss : 1820.259765625\n",
            "Batch 480 / 1132 Sammon Loss : 2401.0400390625\n",
            "Batch 490 / 1132 Sammon Loss : 1376.10595703125\n",
            "Batch 500 / 1132 Sammon Loss : 5270.97705078125\n",
            "Batch 510 / 1132 Sammon Loss : 5153.44140625\n",
            "Batch 520 / 1132 Sammon Loss : 3581.03173828125\n",
            "Batch 530 / 1132 Sammon Loss : 4599.8291015625\n",
            "Batch 540 / 1132 Sammon Loss : 1154.643798828125\n",
            "Batch 550 / 1132 Sammon Loss : 1294.0697021484375\n",
            "Batch 560 / 1132 Sammon Loss : 1392.5859375\n",
            "Batch 570 / 1132 Sammon Loss : 1413.7772216796875\n",
            "Batch 580 / 1132 Sammon Loss : 1626.235107421875\n",
            "Batch 590 / 1132 Sammon Loss : 1766.638427734375\n",
            "Batch 600 / 1132 Sammon Loss : 1278.298095703125\n",
            "Batch 610 / 1132 Sammon Loss : 1128.691162109375\n",
            "Batch 620 / 1132 Sammon Loss : 2997.024658203125\n",
            "Batch 630 / 1132 Sammon Loss : 3124.126953125\n",
            "Batch 640 / 1132 Sammon Loss : 2807.3232421875\n",
            "Batch 650 / 1132 Sammon Loss : 3587.4287109375\n",
            "Batch 660 / 1132 Sammon Loss : 1580.0638427734375\n",
            "Batch 670 / 1132 Sammon Loss : 1239.650146484375\n",
            "Batch 680 / 1132 Sammon Loss : 1736.01806640625\n",
            "Batch 690 / 1132 Sammon Loss : 1166.2066650390625\n",
            "Batch 700 / 1132 Sammon Loss : 1573.596435546875\n",
            "Batch 710 / 1132 Sammon Loss : 1163.2733154296875\n",
            "Batch 720 / 1132 Sammon Loss : 1761.7362060546875\n",
            "Batch 730 / 1132 Sammon Loss : 836.1727905273438\n",
            "Batch 740 / 1132 Sammon Loss : 1501.148681640625\n",
            "Batch 750 / 1132 Sammon Loss : 1158.72802734375\n",
            "Batch 760 / 1132 Sammon Loss : 1511.257568359375\n",
            "Batch 770 / 1132 Sammon Loss : 2329.85546875\n",
            "Batch 780 / 1132 Sammon Loss : 2027.8824462890625\n",
            "Batch 790 / 1132 Sammon Loss : 1307.384765625\n",
            "Batch 800 / 1132 Sammon Loss : 2362.097900390625\n",
            "Batch 810 / 1132 Sammon Loss : 1071.5079345703125\n",
            "Batch 820 / 1132 Sammon Loss : 1262.291015625\n",
            "Batch 830 / 1132 Sammon Loss : 2174.09814453125\n",
            "Batch 840 / 1132 Sammon Loss : 1793.8199462890625\n",
            "Batch 850 / 1132 Sammon Loss : 3580.31689453125\n",
            "Batch 860 / 1132 Sammon Loss : 3877.98583984375\n",
            "Batch 870 / 1132 Sammon Loss : 4286.27490234375\n",
            "Batch 880 / 1132 Sammon Loss : 1839.6187744140625\n",
            "Batch 890 / 1132 Sammon Loss : 1589.640869140625\n",
            "Batch 900 / 1132 Sammon Loss : 1518.288818359375\n",
            "Batch 910 / 1132 Sammon Loss : 1833.033203125\n",
            "Batch 920 / 1132 Sammon Loss : 1305.8140869140625\n",
            "Batch 930 / 1132 Sammon Loss : 2778.265625\n",
            "Batch 940 / 1132 Sammon Loss : 3688.94873046875\n",
            "Batch 950 / 1132 Sammon Loss : 3059.138427734375\n",
            "Batch 960 / 1132 Sammon Loss : 2228.1240234375\n",
            "Batch 970 / 1132 Sammon Loss : 1535.28125\n",
            "Batch 980 / 1132 Sammon Loss : 1260.511962890625\n",
            "Batch 990 / 1132 Sammon Loss : 1733.306884765625\n",
            "Batch 1000 / 1132 Sammon Loss : 1351.599853515625\n",
            "Batch 1010 / 1132 Sammon Loss : 1027.0474853515625\n",
            "Batch 1020 / 1132 Sammon Loss : 2056.583984375\n",
            "Batch 1030 / 1132 Sammon Loss : 1901.7318115234375\n",
            "Batch 1040 / 1132 Sammon Loss : 2237.90966796875\n",
            "Batch 1050 / 1132 Sammon Loss : 1558.663818359375\n",
            "Batch 1060 / 1132 Sammon Loss : 1434.510986328125\n",
            "Batch 1070 / 1132 Sammon Loss : 3624.1484375\n",
            "Batch 1080 / 1132 Sammon Loss : 1985.844482421875\n",
            "Batch 1090 / 1132 Sammon Loss : 1300.08056640625\n",
            "Batch 1100 / 1132 Sammon Loss : 4317.39892578125\n",
            "Batch 1110 / 1132 Sammon Loss : 1402.5511474609375\n",
            "Batch 1120 / 1132 Sammon Loss : 1744.0018310546875\n",
            "Batch 1130 / 1132 Sammon Loss : 1573.51611328125\n",
            "Batch 10 / 505 Regressor Loss : 107739.625\n",
            "Batch 20 / 505 Regressor Loss : 118906.109375\n",
            "Batch 30 / 505 Regressor Loss : 101934.828125\n",
            "Batch 40 / 505 Regressor Loss : 102904.234375\n",
            "Batch 50 / 505 Regressor Loss : 134040.65625\n",
            "Batch 60 / 505 Regressor Loss : 128361.171875\n",
            "Batch 70 / 505 Regressor Loss : 127892.734375\n",
            "Batch 80 / 505 Regressor Loss : 130076.7109375\n",
            "Batch 90 / 505 Regressor Loss : 133293.96875\n",
            "Batch 100 / 505 Regressor Loss : 107953.0\n",
            "Batch 110 / 505 Regressor Loss : 126023.1484375\n",
            "Batch 120 / 505 Regressor Loss : 115159.03125\n",
            "Batch 130 / 505 Regressor Loss : 129693.125\n",
            "Batch 140 / 505 Regressor Loss : 135832.09375\n",
            "Batch 150 / 505 Regressor Loss : 120329.5859375\n",
            "Batch 160 / 505 Regressor Loss : 96112.0234375\n",
            "Batch 170 / 505 Regressor Loss : 104967.921875\n",
            "Batch 180 / 505 Regressor Loss : 134025.40625\n",
            "Batch 190 / 505 Regressor Loss : 128698.609375\n",
            "Batch 200 / 505 Regressor Loss : 105522.90625\n",
            "Batch 210 / 505 Regressor Loss : 113294.46875\n",
            "Batch 220 / 505 Regressor Loss : 125991.984375\n",
            "Batch 230 / 505 Regressor Loss : 118718.40625\n",
            "Batch 240 / 505 Regressor Loss : 110098.6796875\n",
            "Batch 250 / 505 Regressor Loss : 151789.90625\n",
            "Batch 260 / 505 Regressor Loss : 128708.40625\n",
            "Batch 270 / 505 Regressor Loss : 109112.125\n",
            "Batch 280 / 505 Regressor Loss : 127012.171875\n",
            "Batch 290 / 505 Regressor Loss : 125700.625\n",
            "Batch 300 / 505 Regressor Loss : 117659.0625\n",
            "Batch 310 / 505 Regressor Loss : 121862.2421875\n",
            "Batch 320 / 505 Regressor Loss : 148348.375\n",
            "Batch 330 / 505 Regressor Loss : 114811.4296875\n",
            "Batch 340 / 505 Regressor Loss : 112027.71875\n",
            "Batch 350 / 505 Regressor Loss : 95029.671875\n",
            "Batch 360 / 505 Regressor Loss : 102176.25\n",
            "Batch 370 / 505 Regressor Loss : 122135.984375\n",
            "Batch 380 / 505 Regressor Loss : 105632.671875\n",
            "Batch 390 / 505 Regressor Loss : 117644.3046875\n",
            "Batch 400 / 505 Regressor Loss : 141148.21875\n",
            "Batch 410 / 505 Regressor Loss : 106596.2734375\n",
            "Batch 420 / 505 Regressor Loss : 127633.8671875\n",
            "Batch 430 / 505 Regressor Loss : 112047.5859375\n",
            "Batch 440 / 505 Regressor Loss : 113549.4609375\n",
            "Batch 450 / 505 Regressor Loss : 101267.0859375\n",
            "Batch 460 / 505 Regressor Loss : 129367.828125\n",
            "Batch 470 / 505 Regressor Loss : 121735.015625\n",
            "Batch 480 / 505 Regressor Loss : 122662.7421875\n",
            "Batch 490 / 505 Regressor Loss : 122828.6171875\n",
            "Batch 500 / 505 Regressor Loss : 120427.7734375\n",
            "Epoch  67 / 100  Sammon Loss : 997.4713134765625  Regressor Loss : 104208.765625\n",
            "Val Loss : 103310.265625\n",
            "Batch 10 / 1132 Sammon Loss : 3395.90966796875\n",
            "Batch 20 / 1132 Sammon Loss : 3772.62841796875\n",
            "Batch 30 / 1132 Sammon Loss : 3795.936279296875\n",
            "Batch 40 / 1132 Sammon Loss : 3944.746826171875\n",
            "Batch 50 / 1132 Sammon Loss : 4109.50390625\n",
            "Batch 60 / 1132 Sammon Loss : 2065.25390625\n",
            "Batch 70 / 1132 Sammon Loss : 2075.22900390625\n",
            "Batch 80 / 1132 Sammon Loss : 1797.787841796875\n",
            "Batch 90 / 1132 Sammon Loss : 1803.435546875\n",
            "Batch 100 / 1132 Sammon Loss : 1628.9898681640625\n",
            "Batch 110 / 1132 Sammon Loss : 1252.083740234375\n",
            "Batch 120 / 1132 Sammon Loss : 660.243408203125\n",
            "Batch 130 / 1132 Sammon Loss : 1280.632080078125\n",
            "Batch 140 / 1132 Sammon Loss : 1424.482177734375\n",
            "Batch 150 / 1132 Sammon Loss : 1625.0216064453125\n",
            "Batch 160 / 1132 Sammon Loss : 1509.7401123046875\n",
            "Batch 170 / 1132 Sammon Loss : 3508.15380859375\n",
            "Batch 180 / 1132 Sammon Loss : 3992.33544921875\n",
            "Batch 190 / 1132 Sammon Loss : 3000.2138671875\n",
            "Batch 200 / 1132 Sammon Loss : 2754.646484375\n",
            "Batch 210 / 1132 Sammon Loss : 2598.023193359375\n",
            "Batch 220 / 1132 Sammon Loss : 1965.922607421875\n",
            "Batch 230 / 1132 Sammon Loss : 1703.73095703125\n",
            "Batch 240 / 1132 Sammon Loss : 1129.661865234375\n",
            "Batch 250 / 1132 Sammon Loss : 1038.88525390625\n",
            "Batch 260 / 1132 Sammon Loss : 1255.9588623046875\n",
            "Batch 270 / 1132 Sammon Loss : 1395.008056640625\n",
            "Batch 280 / 1132 Sammon Loss : 1071.45068359375\n",
            "Batch 290 / 1132 Sammon Loss : 1161.566650390625\n",
            "Batch 300 / 1132 Sammon Loss : 2491.361572265625\n",
            "Batch 310 / 1132 Sammon Loss : 1178.477783203125\n",
            "Batch 320 / 1132 Sammon Loss : 1015.6847534179688\n",
            "Batch 330 / 1132 Sammon Loss : 1401.1029052734375\n",
            "Batch 340 / 1132 Sammon Loss : 1195.69384765625\n",
            "Batch 350 / 1132 Sammon Loss : 2041.01953125\n",
            "Batch 360 / 1132 Sammon Loss : 2224.265625\n",
            "Batch 370 / 1132 Sammon Loss : 2253.0419921875\n",
            "Batch 380 / 1132 Sammon Loss : 1798.71533203125\n",
            "Batch 390 / 1132 Sammon Loss : 2201.4130859375\n",
            "Batch 400 / 1132 Sammon Loss : 2654.218017578125\n",
            "Batch 410 / 1132 Sammon Loss : 1935.1617431640625\n",
            "Batch 420 / 1132 Sammon Loss : 1918.2059326171875\n",
            "Batch 430 / 1132 Sammon Loss : 2032.31640625\n",
            "Batch 440 / 1132 Sammon Loss : 2943.1318359375\n",
            "Batch 450 / 1132 Sammon Loss : 1837.047607421875\n",
            "Batch 460 / 1132 Sammon Loss : 2272.136474609375\n",
            "Batch 470 / 1132 Sammon Loss : 1818.4254150390625\n",
            "Batch 480 / 1132 Sammon Loss : 2437.220947265625\n",
            "Batch 490 / 1132 Sammon Loss : 1365.994140625\n",
            "Batch 500 / 1132 Sammon Loss : 5304.21142578125\n",
            "Batch 510 / 1132 Sammon Loss : 5158.2265625\n",
            "Batch 520 / 1132 Sammon Loss : 3603.984375\n",
            "Batch 530 / 1132 Sammon Loss : 4617.50341796875\n",
            "Batch 540 / 1132 Sammon Loss : 1187.81591796875\n",
            "Batch 550 / 1132 Sammon Loss : 1316.2052001953125\n",
            "Batch 560 / 1132 Sammon Loss : 1414.9471435546875\n",
            "Batch 570 / 1132 Sammon Loss : 1384.705078125\n",
            "Batch 580 / 1132 Sammon Loss : 1613.95947265625\n",
            "Batch 590 / 1132 Sammon Loss : 1722.9169921875\n",
            "Batch 600 / 1132 Sammon Loss : 1235.4713134765625\n",
            "Batch 610 / 1132 Sammon Loss : 1088.218505859375\n",
            "Batch 620 / 1132 Sammon Loss : 3015.345703125\n",
            "Batch 630 / 1132 Sammon Loss : 3165.22607421875\n",
            "Batch 640 / 1132 Sammon Loss : 2811.70361328125\n",
            "Batch 650 / 1132 Sammon Loss : 3639.8681640625\n",
            "Batch 660 / 1132 Sammon Loss : 1575.4248046875\n",
            "Batch 670 / 1132 Sammon Loss : 1127.0909423828125\n",
            "Batch 680 / 1132 Sammon Loss : 1682.6519775390625\n",
            "Batch 690 / 1132 Sammon Loss : 1134.4229736328125\n",
            "Batch 700 / 1132 Sammon Loss : 1574.5762939453125\n",
            "Batch 710 / 1132 Sammon Loss : 1096.1556396484375\n",
            "Batch 720 / 1132 Sammon Loss : 1737.344970703125\n",
            "Batch 730 / 1132 Sammon Loss : 727.9732666015625\n",
            "Batch 740 / 1132 Sammon Loss : 1416.8226318359375\n",
            "Batch 750 / 1132 Sammon Loss : 1070.996337890625\n",
            "Batch 760 / 1132 Sammon Loss : 1552.1748046875\n",
            "Batch 770 / 1132 Sammon Loss : 2319.365478515625\n",
            "Batch 780 / 1132 Sammon Loss : 2009.2984619140625\n",
            "Batch 790 / 1132 Sammon Loss : 1306.506103515625\n",
            "Batch 800 / 1132 Sammon Loss : 2343.05419921875\n",
            "Batch 810 / 1132 Sammon Loss : 1065.1163330078125\n",
            "Batch 820 / 1132 Sammon Loss : 1229.84912109375\n",
            "Batch 830 / 1132 Sammon Loss : 2153.296875\n",
            "Batch 840 / 1132 Sammon Loss : 1769.775146484375\n",
            "Batch 850 / 1132 Sammon Loss : 3602.48779296875\n",
            "Batch 860 / 1132 Sammon Loss : 3906.738525390625\n",
            "Batch 870 / 1132 Sammon Loss : 4305.6943359375\n",
            "Batch 880 / 1132 Sammon Loss : 1848.891845703125\n",
            "Batch 890 / 1132 Sammon Loss : 1591.596923828125\n",
            "Batch 900 / 1132 Sammon Loss : 1508.33056640625\n",
            "Batch 910 / 1132 Sammon Loss : 1784.10302734375\n",
            "Batch 920 / 1132 Sammon Loss : 1280.707763671875\n",
            "Batch 930 / 1132 Sammon Loss : 2837.2509765625\n",
            "Batch 940 / 1132 Sammon Loss : 3711.000244140625\n",
            "Batch 950 / 1132 Sammon Loss : 3116.246337890625\n",
            "Batch 960 / 1132 Sammon Loss : 2256.158203125\n",
            "Batch 970 / 1132 Sammon Loss : 1521.3345947265625\n",
            "Batch 980 / 1132 Sammon Loss : 1228.1488037109375\n",
            "Batch 990 / 1132 Sammon Loss : 1690.1407470703125\n",
            "Batch 1000 / 1132 Sammon Loss : 1296.0980224609375\n",
            "Batch 1010 / 1132 Sammon Loss : 994.4249267578125\n",
            "Batch 1020 / 1132 Sammon Loss : 2034.7628173828125\n",
            "Batch 1030 / 1132 Sammon Loss : 1916.136474609375\n",
            "Batch 1040 / 1132 Sammon Loss : 2222.5380859375\n",
            "Batch 1050 / 1132 Sammon Loss : 1518.8194580078125\n",
            "Batch 1060 / 1132 Sammon Loss : 1410.563232421875\n",
            "Batch 1070 / 1132 Sammon Loss : 3629.45458984375\n",
            "Batch 1080 / 1132 Sammon Loss : 2006.1390380859375\n",
            "Batch 1090 / 1132 Sammon Loss : 1314.85009765625\n",
            "Batch 1100 / 1132 Sammon Loss : 4371.3203125\n",
            "Batch 1110 / 1132 Sammon Loss : 1405.9140625\n",
            "Batch 1120 / 1132 Sammon Loss : 1731.1669921875\n",
            "Batch 1130 / 1132 Sammon Loss : 1566.312255859375\n",
            "Batch 10 / 505 Regressor Loss : 107523.0859375\n",
            "Batch 20 / 505 Regressor Loss : 118685.421875\n",
            "Batch 30 / 505 Regressor Loss : 101713.0859375\n",
            "Batch 40 / 505 Regressor Loss : 102679.765625\n",
            "Batch 50 / 505 Regressor Loss : 133769.71875\n",
            "Batch 60 / 505 Regressor Loss : 128114.0\n",
            "Batch 70 / 505 Regressor Loss : 127654.4296875\n",
            "Batch 80 / 505 Regressor Loss : 129818.5859375\n",
            "Batch 90 / 505 Regressor Loss : 133058.09375\n",
            "Batch 100 / 505 Regressor Loss : 107721.0\n",
            "Batch 110 / 505 Regressor Loss : 125792.2109375\n",
            "Batch 120 / 505 Regressor Loss : 114945.984375\n",
            "Batch 130 / 505 Regressor Loss : 129462.015625\n",
            "Batch 140 / 505 Regressor Loss : 135585.859375\n",
            "Batch 150 / 505 Regressor Loss : 120100.2734375\n",
            "Batch 160 / 505 Regressor Loss : 95900.734375\n",
            "Batch 170 / 505 Regressor Loss : 104748.390625\n",
            "Batch 180 / 505 Regressor Loss : 133779.109375\n",
            "Batch 190 / 505 Regressor Loss : 128442.25\n",
            "Batch 200 / 505 Regressor Loss : 105318.7734375\n",
            "Batch 210 / 505 Regressor Loss : 113065.703125\n",
            "Batch 220 / 505 Regressor Loss : 125754.5625\n",
            "Batch 230 / 505 Regressor Loss : 118493.4375\n",
            "Batch 240 / 505 Regressor Loss : 109887.5625\n",
            "Batch 250 / 505 Regressor Loss : 151526.28125\n",
            "Batch 260 / 505 Regressor Loss : 128459.75\n",
            "Batch 270 / 505 Regressor Loss : 108897.265625\n",
            "Batch 280 / 505 Regressor Loss : 126771.7421875\n",
            "Batch 290 / 505 Regressor Loss : 125476.3046875\n",
            "Batch 300 / 505 Regressor Loss : 117422.859375\n",
            "Batch 310 / 505 Regressor Loss : 121629.9609375\n",
            "Batch 320 / 505 Regressor Loss : 148082.0\n",
            "Batch 330 / 505 Regressor Loss : 114576.28125\n",
            "Batch 340 / 505 Regressor Loss : 111784.296875\n",
            "Batch 350 / 505 Regressor Loss : 94816.5\n",
            "Batch 360 / 505 Regressor Loss : 101976.6484375\n",
            "Batch 370 / 505 Regressor Loss : 121895.53125\n",
            "Batch 380 / 505 Regressor Loss : 105417.78125\n",
            "Batch 390 / 505 Regressor Loss : 117416.265625\n",
            "Batch 400 / 505 Regressor Loss : 140874.34375\n",
            "Batch 410 / 505 Regressor Loss : 106379.8359375\n",
            "Batch 420 / 505 Regressor Loss : 127398.640625\n",
            "Batch 430 / 505 Regressor Loss : 111823.2734375\n",
            "Batch 440 / 505 Regressor Loss : 113318.65625\n",
            "Batch 450 / 505 Regressor Loss : 101062.5625\n",
            "Batch 460 / 505 Regressor Loss : 129129.8984375\n",
            "Batch 470 / 505 Regressor Loss : 121518.3125\n",
            "Batch 480 / 505 Regressor Loss : 122433.25\n",
            "Batch 490 / 505 Regressor Loss : 122592.453125\n",
            "Batch 500 / 505 Regressor Loss : 120208.0625\n",
            "Epoch  68 / 100  Sammon Loss : 965.23291015625  Regressor Loss : 104004.9609375\n",
            "Val Loss : 103053.515625\n",
            "Batch 10 / 1132 Sammon Loss : 3348.296875\n",
            "Batch 20 / 1132 Sammon Loss : 3760.8173828125\n",
            "Batch 30 / 1132 Sammon Loss : 3763.86376953125\n",
            "Batch 40 / 1132 Sammon Loss : 3946.36669921875\n",
            "Batch 50 / 1132 Sammon Loss : 4086.878662109375\n",
            "Batch 60 / 1132 Sammon Loss : 2069.575927734375\n",
            "Batch 70 / 1132 Sammon Loss : 2075.06298828125\n",
            "Batch 80 / 1132 Sammon Loss : 1786.911865234375\n",
            "Batch 90 / 1132 Sammon Loss : 1793.14208984375\n",
            "Batch 100 / 1132 Sammon Loss : 1621.07568359375\n",
            "Batch 110 / 1132 Sammon Loss : 1237.7255859375\n",
            "Batch 120 / 1132 Sammon Loss : 649.8825073242188\n",
            "Batch 130 / 1132 Sammon Loss : 1318.2869873046875\n",
            "Batch 140 / 1132 Sammon Loss : 1497.864013671875\n",
            "Batch 150 / 1132 Sammon Loss : 1669.4710693359375\n",
            "Batch 160 / 1132 Sammon Loss : 1521.2193603515625\n",
            "Batch 170 / 1132 Sammon Loss : 3464.13037109375\n",
            "Batch 180 / 1132 Sammon Loss : 3920.62939453125\n",
            "Batch 190 / 1132 Sammon Loss : 2941.2763671875\n",
            "Batch 200 / 1132 Sammon Loss : 2738.00927734375\n",
            "Batch 210 / 1132 Sammon Loss : 2559.276123046875\n",
            "Batch 220 / 1132 Sammon Loss : 1945.937255859375\n",
            "Batch 230 / 1132 Sammon Loss : 1709.673583984375\n",
            "Batch 240 / 1132 Sammon Loss : 1141.8974609375\n",
            "Batch 250 / 1132 Sammon Loss : 1047.3250732421875\n",
            "Batch 260 / 1132 Sammon Loss : 1262.8642578125\n",
            "Batch 270 / 1132 Sammon Loss : 1392.480224609375\n",
            "Batch 280 / 1132 Sammon Loss : 1086.43701171875\n",
            "Batch 290 / 1132 Sammon Loss : 1168.3570556640625\n",
            "Batch 300 / 1132 Sammon Loss : 2494.971923828125\n",
            "Batch 310 / 1132 Sammon Loss : 1187.2203369140625\n",
            "Batch 320 / 1132 Sammon Loss : 1005.1461791992188\n",
            "Batch 330 / 1132 Sammon Loss : 1425.9248046875\n",
            "Batch 340 / 1132 Sammon Loss : 1220.484130859375\n",
            "Batch 350 / 1132 Sammon Loss : 2035.0040283203125\n",
            "Batch 360 / 1132 Sammon Loss : 2209.76123046875\n",
            "Batch 370 / 1132 Sammon Loss : 2275.0224609375\n",
            "Batch 380 / 1132 Sammon Loss : 1773.9736328125\n",
            "Batch 390 / 1132 Sammon Loss : 2181.77294921875\n",
            "Batch 400 / 1132 Sammon Loss : 2663.94091796875\n",
            "Batch 410 / 1132 Sammon Loss : 1927.9202880859375\n",
            "Batch 420 / 1132 Sammon Loss : 1939.220458984375\n",
            "Batch 430 / 1132 Sammon Loss : 2059.972412109375\n",
            "Batch 440 / 1132 Sammon Loss : 2959.0234375\n",
            "Batch 450 / 1132 Sammon Loss : 1847.8460693359375\n",
            "Batch 460 / 1132 Sammon Loss : 2288.298828125\n",
            "Batch 470 / 1132 Sammon Loss : 1819.4610595703125\n",
            "Batch 480 / 1132 Sammon Loss : 2422.583984375\n",
            "Batch 490 / 1132 Sammon Loss : 1352.72314453125\n",
            "Batch 500 / 1132 Sammon Loss : 5371.716796875\n",
            "Batch 510 / 1132 Sammon Loss : 5227.7578125\n",
            "Batch 520 / 1132 Sammon Loss : 3646.491943359375\n",
            "Batch 530 / 1132 Sammon Loss : 4674.728515625\n",
            "Batch 540 / 1132 Sammon Loss : 1124.620849609375\n",
            "Batch 550 / 1132 Sammon Loss : 1283.69580078125\n",
            "Batch 560 / 1132 Sammon Loss : 1407.91552734375\n",
            "Batch 570 / 1132 Sammon Loss : 1432.031494140625\n",
            "Batch 580 / 1132 Sammon Loss : 1596.7537841796875\n",
            "Batch 590 / 1132 Sammon Loss : 1733.8377685546875\n",
            "Batch 600 / 1132 Sammon Loss : 1302.644287109375\n",
            "Batch 610 / 1132 Sammon Loss : 1088.01708984375\n",
            "Batch 620 / 1132 Sammon Loss : 3139.093994140625\n",
            "Batch 630 / 1132 Sammon Loss : 3198.0615234375\n",
            "Batch 640 / 1132 Sammon Loss : 2865.919921875\n",
            "Batch 650 / 1132 Sammon Loss : 3682.8974609375\n",
            "Batch 660 / 1132 Sammon Loss : 1590.7608642578125\n",
            "Batch 670 / 1132 Sammon Loss : 1248.2451171875\n",
            "Batch 680 / 1132 Sammon Loss : 1727.056884765625\n",
            "Batch 690 / 1132 Sammon Loss : 1137.1453857421875\n",
            "Batch 700 / 1132 Sammon Loss : 1552.956298828125\n",
            "Batch 710 / 1132 Sammon Loss : 1102.637451171875\n",
            "Batch 720 / 1132 Sammon Loss : 1745.8682861328125\n",
            "Batch 730 / 1132 Sammon Loss : 752.8839111328125\n",
            "Batch 740 / 1132 Sammon Loss : 1425.6964111328125\n",
            "Batch 750 / 1132 Sammon Loss : 1081.0521240234375\n",
            "Batch 760 / 1132 Sammon Loss : 1567.817626953125\n",
            "Batch 770 / 1132 Sammon Loss : 2344.38916015625\n",
            "Batch 780 / 1132 Sammon Loss : 2067.78857421875\n",
            "Batch 790 / 1132 Sammon Loss : 1325.258056640625\n",
            "Batch 800 / 1132 Sammon Loss : 2360.073486328125\n",
            "Batch 810 / 1132 Sammon Loss : 1099.6898193359375\n",
            "Batch 820 / 1132 Sammon Loss : 1239.238525390625\n",
            "Batch 830 / 1132 Sammon Loss : 2219.8447265625\n",
            "Batch 840 / 1132 Sammon Loss : 1809.5101318359375\n",
            "Batch 850 / 1132 Sammon Loss : 3635.624267578125\n",
            "Batch 860 / 1132 Sammon Loss : 3936.55029296875\n",
            "Batch 870 / 1132 Sammon Loss : 4344.1240234375\n",
            "Batch 880 / 1132 Sammon Loss : 1889.9375\n",
            "Batch 890 / 1132 Sammon Loss : 1608.93408203125\n",
            "Batch 900 / 1132 Sammon Loss : 1528.224365234375\n",
            "Batch 910 / 1132 Sammon Loss : 1817.693359375\n",
            "Batch 920 / 1132 Sammon Loss : 1291.8419189453125\n",
            "Batch 930 / 1132 Sammon Loss : 2847.7841796875\n",
            "Batch 940 / 1132 Sammon Loss : 3737.498291015625\n",
            "Batch 950 / 1132 Sammon Loss : 3116.431640625\n",
            "Batch 960 / 1132 Sammon Loss : 2262.68896484375\n",
            "Batch 970 / 1132 Sammon Loss : 1534.613037109375\n",
            "Batch 980 / 1132 Sammon Loss : 1252.33837890625\n",
            "Batch 990 / 1132 Sammon Loss : 1737.01171875\n",
            "Batch 1000 / 1132 Sammon Loss : 1346.24267578125\n",
            "Batch 1010 / 1132 Sammon Loss : 1009.60791015625\n",
            "Batch 1020 / 1132 Sammon Loss : 2081.42333984375\n",
            "Batch 1030 / 1132 Sammon Loss : 1923.0455322265625\n",
            "Batch 1040 / 1132 Sammon Loss : 2239.8291015625\n",
            "Batch 1050 / 1132 Sammon Loss : 1575.963623046875\n",
            "Batch 1060 / 1132 Sammon Loss : 1485.7412109375\n",
            "Batch 1070 / 1132 Sammon Loss : 3641.3212890625\n",
            "Batch 1080 / 1132 Sammon Loss : 2074.869384765625\n",
            "Batch 1090 / 1132 Sammon Loss : 1399.1842041015625\n",
            "Batch 1100 / 1132 Sammon Loss : 4405.36767578125\n",
            "Batch 1110 / 1132 Sammon Loss : 1469.17578125\n",
            "Batch 1120 / 1132 Sammon Loss : 1802.8248291015625\n",
            "Batch 1130 / 1132 Sammon Loss : 1603.2197265625\n",
            "Batch 10 / 505 Regressor Loss : 107306.9296875\n",
            "Batch 20 / 505 Regressor Loss : 118465.125\n",
            "Batch 30 / 505 Regressor Loss : 101491.7421875\n",
            "Batch 40 / 505 Regressor Loss : 102455.6796875\n",
            "Batch 50 / 505 Regressor Loss : 133499.1875\n",
            "Batch 60 / 505 Regressor Loss : 127867.234375\n",
            "Batch 70 / 505 Regressor Loss : 127416.546875\n",
            "Batch 80 / 505 Regressor Loss : 129560.890625\n",
            "Batch 90 / 505 Regressor Loss : 132822.578125\n",
            "Batch 100 / 505 Regressor Loss : 107489.3984375\n",
            "Batch 110 / 505 Regressor Loss : 125561.65625\n",
            "Batch 120 / 505 Regressor Loss : 114733.3125\n",
            "Batch 130 / 505 Regressor Loss : 129231.28125\n",
            "Batch 140 / 505 Regressor Loss : 135340.046875\n",
            "Batch 150 / 505 Regressor Loss : 119871.34375\n",
            "Batch 160 / 505 Regressor Loss : 95689.8359375\n",
            "Batch 170 / 505 Regressor Loss : 104529.265625\n",
            "Batch 180 / 505 Regressor Loss : 133533.21875\n",
            "Batch 190 / 505 Regressor Loss : 128186.3125\n",
            "Batch 200 / 505 Regressor Loss : 105115.0234375\n",
            "Batch 210 / 505 Regressor Loss : 112837.328125\n",
            "Batch 220 / 505 Regressor Loss : 125517.546875\n",
            "Batch 230 / 505 Regressor Loss : 118268.875\n",
            "Batch 240 / 505 Regressor Loss : 109676.8359375\n",
            "Batch 250 / 505 Regressor Loss : 151263.046875\n",
            "Batch 260 / 505 Regressor Loss : 128211.53125\n",
            "Batch 270 / 505 Regressor Loss : 108682.8046875\n",
            "Batch 280 / 505 Regressor Loss : 126531.7734375\n",
            "Batch 290 / 505 Regressor Loss : 125252.375\n",
            "Batch 300 / 505 Regressor Loss : 117187.046875\n",
            "Batch 310 / 505 Regressor Loss : 121398.09375\n",
            "Batch 320 / 505 Regressor Loss : 147816.09375\n",
            "Batch 330 / 505 Regressor Loss : 114341.53125\n",
            "Batch 340 / 505 Regressor Loss : 111541.296875\n",
            "Batch 350 / 505 Regressor Loss : 94603.734375\n",
            "Batch 360 / 505 Regressor Loss : 101777.4296875\n",
            "Batch 370 / 505 Regressor Loss : 121655.5\n",
            "Batch 380 / 505 Regressor Loss : 105203.296875\n",
            "Batch 390 / 505 Regressor Loss : 117188.609375\n",
            "Batch 400 / 505 Regressor Loss : 140600.90625\n",
            "Batch 410 / 505 Regressor Loss : 106163.78125\n",
            "Batch 420 / 505 Regressor Loss : 127163.796875\n",
            "Batch 430 / 505 Regressor Loss : 111599.375\n",
            "Batch 440 / 505 Regressor Loss : 113088.234375\n",
            "Batch 450 / 505 Regressor Loss : 100858.4375\n",
            "Batch 460 / 505 Regressor Loss : 128892.375\n",
            "Batch 470 / 505 Regressor Loss : 121302.0\n",
            "Batch 480 / 505 Regressor Loss : 122204.171875\n",
            "Batch 490 / 505 Regressor Loss : 122356.6796875\n",
            "Batch 500 / 505 Regressor Loss : 119988.78125\n",
            "Epoch  69 / 100  Sammon Loss : 1075.0419921875  Regressor Loss : 103801.5546875\n",
            "Val Loss : 102797.1796875\n",
            "Batch 10 / 1132 Sammon Loss : 3444.47900390625\n",
            "Batch 20 / 1132 Sammon Loss : 3802.346435546875\n",
            "Batch 30 / 1132 Sammon Loss : 3847.271240234375\n",
            "Batch 40 / 1132 Sammon Loss : 3976.80517578125\n",
            "Batch 50 / 1132 Sammon Loss : 4189.2705078125\n",
            "Batch 60 / 1132 Sammon Loss : 2082.260986328125\n",
            "Batch 70 / 1132 Sammon Loss : 2115.3818359375\n",
            "Batch 80 / 1132 Sammon Loss : 1810.643310546875\n",
            "Batch 90 / 1132 Sammon Loss : 1847.0616455078125\n",
            "Batch 100 / 1132 Sammon Loss : 1656.770751953125\n",
            "Batch 110 / 1132 Sammon Loss : 1294.042236328125\n",
            "Batch 120 / 1132 Sammon Loss : 647.2677001953125\n",
            "Batch 130 / 1132 Sammon Loss : 1281.5537109375\n",
            "Batch 140 / 1132 Sammon Loss : 1482.4517822265625\n",
            "Batch 150 / 1132 Sammon Loss : 1644.02685546875\n",
            "Batch 160 / 1132 Sammon Loss : 1500.85546875\n",
            "Batch 170 / 1132 Sammon Loss : 3560.26171875\n",
            "Batch 180 / 1132 Sammon Loss : 4027.28515625\n",
            "Batch 190 / 1132 Sammon Loss : 3012.8212890625\n",
            "Batch 200 / 1132 Sammon Loss : 2788.86962890625\n",
            "Batch 210 / 1132 Sammon Loss : 2627.93212890625\n",
            "Batch 220 / 1132 Sammon Loss : 1963.800537109375\n",
            "Batch 230 / 1132 Sammon Loss : 1719.9703369140625\n",
            "Batch 240 / 1132 Sammon Loss : 1150.91455078125\n",
            "Batch 250 / 1132 Sammon Loss : 1058.4549560546875\n",
            "Batch 260 / 1132 Sammon Loss : 1274.2706298828125\n",
            "Batch 270 / 1132 Sammon Loss : 1420.2718505859375\n",
            "Batch 280 / 1132 Sammon Loss : 1097.58154296875\n",
            "Batch 290 / 1132 Sammon Loss : 1200.90966796875\n",
            "Batch 300 / 1132 Sammon Loss : 2542.1591796875\n",
            "Batch 310 / 1132 Sammon Loss : 1196.07275390625\n",
            "Batch 320 / 1132 Sammon Loss : 980.6173095703125\n",
            "Batch 330 / 1132 Sammon Loss : 1410.574951171875\n",
            "Batch 340 / 1132 Sammon Loss : 1194.640869140625\n",
            "Batch 350 / 1132 Sammon Loss : 2075.62890625\n",
            "Batch 360 / 1132 Sammon Loss : 2285.312744140625\n",
            "Batch 370 / 1132 Sammon Loss : 2294.090576171875\n",
            "Batch 380 / 1132 Sammon Loss : 1829.2431640625\n",
            "Batch 390 / 1132 Sammon Loss : 2248.62744140625\n",
            "Batch 400 / 1132 Sammon Loss : 2726.593505859375\n",
            "Batch 410 / 1132 Sammon Loss : 1960.955078125\n",
            "Batch 420 / 1132 Sammon Loss : 1952.728759765625\n",
            "Batch 430 / 1132 Sammon Loss : 2057.993408203125\n",
            "Batch 440 / 1132 Sammon Loss : 2960.060546875\n",
            "Batch 450 / 1132 Sammon Loss : 1854.300537109375\n",
            "Batch 460 / 1132 Sammon Loss : 2300.38134765625\n",
            "Batch 470 / 1132 Sammon Loss : 1854.0657958984375\n",
            "Batch 480 / 1132 Sammon Loss : 2434.19921875\n",
            "Batch 490 / 1132 Sammon Loss : 1374.2178955078125\n",
            "Batch 500 / 1132 Sammon Loss : 5370.8125\n",
            "Batch 510 / 1132 Sammon Loss : 5250.1650390625\n",
            "Batch 520 / 1132 Sammon Loss : 3643.3974609375\n",
            "Batch 530 / 1132 Sammon Loss : 4677.8486328125\n",
            "Batch 540 / 1132 Sammon Loss : 1178.3282470703125\n",
            "Batch 550 / 1132 Sammon Loss : 1310.728759765625\n",
            "Batch 560 / 1132 Sammon Loss : 1447.324951171875\n",
            "Batch 570 / 1132 Sammon Loss : 1432.21875\n",
            "Batch 580 / 1132 Sammon Loss : 1650.4310302734375\n",
            "Batch 590 / 1132 Sammon Loss : 1779.98583984375\n",
            "Batch 600 / 1132 Sammon Loss : 1271.3211669921875\n",
            "Batch 610 / 1132 Sammon Loss : 1137.8563232421875\n",
            "Batch 620 / 1132 Sammon Loss : 3042.9501953125\n",
            "Batch 630 / 1132 Sammon Loss : 3202.618408203125\n",
            "Batch 640 / 1132 Sammon Loss : 2886.607421875\n",
            "Batch 650 / 1132 Sammon Loss : 3677.226318359375\n",
            "Batch 660 / 1132 Sammon Loss : 1594.659912109375\n",
            "Batch 670 / 1132 Sammon Loss : 1198.2860107421875\n",
            "Batch 680 / 1132 Sammon Loss : 1730.595703125\n",
            "Batch 690 / 1132 Sammon Loss : 1175.9111328125\n",
            "Batch 700 / 1132 Sammon Loss : 1590.338623046875\n",
            "Batch 710 / 1132 Sammon Loss : 1130.507080078125\n",
            "Batch 720 / 1132 Sammon Loss : 1757.02099609375\n",
            "Batch 730 / 1132 Sammon Loss : 778.7435302734375\n",
            "Batch 740 / 1132 Sammon Loss : 1487.12890625\n",
            "Batch 750 / 1132 Sammon Loss : 1140.8837890625\n",
            "Batch 760 / 1132 Sammon Loss : 1590.4034423828125\n",
            "Batch 770 / 1132 Sammon Loss : 2380.14501953125\n",
            "Batch 780 / 1132 Sammon Loss : 2078.138916015625\n",
            "Batch 790 / 1132 Sammon Loss : 1368.2918701171875\n",
            "Batch 800 / 1132 Sammon Loss : 2420.037353515625\n",
            "Batch 810 / 1132 Sammon Loss : 1126.7720947265625\n",
            "Batch 820 / 1132 Sammon Loss : 1256.2381591796875\n",
            "Batch 830 / 1132 Sammon Loss : 2206.707275390625\n",
            "Batch 840 / 1132 Sammon Loss : 1827.2205810546875\n",
            "Batch 850 / 1132 Sammon Loss : 3644.140380859375\n",
            "Batch 860 / 1132 Sammon Loss : 3951.68896484375\n",
            "Batch 870 / 1132 Sammon Loss : 4355.8193359375\n",
            "Batch 880 / 1132 Sammon Loss : 1893.7838134765625\n",
            "Batch 890 / 1132 Sammon Loss : 1633.0595703125\n",
            "Batch 900 / 1132 Sammon Loss : 1550.309326171875\n",
            "Batch 910 / 1132 Sammon Loss : 1828.45166015625\n",
            "Batch 920 / 1132 Sammon Loss : 1320.724365234375\n",
            "Batch 930 / 1132 Sammon Loss : 2869.6796875\n",
            "Batch 940 / 1132 Sammon Loss : 3768.79443359375\n",
            "Batch 950 / 1132 Sammon Loss : 3131.46240234375\n",
            "Batch 960 / 1132 Sammon Loss : 2281.97265625\n",
            "Batch 970 / 1132 Sammon Loss : 1566.1373291015625\n",
            "Batch 980 / 1132 Sammon Loss : 1315.1162109375\n",
            "Batch 990 / 1132 Sammon Loss : 1775.50439453125\n",
            "Batch 1000 / 1132 Sammon Loss : 1380.377685546875\n",
            "Batch 1010 / 1132 Sammon Loss : 1033.42822265625\n",
            "Batch 1020 / 1132 Sammon Loss : 2106.0751953125\n",
            "Batch 1030 / 1132 Sammon Loss : 1928.751708984375\n",
            "Batch 1040 / 1132 Sammon Loss : 2279.29736328125\n",
            "Batch 1050 / 1132 Sammon Loss : 1572.5577392578125\n",
            "Batch 1060 / 1132 Sammon Loss : 1455.961669921875\n",
            "Batch 1070 / 1132 Sammon Loss : 3685.21337890625\n",
            "Batch 1080 / 1132 Sammon Loss : 1992.5322265625\n",
            "Batch 1090 / 1132 Sammon Loss : 1355.29443359375\n",
            "Batch 1100 / 1132 Sammon Loss : 4354.07568359375\n",
            "Batch 1110 / 1132 Sammon Loss : 1459.567138671875\n",
            "Batch 1120 / 1132 Sammon Loss : 1773.0965576171875\n",
            "Batch 1130 / 1132 Sammon Loss : 1590.7904052734375\n",
            "Batch 10 / 505 Regressor Loss : 107091.15625\n",
            "Batch 20 / 505 Regressor Loss : 118245.21875\n",
            "Batch 30 / 505 Regressor Loss : 101270.796875\n",
            "Batch 40 / 505 Regressor Loss : 102232.0\n",
            "Batch 50 / 505 Regressor Loss : 133229.109375\n",
            "Batch 60 / 505 Regressor Loss : 127620.859375\n",
            "Batch 70 / 505 Regressor Loss : 127179.03125\n",
            "Batch 80 / 505 Regressor Loss : 129303.625\n",
            "Batch 90 / 505 Regressor Loss : 132587.46875\n",
            "Batch 100 / 505 Regressor Loss : 107258.1875\n",
            "Batch 110 / 505 Regressor Loss : 125331.46875\n",
            "Batch 120 / 505 Regressor Loss : 114521.0234375\n",
            "Batch 130 / 505 Regressor Loss : 129000.921875\n",
            "Batch 140 / 505 Regressor Loss : 135094.625\n",
            "Batch 150 / 505 Regressor Loss : 119642.828125\n",
            "Batch 160 / 505 Regressor Loss : 95479.3359375\n",
            "Batch 170 / 505 Regressor Loss : 104310.5234375\n",
            "Batch 180 / 505 Regressor Loss : 133287.71875\n",
            "Batch 190 / 505 Regressor Loss : 127930.7734375\n",
            "Batch 200 / 505 Regressor Loss : 104911.640625\n",
            "Batch 210 / 505 Regressor Loss : 112609.359375\n",
            "Batch 220 / 505 Regressor Loss : 125280.9609375\n",
            "Batch 230 / 505 Regressor Loss : 118044.71875\n",
            "Batch 240 / 505 Regressor Loss : 109466.484375\n",
            "Batch 250 / 505 Regressor Loss : 151000.234375\n",
            "Batch 260 / 505 Regressor Loss : 127963.75\n",
            "Batch 270 / 505 Regressor Loss : 108468.75\n",
            "Batch 280 / 505 Regressor Loss : 126292.1875\n",
            "Batch 290 / 505 Regressor Loss : 125028.84375\n",
            "Batch 300 / 505 Regressor Loss : 116951.625\n",
            "Batch 310 / 505 Regressor Loss : 121166.609375\n",
            "Batch 320 / 505 Regressor Loss : 147550.59375\n",
            "Batch 330 / 505 Regressor Loss : 114107.171875\n",
            "Batch 340 / 505 Regressor Loss : 111298.6875\n",
            "Batch 350 / 505 Regressor Loss : 94391.3671875\n",
            "Batch 360 / 505 Regressor Loss : 101578.5859375\n",
            "Batch 370 / 505 Regressor Loss : 121415.8984375\n",
            "Batch 380 / 505 Regressor Loss : 104989.21875\n",
            "Batch 390 / 505 Regressor Loss : 116961.359375\n",
            "Batch 400 / 505 Regressor Loss : 140327.9375\n",
            "Batch 410 / 505 Regressor Loss : 105948.15625\n",
            "Batch 420 / 505 Regressor Loss : 126929.359375\n",
            "Batch 430 / 505 Regressor Loss : 111375.859375\n",
            "Batch 440 / 505 Regressor Loss : 112858.21875\n",
            "Batch 450 / 505 Regressor Loss : 100654.703125\n",
            "Batch 460 / 505 Regressor Loss : 128655.2421875\n",
            "Batch 470 / 505 Regressor Loss : 121086.0546875\n",
            "Batch 480 / 505 Regressor Loss : 121975.484375\n",
            "Batch 490 / 505 Regressor Loss : 122121.3125\n",
            "Batch 500 / 505 Regressor Loss : 119769.8984375\n",
            "Epoch  70 / 100  Sammon Loss : 1002.931396484375  Regressor Loss : 103598.53125\n",
            "Val Loss : 102541.25\n",
            "Batch 10 / 1132 Sammon Loss : 3387.23828125\n",
            "Batch 20 / 1132 Sammon Loss : 3796.130859375\n",
            "Batch 30 / 1132 Sammon Loss : 3838.366455078125\n",
            "Batch 40 / 1132 Sammon Loss : 4025.004638671875\n",
            "Batch 50 / 1132 Sammon Loss : 4198.052734375\n",
            "Batch 60 / 1132 Sammon Loss : 2106.142822265625\n",
            "Batch 70 / 1132 Sammon Loss : 2163.691162109375\n",
            "Batch 80 / 1132 Sammon Loss : 1859.1370849609375\n",
            "Batch 90 / 1132 Sammon Loss : 1862.679931640625\n",
            "Batch 100 / 1132 Sammon Loss : 1686.595947265625\n",
            "Batch 110 / 1132 Sammon Loss : 1299.40625\n",
            "Batch 120 / 1132 Sammon Loss : 684.389404296875\n",
            "Batch 130 / 1132 Sammon Loss : 1305.1778564453125\n",
            "Batch 140 / 1132 Sammon Loss : 1472.8212890625\n",
            "Batch 150 / 1132 Sammon Loss : 1665.3282470703125\n",
            "Batch 160 / 1132 Sammon Loss : 1533.342041015625\n",
            "Batch 170 / 1132 Sammon Loss : 3559.971435546875\n",
            "Batch 180 / 1132 Sammon Loss : 4037.93896484375\n",
            "Batch 190 / 1132 Sammon Loss : 3023.27783203125\n",
            "Batch 200 / 1132 Sammon Loss : 2828.9609375\n",
            "Batch 210 / 1132 Sammon Loss : 2635.71142578125\n",
            "Batch 220 / 1132 Sammon Loss : 2012.107421875\n",
            "Batch 230 / 1132 Sammon Loss : 1767.9197998046875\n",
            "Batch 240 / 1132 Sammon Loss : 1177.306396484375\n",
            "Batch 250 / 1132 Sammon Loss : 1130.197998046875\n",
            "Batch 260 / 1132 Sammon Loss : 1317.67236328125\n",
            "Batch 270 / 1132 Sammon Loss : 1447.520751953125\n",
            "Batch 280 / 1132 Sammon Loss : 1167.5478515625\n",
            "Batch 290 / 1132 Sammon Loss : 1228.056640625\n",
            "Batch 300 / 1132 Sammon Loss : 2558.17138671875\n",
            "Batch 310 / 1132 Sammon Loss : 1215.201416015625\n",
            "Batch 320 / 1132 Sammon Loss : 1047.546875\n",
            "Batch 330 / 1132 Sammon Loss : 1445.1240234375\n",
            "Batch 340 / 1132 Sammon Loss : 1220.3570556640625\n",
            "Batch 350 / 1132 Sammon Loss : 2123.559326171875\n",
            "Batch 360 / 1132 Sammon Loss : 2288.419921875\n",
            "Batch 370 / 1132 Sammon Loss : 2358.22998046875\n",
            "Batch 380 / 1132 Sammon Loss : 1881.8555908203125\n",
            "Batch 390 / 1132 Sammon Loss : 2297.43701171875\n",
            "Batch 400 / 1132 Sammon Loss : 2759.21728515625\n",
            "Batch 410 / 1132 Sammon Loss : 2012.0582275390625\n",
            "Batch 420 / 1132 Sammon Loss : 1990.482421875\n",
            "Batch 430 / 1132 Sammon Loss : 2158.5185546875\n",
            "Batch 440 / 1132 Sammon Loss : 3083.72509765625\n",
            "Batch 450 / 1132 Sammon Loss : 1934.0396728515625\n",
            "Batch 460 / 1132 Sammon Loss : 2381.109375\n",
            "Batch 470 / 1132 Sammon Loss : 1916.5831298828125\n",
            "Batch 480 / 1132 Sammon Loss : 2465.96142578125\n",
            "Batch 490 / 1132 Sammon Loss : 1409.413818359375\n",
            "Batch 500 / 1132 Sammon Loss : 5433.892578125\n",
            "Batch 510 / 1132 Sammon Loss : 5278.7529296875\n",
            "Batch 520 / 1132 Sammon Loss : 3698.62548828125\n",
            "Batch 530 / 1132 Sammon Loss : 4749.58203125\n",
            "Batch 540 / 1132 Sammon Loss : 1191.876953125\n",
            "Batch 550 / 1132 Sammon Loss : 1338.93896484375\n",
            "Batch 560 / 1132 Sammon Loss : 1485.9212646484375\n",
            "Batch 570 / 1132 Sammon Loss : 1451.79443359375\n",
            "Batch 580 / 1132 Sammon Loss : 1690.2677001953125\n",
            "Batch 590 / 1132 Sammon Loss : 1800.475341796875\n",
            "Batch 600 / 1132 Sammon Loss : 1365.56298828125\n",
            "Batch 610 / 1132 Sammon Loss : 1209.46728515625\n",
            "Batch 620 / 1132 Sammon Loss : 3097.7724609375\n",
            "Batch 630 / 1132 Sammon Loss : 3245.677978515625\n",
            "Batch 640 / 1132 Sammon Loss : 2906.834228515625\n",
            "Batch 650 / 1132 Sammon Loss : 3698.63623046875\n",
            "Batch 660 / 1132 Sammon Loss : 1688.25244140625\n",
            "Batch 670 / 1132 Sammon Loss : 1351.166748046875\n",
            "Batch 680 / 1132 Sammon Loss : 1825.7840576171875\n",
            "Batch 690 / 1132 Sammon Loss : 1264.4271240234375\n",
            "Batch 700 / 1132 Sammon Loss : 1657.192626953125\n",
            "Batch 710 / 1132 Sammon Loss : 1191.7559814453125\n",
            "Batch 720 / 1132 Sammon Loss : 1811.653076171875\n",
            "Batch 730 / 1132 Sammon Loss : 853.8897705078125\n",
            "Batch 740 / 1132 Sammon Loss : 1539.76416015625\n",
            "Batch 750 / 1132 Sammon Loss : 1185.1558837890625\n",
            "Batch 760 / 1132 Sammon Loss : 1639.7880859375\n",
            "Batch 770 / 1132 Sammon Loss : 2482.33642578125\n",
            "Batch 780 / 1132 Sammon Loss : 2154.16259765625\n",
            "Batch 790 / 1132 Sammon Loss : 1439.222900390625\n",
            "Batch 800 / 1132 Sammon Loss : 2494.0859375\n",
            "Batch 810 / 1132 Sammon Loss : 1177.526123046875\n",
            "Batch 820 / 1132 Sammon Loss : 1355.2254638671875\n",
            "Batch 830 / 1132 Sammon Loss : 2278.66015625\n",
            "Batch 840 / 1132 Sammon Loss : 1926.999267578125\n",
            "Batch 850 / 1132 Sammon Loss : 3729.34814453125\n",
            "Batch 860 / 1132 Sammon Loss : 4049.963134765625\n",
            "Batch 870 / 1132 Sammon Loss : 4424.39404296875\n",
            "Batch 880 / 1132 Sammon Loss : 1950.4632568359375\n",
            "Batch 890 / 1132 Sammon Loss : 1692.4249267578125\n",
            "Batch 900 / 1132 Sammon Loss : 1635.31103515625\n",
            "Batch 910 / 1132 Sammon Loss : 1909.908935546875\n",
            "Batch 920 / 1132 Sammon Loss : 1390.47607421875\n",
            "Batch 930 / 1132 Sammon Loss : 3002.497314453125\n",
            "Batch 940 / 1132 Sammon Loss : 3889.37109375\n",
            "Batch 950 / 1132 Sammon Loss : 3255.951171875\n",
            "Batch 960 / 1132 Sammon Loss : 2390.19189453125\n",
            "Batch 970 / 1132 Sammon Loss : 1648.68896484375\n",
            "Batch 980 / 1132 Sammon Loss : 1407.7880859375\n",
            "Batch 990 / 1132 Sammon Loss : 1846.5008544921875\n",
            "Batch 1000 / 1132 Sammon Loss : 1452.7149658203125\n",
            "Batch 1010 / 1132 Sammon Loss : 1097.5498046875\n",
            "Batch 1020 / 1132 Sammon Loss : 2151.7275390625\n",
            "Batch 1030 / 1132 Sammon Loss : 2043.805419921875\n",
            "Batch 1040 / 1132 Sammon Loss : 2369.364013671875\n",
            "Batch 1050 / 1132 Sammon Loss : 1646.1571044921875\n",
            "Batch 1060 / 1132 Sammon Loss : 1543.08740234375\n",
            "Batch 1070 / 1132 Sammon Loss : 3799.1435546875\n",
            "Batch 1080 / 1132 Sammon Loss : 2138.23291015625\n",
            "Batch 1090 / 1132 Sammon Loss : 1455.99169921875\n",
            "Batch 1100 / 1132 Sammon Loss : 4509.79541015625\n",
            "Batch 1110 / 1132 Sammon Loss : 1554.897216796875\n",
            "Batch 1120 / 1132 Sammon Loss : 1907.188720703125\n",
            "Batch 1130 / 1132 Sammon Loss : 1704.389892578125\n",
            "Batch 10 / 505 Regressor Loss : 106875.7734375\n",
            "Batch 20 / 505 Regressor Loss : 118025.7109375\n",
            "Batch 30 / 505 Regressor Loss : 101050.25\n",
            "Batch 40 / 505 Regressor Loss : 102008.71875\n",
            "Batch 50 / 505 Regressor Loss : 132959.46875\n",
            "Batch 60 / 505 Regressor Loss : 127374.8984375\n",
            "Batch 70 / 505 Regressor Loss : 126941.921875\n",
            "Batch 80 / 505 Regressor Loss : 129046.8046875\n",
            "Batch 90 / 505 Regressor Loss : 132352.75\n",
            "Batch 100 / 505 Regressor Loss : 107027.3671875\n",
            "Batch 110 / 505 Regressor Loss : 125101.65625\n",
            "Batch 120 / 505 Regressor Loss : 114309.109375\n",
            "Batch 130 / 505 Regressor Loss : 128770.9375\n",
            "Batch 140 / 505 Regressor Loss : 134849.609375\n",
            "Batch 150 / 505 Regressor Loss : 119414.703125\n",
            "Batch 160 / 505 Regressor Loss : 95269.234375\n",
            "Batch 170 / 505 Regressor Loss : 104092.1875\n",
            "Batch 180 / 505 Regressor Loss : 133042.65625\n",
            "Batch 190 / 505 Regressor Loss : 127675.6796875\n",
            "Batch 200 / 505 Regressor Loss : 104708.640625\n",
            "Batch 210 / 505 Regressor Loss : 112381.796875\n",
            "Batch 220 / 505 Regressor Loss : 125044.78125\n",
            "Batch 230 / 505 Regressor Loss : 117820.9375\n",
            "Batch 240 / 505 Regressor Loss : 109256.5234375\n",
            "Batch 250 / 505 Regressor Loss : 150737.828125\n",
            "Batch 260 / 505 Regressor Loss : 127716.375\n",
            "Batch 270 / 505 Regressor Loss : 108255.0859375\n",
            "Batch 280 / 505 Regressor Loss : 126053.0234375\n",
            "Batch 290 / 505 Regressor Loss : 124805.6875\n",
            "Batch 300 / 505 Regressor Loss : 116716.6171875\n",
            "Batch 310 / 505 Regressor Loss : 120935.5\n",
            "Batch 320 / 505 Regressor Loss : 147285.5\n",
            "Batch 330 / 505 Regressor Loss : 113873.21875\n",
            "Batch 340 / 505 Regressor Loss : 111056.5\n",
            "Batch 350 / 505 Regressor Loss : 94179.3984375\n",
            "Batch 360 / 505 Regressor Loss : 101380.1171875\n",
            "Batch 370 / 505 Regressor Loss : 121176.6875\n",
            "Batch 380 / 505 Regressor Loss : 104775.53125\n",
            "Batch 390 / 505 Regressor Loss : 116734.5\n",
            "Batch 400 / 505 Regressor Loss : 140055.40625\n",
            "Batch 410 / 505 Regressor Loss : 105732.9609375\n",
            "Batch 420 / 505 Regressor Loss : 126695.3125\n",
            "Batch 430 / 505 Regressor Loss : 111152.734375\n",
            "Batch 440 / 505 Regressor Loss : 112628.6171875\n",
            "Batch 450 / 505 Regressor Loss : 100451.359375\n",
            "Batch 460 / 505 Regressor Loss : 128418.515625\n",
            "Batch 470 / 505 Regressor Loss : 120870.484375\n",
            "Batch 480 / 505 Regressor Loss : 121747.1875\n",
            "Batch 490 / 505 Regressor Loss : 121886.359375\n",
            "Batch 500 / 505 Regressor Loss : 119551.3984375\n",
            "Epoch  71 / 100  Sammon Loss : 1118.83740234375  Regressor Loss : 103395.875\n",
            "Val Loss : 102285.734375\n",
            "Batch 10 / 1132 Sammon Loss : 3473.8017578125\n",
            "Batch 20 / 1132 Sammon Loss : 3945.05322265625\n",
            "Batch 30 / 1132 Sammon Loss : 3969.234375\n",
            "Batch 40 / 1132 Sammon Loss : 4133.990234375\n",
            "Batch 50 / 1132 Sammon Loss : 4304.61669921875\n",
            "Batch 60 / 1132 Sammon Loss : 2199.68408203125\n",
            "Batch 70 / 1132 Sammon Loss : 2255.9794921875\n",
            "Batch 80 / 1132 Sammon Loss : 1963.364501953125\n",
            "Batch 90 / 1132 Sammon Loss : 1955.994384765625\n",
            "Batch 100 / 1132 Sammon Loss : 1778.572998046875\n",
            "Batch 110 / 1132 Sammon Loss : 1372.091796875\n",
            "Batch 120 / 1132 Sammon Loss : 719.75830078125\n",
            "Batch 130 / 1132 Sammon Loss : 1414.9261474609375\n",
            "Batch 140 / 1132 Sammon Loss : 1536.146484375\n",
            "Batch 150 / 1132 Sammon Loss : 1769.014404296875\n",
            "Batch 160 / 1132 Sammon Loss : 1654.5615234375\n",
            "Batch 170 / 1132 Sammon Loss : 3689.126953125\n",
            "Batch 180 / 1132 Sammon Loss : 4216.7080078125\n",
            "Batch 190 / 1132 Sammon Loss : 3188.18408203125\n",
            "Batch 200 / 1132 Sammon Loss : 2973.30908203125\n",
            "Batch 210 / 1132 Sammon Loss : 2791.9306640625\n",
            "Batch 220 / 1132 Sammon Loss : 2141.1572265625\n",
            "Batch 230 / 1132 Sammon Loss : 1823.13623046875\n",
            "Batch 240 / 1132 Sammon Loss : 1247.951171875\n",
            "Batch 250 / 1132 Sammon Loss : 1182.681884765625\n",
            "Batch 260 / 1132 Sammon Loss : 1416.7008056640625\n",
            "Batch 270 / 1132 Sammon Loss : 1525.7193603515625\n",
            "Batch 280 / 1132 Sammon Loss : 1202.197021484375\n",
            "Batch 290 / 1132 Sammon Loss : 1304.132080078125\n",
            "Batch 300 / 1132 Sammon Loss : 2674.38525390625\n",
            "Batch 310 / 1132 Sammon Loss : 1292.024169921875\n",
            "Batch 320 / 1132 Sammon Loss : 1113.723388671875\n",
            "Batch 330 / 1132 Sammon Loss : 1507.555419921875\n",
            "Batch 340 / 1132 Sammon Loss : 1296.235595703125\n",
            "Batch 350 / 1132 Sammon Loss : 2199.31201171875\n",
            "Batch 360 / 1132 Sammon Loss : 2371.79541015625\n",
            "Batch 370 / 1132 Sammon Loss : 2375.45361328125\n",
            "Batch 380 / 1132 Sammon Loss : 1931.2021484375\n",
            "Batch 390 / 1132 Sammon Loss : 2342.626708984375\n",
            "Batch 400 / 1132 Sammon Loss : 2792.4453125\n",
            "Batch 410 / 1132 Sammon Loss : 2040.79638671875\n",
            "Batch 420 / 1132 Sammon Loss : 2021.7423095703125\n",
            "Batch 430 / 1132 Sammon Loss : 2201.888671875\n",
            "Batch 440 / 1132 Sammon Loss : 3031.04931640625\n",
            "Batch 450 / 1132 Sammon Loss : 1964.52490234375\n",
            "Batch 460 / 1132 Sammon Loss : 2408.279541015625\n",
            "Batch 470 / 1132 Sammon Loss : 1944.302490234375\n",
            "Batch 480 / 1132 Sammon Loss : 2518.408447265625\n",
            "Batch 490 / 1132 Sammon Loss : 1496.0625\n",
            "Batch 500 / 1132 Sammon Loss : 5538.701171875\n",
            "Batch 510 / 1132 Sammon Loss : 5384.0927734375\n",
            "Batch 520 / 1132 Sammon Loss : 3774.24462890625\n",
            "Batch 530 / 1132 Sammon Loss : 4873.134765625\n",
            "Batch 540 / 1132 Sammon Loss : 1275.59326171875\n",
            "Batch 550 / 1132 Sammon Loss : 1422.629638671875\n",
            "Batch 560 / 1132 Sammon Loss : 1568.554443359375\n",
            "Batch 570 / 1132 Sammon Loss : 1524.4715576171875\n",
            "Batch 580 / 1132 Sammon Loss : 1713.833984375\n",
            "Batch 590 / 1132 Sammon Loss : 1840.598388671875\n",
            "Batch 600 / 1132 Sammon Loss : 1379.091064453125\n",
            "Batch 610 / 1132 Sammon Loss : 1234.778564453125\n",
            "Batch 620 / 1132 Sammon Loss : 3199.6484375\n",
            "Batch 630 / 1132 Sammon Loss : 3377.822998046875\n",
            "Batch 640 / 1132 Sammon Loss : 3049.622314453125\n",
            "Batch 650 / 1132 Sammon Loss : 3858.739013671875\n",
            "Batch 660 / 1132 Sammon Loss : 1713.818115234375\n",
            "Batch 670 / 1132 Sammon Loss : 1282.561767578125\n",
            "Batch 680 / 1132 Sammon Loss : 1845.037841796875\n",
            "Batch 690 / 1132 Sammon Loss : 1260.2215576171875\n",
            "Batch 700 / 1132 Sammon Loss : 1730.7890625\n",
            "Batch 710 / 1132 Sammon Loss : 1224.2415771484375\n",
            "Batch 720 / 1132 Sammon Loss : 1892.492919921875\n",
            "Batch 730 / 1132 Sammon Loss : 834.665771484375\n",
            "Batch 740 / 1132 Sammon Loss : 1578.593994140625\n",
            "Batch 750 / 1132 Sammon Loss : 1196.84619140625\n",
            "Batch 760 / 1132 Sammon Loss : 1718.830810546875\n",
            "Batch 770 / 1132 Sammon Loss : 2509.0771484375\n",
            "Batch 780 / 1132 Sammon Loss : 2205.5087890625\n",
            "Batch 790 / 1132 Sammon Loss : 1460.566650390625\n",
            "Batch 800 / 1132 Sammon Loss : 2493.611328125\n",
            "Batch 810 / 1132 Sammon Loss : 1200.603271484375\n",
            "Batch 820 / 1132 Sammon Loss : 1351.751220703125\n",
            "Batch 830 / 1132 Sammon Loss : 2283.96630859375\n",
            "Batch 840 / 1132 Sammon Loss : 1945.5977783203125\n",
            "Batch 850 / 1132 Sammon Loss : 3783.11279296875\n",
            "Batch 860 / 1132 Sammon Loss : 4081.3486328125\n",
            "Batch 870 / 1132 Sammon Loss : 4453.65576171875\n",
            "Batch 880 / 1132 Sammon Loss : 1993.1986083984375\n",
            "Batch 890 / 1132 Sammon Loss : 1736.2041015625\n",
            "Batch 900 / 1132 Sammon Loss : 1653.6075439453125\n",
            "Batch 910 / 1132 Sammon Loss : 1879.7117919921875\n",
            "Batch 920 / 1132 Sammon Loss : 1386.092041015625\n",
            "Batch 930 / 1132 Sammon Loss : 3013.4892578125\n",
            "Batch 940 / 1132 Sammon Loss : 3916.114501953125\n",
            "Batch 950 / 1132 Sammon Loss : 3278.238525390625\n",
            "Batch 960 / 1132 Sammon Loss : 2402.33154296875\n",
            "Batch 970 / 1132 Sammon Loss : 1628.2015380859375\n",
            "Batch 980 / 1132 Sammon Loss : 1369.4263916015625\n",
            "Batch 990 / 1132 Sammon Loss : 1839.7808837890625\n",
            "Batch 1000 / 1132 Sammon Loss : 1442.7327880859375\n",
            "Batch 1010 / 1132 Sammon Loss : 1076.6243896484375\n",
            "Batch 1020 / 1132 Sammon Loss : 2142.826171875\n",
            "Batch 1030 / 1132 Sammon Loss : 2035.037841796875\n",
            "Batch 1040 / 1132 Sammon Loss : 2395.0205078125\n",
            "Batch 1050 / 1132 Sammon Loss : 1628.9666748046875\n",
            "Batch 1060 / 1132 Sammon Loss : 1538.087646484375\n",
            "Batch 1070 / 1132 Sammon Loss : 3841.874267578125\n",
            "Batch 1080 / 1132 Sammon Loss : 2116.66259765625\n",
            "Batch 1090 / 1132 Sammon Loss : 1419.0899658203125\n",
            "Batch 1100 / 1132 Sammon Loss : 4530.8466796875\n",
            "Batch 1110 / 1132 Sammon Loss : 1537.053466796875\n",
            "Batch 1120 / 1132 Sammon Loss : 1866.7938232421875\n",
            "Batch 1130 / 1132 Sammon Loss : 1680.1466064453125\n",
            "Batch 10 / 505 Regressor Loss : 106660.7734375\n",
            "Batch 20 / 505 Regressor Loss : 117806.59375\n",
            "Batch 30 / 505 Regressor Loss : 100830.125\n",
            "Batch 40 / 505 Regressor Loss : 101785.828125\n",
            "Batch 50 / 505 Regressor Loss : 132690.234375\n",
            "Batch 60 / 505 Regressor Loss : 127129.3359375\n",
            "Batch 70 / 505 Regressor Loss : 126705.203125\n",
            "Batch 80 / 505 Regressor Loss : 128790.40625\n",
            "Batch 90 / 505 Regressor Loss : 132118.40625\n",
            "Batch 100 / 505 Regressor Loss : 106796.953125\n",
            "Batch 110 / 505 Regressor Loss : 124872.234375\n",
            "Batch 120 / 505 Regressor Loss : 114097.59375\n",
            "Batch 130 / 505 Regressor Loss : 128541.3359375\n",
            "Batch 140 / 505 Regressor Loss : 134605.0\n",
            "Batch 150 / 505 Regressor Loss : 119186.9609375\n",
            "Batch 160 / 505 Regressor Loss : 95059.546875\n",
            "Batch 170 / 505 Regressor Loss : 103874.25\n",
            "Batch 180 / 505 Regressor Loss : 132797.96875\n",
            "Batch 190 / 505 Regressor Loss : 127421.0\n",
            "Batch 200 / 505 Regressor Loss : 104506.015625\n",
            "Batch 210 / 505 Regressor Loss : 112154.625\n",
            "Batch 220 / 505 Regressor Loss : 124809.03125\n",
            "Batch 230 / 505 Regressor Loss : 117597.5625\n",
            "Batch 240 / 505 Regressor Loss : 109046.953125\n",
            "Batch 250 / 505 Regressor Loss : 150475.8125\n",
            "Batch 260 / 505 Regressor Loss : 127469.4609375\n",
            "Batch 270 / 505 Regressor Loss : 108041.8359375\n",
            "Batch 280 / 505 Regressor Loss : 125814.265625\n",
            "Batch 290 / 505 Regressor Loss : 124582.953125\n",
            "Batch 300 / 505 Regressor Loss : 116482.0234375\n",
            "Batch 310 / 505 Regressor Loss : 120704.8359375\n",
            "Batch 320 / 505 Regressor Loss : 147020.875\n",
            "Batch 330 / 505 Regressor Loss : 113639.671875\n",
            "Batch 340 / 505 Regressor Loss : 110814.71875\n",
            "Batch 350 / 505 Regressor Loss : 93967.828125\n",
            "Batch 360 / 505 Regressor Loss : 101182.03125\n",
            "Batch 370 / 505 Regressor Loss : 120937.8984375\n",
            "Batch 380 / 505 Regressor Loss : 104562.25\n",
            "Batch 390 / 505 Regressor Loss : 116508.0234375\n",
            "Batch 400 / 505 Regressor Loss : 139783.3125\n",
            "Batch 410 / 505 Regressor Loss : 105518.1484375\n",
            "Batch 420 / 505 Regressor Loss : 126461.671875\n",
            "Batch 430 / 505 Regressor Loss : 110929.9921875\n",
            "Batch 440 / 505 Regressor Loss : 112399.3984375\n",
            "Batch 450 / 505 Regressor Loss : 100248.40625\n",
            "Batch 460 / 505 Regressor Loss : 128182.171875\n",
            "Batch 470 / 505 Regressor Loss : 120655.28125\n",
            "Batch 480 / 505 Regressor Loss : 121519.296875\n",
            "Batch 490 / 505 Regressor Loss : 121651.796875\n",
            "Batch 500 / 505 Regressor Loss : 119333.296875\n",
            "Epoch  72 / 100  Sammon Loss : 1058.821533203125  Regressor Loss : 103193.640625\n",
            "Val Loss : 102030.625\n",
            "Batch 10 / 1132 Sammon Loss : 3479.595703125\n",
            "Batch 20 / 1132 Sammon Loss : 3936.94140625\n",
            "Batch 30 / 1132 Sammon Loss : 3951.30126953125\n",
            "Batch 40 / 1132 Sammon Loss : 4126.02978515625\n",
            "Batch 50 / 1132 Sammon Loss : 4275.306640625\n",
            "Batch 60 / 1132 Sammon Loss : 2186.393798828125\n",
            "Batch 70 / 1132 Sammon Loss : 2229.371337890625\n",
            "Batch 80 / 1132 Sammon Loss : 1923.41943359375\n",
            "Batch 90 / 1132 Sammon Loss : 1962.491455078125\n",
            "Batch 100 / 1132 Sammon Loss : 1808.5484619140625\n",
            "Batch 110 / 1132 Sammon Loss : 1393.216064453125\n",
            "Batch 120 / 1132 Sammon Loss : 765.5798950195312\n",
            "Batch 130 / 1132 Sammon Loss : 1417.395263671875\n",
            "Batch 140 / 1132 Sammon Loss : 1532.0059814453125\n",
            "Batch 150 / 1132 Sammon Loss : 1694.271728515625\n",
            "Batch 160 / 1132 Sammon Loss : 1587.58349609375\n",
            "Batch 170 / 1132 Sammon Loss : 3588.837890625\n",
            "Batch 180 / 1132 Sammon Loss : 4138.42529296875\n",
            "Batch 190 / 1132 Sammon Loss : 3117.802734375\n",
            "Batch 200 / 1132 Sammon Loss : 2889.62939453125\n",
            "Batch 210 / 1132 Sammon Loss : 2700.022216796875\n",
            "Batch 220 / 1132 Sammon Loss : 2039.119140625\n",
            "Batch 230 / 1132 Sammon Loss : 1755.397705078125\n",
            "Batch 240 / 1132 Sammon Loss : 1176.9306640625\n",
            "Batch 250 / 1132 Sammon Loss : 1126.206787109375\n",
            "Batch 260 / 1132 Sammon Loss : 1354.410400390625\n",
            "Batch 270 / 1132 Sammon Loss : 1463.9903564453125\n",
            "Batch 280 / 1132 Sammon Loss : 1173.36083984375\n",
            "Batch 290 / 1132 Sammon Loss : 1224.89404296875\n",
            "Batch 300 / 1132 Sammon Loss : 2577.307861328125\n",
            "Batch 310 / 1132 Sammon Loss : 1236.6357421875\n",
            "Batch 320 / 1132 Sammon Loss : 1092.3338623046875\n",
            "Batch 330 / 1132 Sammon Loss : 1466.454345703125\n",
            "Batch 340 / 1132 Sammon Loss : 1307.825439453125\n",
            "Batch 350 / 1132 Sammon Loss : 2155.344482421875\n",
            "Batch 360 / 1132 Sammon Loss : 2380.73876953125\n",
            "Batch 370 / 1132 Sammon Loss : 2368.602783203125\n",
            "Batch 380 / 1132 Sammon Loss : 1893.990966796875\n",
            "Batch 390 / 1132 Sammon Loss : 2296.516357421875\n",
            "Batch 400 / 1132 Sammon Loss : 2819.673583984375\n",
            "Batch 410 / 1132 Sammon Loss : 2040.942626953125\n",
            "Batch 420 / 1132 Sammon Loss : 2019.8446044921875\n",
            "Batch 430 / 1132 Sammon Loss : 2185.50341796875\n",
            "Batch 440 / 1132 Sammon Loss : 3093.420166015625\n",
            "Batch 450 / 1132 Sammon Loss : 1953.776611328125\n",
            "Batch 460 / 1132 Sammon Loss : 2402.97119140625\n",
            "Batch 470 / 1132 Sammon Loss : 1960.600341796875\n",
            "Batch 480 / 1132 Sammon Loss : 2553.11083984375\n",
            "Batch 490 / 1132 Sammon Loss : 1486.5560302734375\n",
            "Batch 500 / 1132 Sammon Loss : 5477.1767578125\n",
            "Batch 510 / 1132 Sammon Loss : 5358.990234375\n",
            "Batch 520 / 1132 Sammon Loss : 3755.1728515625\n",
            "Batch 530 / 1132 Sammon Loss : 4775.91357421875\n",
            "Batch 540 / 1132 Sammon Loss : 1229.704833984375\n",
            "Batch 550 / 1132 Sammon Loss : 1392.1650390625\n",
            "Batch 560 / 1132 Sammon Loss : 1514.084716796875\n",
            "Batch 570 / 1132 Sammon Loss : 1530.865966796875\n",
            "Batch 580 / 1132 Sammon Loss : 1734.23291015625\n",
            "Batch 590 / 1132 Sammon Loss : 1873.3905029296875\n",
            "Batch 600 / 1132 Sammon Loss : 1375.7008056640625\n",
            "Batch 610 / 1132 Sammon Loss : 1192.8538818359375\n",
            "Batch 620 / 1132 Sammon Loss : 3234.2255859375\n",
            "Batch 630 / 1132 Sammon Loss : 3345.87158203125\n",
            "Batch 640 / 1132 Sammon Loss : 3015.328125\n",
            "Batch 650 / 1132 Sammon Loss : 3822.2177734375\n",
            "Batch 660 / 1132 Sammon Loss : 1685.445068359375\n",
            "Batch 670 / 1132 Sammon Loss : 1301.8560791015625\n",
            "Batch 680 / 1132 Sammon Loss : 1831.3922119140625\n",
            "Batch 690 / 1132 Sammon Loss : 1231.045654296875\n",
            "Batch 700 / 1132 Sammon Loss : 1689.006103515625\n",
            "Batch 710 / 1132 Sammon Loss : 1209.9552001953125\n",
            "Batch 720 / 1132 Sammon Loss : 1846.366455078125\n",
            "Batch 730 / 1132 Sammon Loss : 827.0075073242188\n",
            "Batch 740 / 1132 Sammon Loss : 1557.27587890625\n",
            "Batch 750 / 1132 Sammon Loss : 1195.612548828125\n",
            "Batch 760 / 1132 Sammon Loss : 1697.6865234375\n",
            "Batch 770 / 1132 Sammon Loss : 2478.398193359375\n",
            "Batch 780 / 1132 Sammon Loss : 2194.0595703125\n",
            "Batch 790 / 1132 Sammon Loss : 1464.891357421875\n",
            "Batch 800 / 1132 Sammon Loss : 2496.593505859375\n",
            "Batch 810 / 1132 Sammon Loss : 1212.95166015625\n",
            "Batch 820 / 1132 Sammon Loss : 1366.1507568359375\n",
            "Batch 830 / 1132 Sammon Loss : 2303.221435546875\n",
            "Batch 840 / 1132 Sammon Loss : 1940.116943359375\n",
            "Batch 850 / 1132 Sammon Loss : 3719.875244140625\n",
            "Batch 860 / 1132 Sammon Loss : 4047.634033203125\n",
            "Batch 870 / 1132 Sammon Loss : 4437.8037109375\n",
            "Batch 880 / 1132 Sammon Loss : 1988.834228515625\n",
            "Batch 890 / 1132 Sammon Loss : 1765.3646240234375\n",
            "Batch 900 / 1132 Sammon Loss : 1646.049072265625\n",
            "Batch 910 / 1132 Sammon Loss : 1967.020751953125\n",
            "Batch 920 / 1132 Sammon Loss : 1459.3941650390625\n",
            "Batch 930 / 1132 Sammon Loss : 2911.216796875\n",
            "Batch 940 / 1132 Sammon Loss : 3889.5537109375\n",
            "Batch 950 / 1132 Sammon Loss : 3256.59814453125\n",
            "Batch 960 / 1132 Sammon Loss : 2387.4921875\n",
            "Batch 970 / 1132 Sammon Loss : 1672.385498046875\n",
            "Batch 980 / 1132 Sammon Loss : 1451.3857421875\n",
            "Batch 990 / 1132 Sammon Loss : 1923.9989013671875\n",
            "Batch 1000 / 1132 Sammon Loss : 1544.710693359375\n",
            "Batch 1010 / 1132 Sammon Loss : 1148.4267578125\n",
            "Batch 1020 / 1132 Sammon Loss : 2165.4931640625\n",
            "Batch 1030 / 1132 Sammon Loss : 2036.732421875\n",
            "Batch 1040 / 1132 Sammon Loss : 2389.26171875\n",
            "Batch 1050 / 1132 Sammon Loss : 1693.527099609375\n",
            "Batch 1060 / 1132 Sammon Loss : 1589.786376953125\n",
            "Batch 1070 / 1132 Sammon Loss : 3808.50634765625\n",
            "Batch 1080 / 1132 Sammon Loss : 2129.20166015625\n",
            "Batch 1090 / 1132 Sammon Loss : 1476.828857421875\n",
            "Batch 1100 / 1132 Sammon Loss : 4509.85791015625\n",
            "Batch 1110 / 1132 Sammon Loss : 1547.8817138671875\n",
            "Batch 1120 / 1132 Sammon Loss : 1883.114013671875\n",
            "Batch 1130 / 1132 Sammon Loss : 1712.725341796875\n",
            "Batch 10 / 505 Regressor Loss : 106446.171875\n",
            "Batch 20 / 505 Regressor Loss : 117587.875\n",
            "Batch 30 / 505 Regressor Loss : 100610.3984375\n",
            "Batch 40 / 505 Regressor Loss : 101563.328125\n",
            "Batch 50 / 505 Regressor Loss : 132421.4375\n",
            "Batch 60 / 505 Regressor Loss : 126884.2109375\n",
            "Batch 70 / 505 Regressor Loss : 126468.875\n",
            "Batch 80 / 505 Regressor Loss : 128534.4375\n",
            "Batch 90 / 505 Regressor Loss : 131884.484375\n",
            "Batch 100 / 505 Regressor Loss : 106566.921875\n",
            "Batch 110 / 505 Regressor Loss : 124643.171875\n",
            "Batch 120 / 505 Regressor Loss : 113886.4609375\n",
            "Batch 130 / 505 Regressor Loss : 128312.109375\n",
            "Batch 140 / 505 Regressor Loss : 134360.8125\n",
            "Batch 150 / 505 Regressor Loss : 118959.6171875\n",
            "Batch 160 / 505 Regressor Loss : 94850.25\n",
            "Batch 170 / 505 Regressor Loss : 103656.7109375\n",
            "Batch 180 / 505 Regressor Loss : 132553.71875\n",
            "Batch 190 / 505 Regressor Loss : 127166.734375\n",
            "Batch 200 / 505 Regressor Loss : 104303.75\n",
            "Batch 210 / 505 Regressor Loss : 111927.875\n",
            "Batch 220 / 505 Regressor Loss : 124573.6875\n",
            "Batch 230 / 505 Regressor Loss : 117374.5859375\n",
            "Batch 240 / 505 Regressor Loss : 108837.75\n",
            "Batch 250 / 505 Regressor Loss : 150214.234375\n",
            "Batch 260 / 505 Regressor Loss : 127222.96875\n",
            "Batch 270 / 505 Regressor Loss : 107828.984375\n",
            "Batch 280 / 505 Regressor Loss : 125575.90625\n",
            "Batch 290 / 505 Regressor Loss : 124360.5859375\n",
            "Batch 300 / 505 Regressor Loss : 116247.8046875\n",
            "Batch 310 / 505 Regressor Loss : 120474.5234375\n",
            "Batch 320 / 505 Regressor Loss : 146756.640625\n",
            "Batch 330 / 505 Regressor Loss : 113406.515625\n",
            "Batch 340 / 505 Regressor Loss : 110573.34375\n",
            "Batch 350 / 505 Regressor Loss : 93756.65625\n",
            "Batch 360 / 505 Regressor Loss : 100984.3359375\n",
            "Batch 370 / 505 Regressor Loss : 120699.5234375\n",
            "Batch 380 / 505 Regressor Loss : 104349.359375\n",
            "Batch 390 / 505 Regressor Loss : 116281.953125\n",
            "Batch 400 / 505 Regressor Loss : 139511.671875\n",
            "Batch 410 / 505 Regressor Loss : 105303.75\n",
            "Batch 420 / 505 Regressor Loss : 126228.4296875\n",
            "Batch 430 / 505 Regressor Loss : 110707.640625\n",
            "Batch 440 / 505 Regressor Loss : 112170.5859375\n",
            "Batch 450 / 505 Regressor Loss : 100045.84375\n",
            "Batch 460 / 505 Regressor Loss : 127946.2421875\n",
            "Batch 470 / 505 Regressor Loss : 120440.4609375\n",
            "Batch 480 / 505 Regressor Loss : 121291.78125\n",
            "Batch 490 / 505 Regressor Loss : 121417.6484375\n",
            "Batch 500 / 505 Regressor Loss : 119115.5859375\n",
            "Epoch  73 / 100  Sammon Loss : 1116.0748291015625  Regressor Loss : 102991.75\n",
            "Val Loss : 101775.9375\n",
            "Batch 10 / 1132 Sammon Loss : 3497.64892578125\n",
            "Batch 20 / 1132 Sammon Loss : 3935.49951171875\n",
            "Batch 30 / 1132 Sammon Loss : 3984.7294921875\n",
            "Batch 40 / 1132 Sammon Loss : 4132.947265625\n",
            "Batch 50 / 1132 Sammon Loss : 4319.25634765625\n",
            "Batch 60 / 1132 Sammon Loss : 2204.24658203125\n",
            "Batch 70 / 1132 Sammon Loss : 2249.9560546875\n",
            "Batch 80 / 1132 Sammon Loss : 1943.0186767578125\n",
            "Batch 90 / 1132 Sammon Loss : 1975.968994140625\n",
            "Batch 100 / 1132 Sammon Loss : 1776.1116943359375\n",
            "Batch 110 / 1132 Sammon Loss : 1374.4190673828125\n",
            "Batch 120 / 1132 Sammon Loss : 742.75634765625\n",
            "Batch 130 / 1132 Sammon Loss : 1395.007568359375\n",
            "Batch 140 / 1132 Sammon Loss : 1518.789306640625\n",
            "Batch 150 / 1132 Sammon Loss : 1746.2730712890625\n",
            "Batch 160 / 1132 Sammon Loss : 1627.6800537109375\n",
            "Batch 170 / 1132 Sammon Loss : 3708.0634765625\n",
            "Batch 180 / 1132 Sammon Loss : 4250.771484375\n",
            "Batch 190 / 1132 Sammon Loss : 3183.400390625\n",
            "Batch 200 / 1132 Sammon Loss : 2989.1591796875\n",
            "Batch 210 / 1132 Sammon Loss : 2773.16552734375\n",
            "Batch 220 / 1132 Sammon Loss : 2126.7919921875\n",
            "Batch 230 / 1132 Sammon Loss : 1823.686767578125\n",
            "Batch 240 / 1132 Sammon Loss : 1267.78662109375\n",
            "Batch 250 / 1132 Sammon Loss : 1140.091796875\n",
            "Batch 260 / 1132 Sammon Loss : 1385.1322021484375\n",
            "Batch 270 / 1132 Sammon Loss : 1512.5595703125\n",
            "Batch 280 / 1132 Sammon Loss : 1166.42041015625\n",
            "Batch 290 / 1132 Sammon Loss : 1273.13720703125\n",
            "Batch 300 / 1132 Sammon Loss : 2685.617919921875\n",
            "Batch 310 / 1132 Sammon Loss : 1280.176025390625\n",
            "Batch 320 / 1132 Sammon Loss : 1050.03466796875\n",
            "Batch 330 / 1132 Sammon Loss : 1486.858154296875\n",
            "Batch 340 / 1132 Sammon Loss : 1271.5115966796875\n",
            "Batch 350 / 1132 Sammon Loss : 2179.3017578125\n",
            "Batch 360 / 1132 Sammon Loss : 2409.1728515625\n",
            "Batch 370 / 1132 Sammon Loss : 2428.175537109375\n",
            "Batch 380 / 1132 Sammon Loss : 1941.4322509765625\n",
            "Batch 390 / 1132 Sammon Loss : 2349.60546875\n",
            "Batch 400 / 1132 Sammon Loss : 2910.1005859375\n",
            "Batch 410 / 1132 Sammon Loss : 2078.644287109375\n",
            "Batch 420 / 1132 Sammon Loss : 2078.90576171875\n",
            "Batch 430 / 1132 Sammon Loss : 2271.392333984375\n",
            "Batch 440 / 1132 Sammon Loss : 3186.224609375\n",
            "Batch 450 / 1132 Sammon Loss : 1982.166748046875\n",
            "Batch 460 / 1132 Sammon Loss : 2427.489990234375\n",
            "Batch 470 / 1132 Sammon Loss : 1957.292724609375\n",
            "Batch 480 / 1132 Sammon Loss : 2559.85986328125\n",
            "Batch 490 / 1132 Sammon Loss : 1492.939697265625\n",
            "Batch 500 / 1132 Sammon Loss : 5571.33984375\n",
            "Batch 510 / 1132 Sammon Loss : 5408.99365234375\n",
            "Batch 520 / 1132 Sammon Loss : 3809.02734375\n",
            "Batch 530 / 1132 Sammon Loss : 4830.669921875\n",
            "Batch 540 / 1132 Sammon Loss : 1288.47216796875\n",
            "Batch 550 / 1132 Sammon Loss : 1446.7598876953125\n",
            "Batch 560 / 1132 Sammon Loss : 1586.799072265625\n",
            "Batch 570 / 1132 Sammon Loss : 1590.69384765625\n",
            "Batch 580 / 1132 Sammon Loss : 1812.93017578125\n",
            "Batch 590 / 1132 Sammon Loss : 1941.227783203125\n",
            "Batch 600 / 1132 Sammon Loss : 1452.48876953125\n",
            "Batch 610 / 1132 Sammon Loss : 1348.3797607421875\n",
            "Batch 620 / 1132 Sammon Loss : 3206.77978515625\n",
            "Batch 630 / 1132 Sammon Loss : 3363.526611328125\n",
            "Batch 640 / 1132 Sammon Loss : 3054.810302734375\n",
            "Batch 650 / 1132 Sammon Loss : 3837.22607421875\n",
            "Batch 660 / 1132 Sammon Loss : 1726.073486328125\n",
            "Batch 670 / 1132 Sammon Loss : 1385.942138671875\n",
            "Batch 680 / 1132 Sammon Loss : 1907.62060546875\n",
            "Batch 690 / 1132 Sammon Loss : 1277.537109375\n",
            "Batch 700 / 1132 Sammon Loss : 1699.7139892578125\n",
            "Batch 710 / 1132 Sammon Loss : 1246.0242919921875\n",
            "Batch 720 / 1132 Sammon Loss : 1882.8712158203125\n",
            "Batch 730 / 1132 Sammon Loss : 868.02978515625\n",
            "Batch 740 / 1132 Sammon Loss : 1583.56884765625\n",
            "Batch 750 / 1132 Sammon Loss : 1228.612548828125\n",
            "Batch 760 / 1132 Sammon Loss : 1720.3931884765625\n",
            "Batch 770 / 1132 Sammon Loss : 2548.56787109375\n",
            "Batch 780 / 1132 Sammon Loss : 2244.6982421875\n",
            "Batch 790 / 1132 Sammon Loss : 1486.433349609375\n",
            "Batch 800 / 1132 Sammon Loss : 2534.97705078125\n",
            "Batch 810 / 1132 Sammon Loss : 1251.1864013671875\n",
            "Batch 820 / 1132 Sammon Loss : 1382.5325927734375\n",
            "Batch 830 / 1132 Sammon Loss : 2371.612548828125\n",
            "Batch 840 / 1132 Sammon Loss : 1982.986328125\n",
            "Batch 850 / 1132 Sammon Loss : 3808.09326171875\n",
            "Batch 860 / 1132 Sammon Loss : 4129.89990234375\n",
            "Batch 870 / 1132 Sammon Loss : 4527.576171875\n",
            "Batch 880 / 1132 Sammon Loss : 2052.654296875\n",
            "Batch 890 / 1132 Sammon Loss : 1808.9722900390625\n",
            "Batch 900 / 1132 Sammon Loss : 1689.635009765625\n",
            "Batch 910 / 1132 Sammon Loss : 1944.15478515625\n",
            "Batch 920 / 1132 Sammon Loss : 1449.4384765625\n",
            "Batch 930 / 1132 Sammon Loss : 3018.4150390625\n",
            "Batch 940 / 1132 Sammon Loss : 3968.94580078125\n",
            "Batch 950 / 1132 Sammon Loss : 3316.123046875\n",
            "Batch 960 / 1132 Sammon Loss : 2432.423828125\n",
            "Batch 970 / 1132 Sammon Loss : 1687.5089111328125\n",
            "Batch 980 / 1132 Sammon Loss : 1414.1026611328125\n",
            "Batch 990 / 1132 Sammon Loss : 1890.653076171875\n",
            "Batch 1000 / 1132 Sammon Loss : 1503.93896484375\n",
            "Batch 1010 / 1132 Sammon Loss : 1162.4276123046875\n",
            "Batch 1020 / 1132 Sammon Loss : 2211.598876953125\n",
            "Batch 1030 / 1132 Sammon Loss : 2075.15185546875\n",
            "Batch 1040 / 1132 Sammon Loss : 2406.03466796875\n",
            "Batch 1050 / 1132 Sammon Loss : 1702.695068359375\n",
            "Batch 1060 / 1132 Sammon Loss : 1603.01513671875\n",
            "Batch 1070 / 1132 Sammon Loss : 3859.55615234375\n",
            "Batch 1080 / 1132 Sammon Loss : 2172.69873046875\n",
            "Batch 1090 / 1132 Sammon Loss : 1451.371826171875\n",
            "Batch 1100 / 1132 Sammon Loss : 4622.2724609375\n",
            "Batch 1110 / 1132 Sammon Loss : 1577.126953125\n",
            "Batch 1120 / 1132 Sammon Loss : 1894.539794921875\n",
            "Batch 1130 / 1132 Sammon Loss : 1737.4493408203125\n",
            "Batch 10 / 505 Regressor Loss : 106231.9609375\n",
            "Batch 20 / 505 Regressor Loss : 117369.546875\n",
            "Batch 30 / 505 Regressor Loss : 100391.0625\n",
            "Batch 40 / 505 Regressor Loss : 101341.234375\n",
            "Batch 50 / 505 Regressor Loss : 132153.0625\n",
            "Batch 60 / 505 Regressor Loss : 126639.484375\n",
            "Batch 70 / 505 Regressor Loss : 126232.9375\n",
            "Batch 80 / 505 Regressor Loss : 128278.90625\n",
            "Batch 90 / 505 Regressor Loss : 131650.921875\n",
            "Batch 100 / 505 Regressor Loss : 106337.3046875\n",
            "Batch 110 / 505 Regressor Loss : 124414.5\n",
            "Batch 120 / 505 Regressor Loss : 113675.7109375\n",
            "Batch 130 / 505 Regressor Loss : 128083.265625\n",
            "Batch 140 / 505 Regressor Loss : 134117.03125\n",
            "Batch 150 / 505 Regressor Loss : 118732.6796875\n",
            "Batch 160 / 505 Regressor Loss : 94641.34375\n",
            "Batch 170 / 505 Regressor Loss : 103439.578125\n",
            "Batch 180 / 505 Regressor Loss : 132309.875\n",
            "Batch 190 / 505 Regressor Loss : 126912.875\n",
            "Batch 200 / 505 Regressor Loss : 104101.8984375\n",
            "Batch 210 / 505 Regressor Loss : 111701.5\n",
            "Batch 220 / 505 Regressor Loss : 124338.7734375\n",
            "Batch 230 / 505 Regressor Loss : 117152.0\n",
            "Batch 240 / 505 Regressor Loss : 108628.9375\n",
            "Batch 250 / 505 Regressor Loss : 149953.0625\n",
            "Batch 260 / 505 Regressor Loss : 126976.921875\n",
            "Batch 270 / 505 Regressor Loss : 107616.5234375\n",
            "Batch 280 / 505 Regressor Loss : 125337.984375\n",
            "Batch 290 / 505 Regressor Loss : 124138.625\n",
            "Batch 300 / 505 Regressor Loss : 116014.0\n",
            "Batch 310 / 505 Regressor Loss : 120244.640625\n",
            "Batch 320 / 505 Regressor Loss : 146492.859375\n",
            "Batch 330 / 505 Regressor Loss : 113173.75\n",
            "Batch 340 / 505 Regressor Loss : 110332.3984375\n",
            "Batch 350 / 505 Regressor Loss : 93545.8984375\n",
            "Batch 360 / 505 Regressor Loss : 100787.0234375\n",
            "Batch 370 / 505 Regressor Loss : 120461.5625\n",
            "Batch 380 / 505 Regressor Loss : 104136.890625\n",
            "Batch 390 / 505 Regressor Loss : 116056.25\n",
            "Batch 400 / 505 Regressor Loss : 139240.484375\n",
            "Batch 410 / 505 Regressor Loss : 105089.78125\n",
            "Batch 420 / 505 Regressor Loss : 125995.5859375\n",
            "Batch 430 / 505 Regressor Loss : 110485.6875\n",
            "Batch 440 / 505 Regressor Loss : 111942.171875\n",
            "Batch 450 / 505 Regressor Loss : 99843.6875\n",
            "Batch 460 / 505 Regressor Loss : 127710.7109375\n",
            "Batch 470 / 505 Regressor Loss : 120226.0234375\n",
            "Batch 480 / 505 Regressor Loss : 121064.671875\n",
            "Batch 490 / 505 Regressor Loss : 121183.8984375\n",
            "Batch 500 / 505 Regressor Loss : 118898.28125\n",
            "Epoch  74 / 100  Sammon Loss : 1082.593994140625  Regressor Loss : 102790.28125\n",
            "Val Loss : 101521.671875\n",
            "Batch 10 / 1132 Sammon Loss : 3530.30224609375\n",
            "Batch 20 / 1132 Sammon Loss : 3979.822265625\n",
            "Batch 30 / 1132 Sammon Loss : 3998.38232421875\n",
            "Batch 40 / 1132 Sammon Loss : 4184.173828125\n",
            "Batch 50 / 1132 Sammon Loss : 4354.0888671875\n",
            "Batch 60 / 1132 Sammon Loss : 2220.76220703125\n",
            "Batch 70 / 1132 Sammon Loss : 2269.169677734375\n",
            "Batch 80 / 1132 Sammon Loss : 1982.4505615234375\n",
            "Batch 90 / 1132 Sammon Loss : 1990.9061279296875\n",
            "Batch 100 / 1132 Sammon Loss : 1822.1376953125\n",
            "Batch 110 / 1132 Sammon Loss : 1416.2894287109375\n",
            "Batch 120 / 1132 Sammon Loss : 753.400146484375\n",
            "Batch 130 / 1132 Sammon Loss : 1421.823974609375\n",
            "Batch 140 / 1132 Sammon Loss : 1590.449462890625\n",
            "Batch 150 / 1132 Sammon Loss : 1810.9306640625\n",
            "Batch 160 / 1132 Sammon Loss : 1672.97265625\n",
            "Batch 170 / 1132 Sammon Loss : 3752.640625\n",
            "Batch 180 / 1132 Sammon Loss : 4268.58349609375\n",
            "Batch 190 / 1132 Sammon Loss : 3180.63134765625\n",
            "Batch 200 / 1132 Sammon Loss : 2977.42578125\n",
            "Batch 210 / 1132 Sammon Loss : 2772.498779296875\n",
            "Batch 220 / 1132 Sammon Loss : 2113.763916015625\n",
            "Batch 230 / 1132 Sammon Loss : 1843.572509765625\n",
            "Batch 240 / 1132 Sammon Loss : 1262.68603515625\n",
            "Batch 250 / 1132 Sammon Loss : 1176.4034423828125\n",
            "Batch 260 / 1132 Sammon Loss : 1420.442626953125\n",
            "Batch 270 / 1132 Sammon Loss : 1512.515625\n",
            "Batch 280 / 1132 Sammon Loss : 1191.6475830078125\n",
            "Batch 290 / 1132 Sammon Loss : 1268.782958984375\n",
            "Batch 300 / 1132 Sammon Loss : 2662.136474609375\n",
            "Batch 310 / 1132 Sammon Loss : 1302.9287109375\n",
            "Batch 320 / 1132 Sammon Loss : 1120.70556640625\n",
            "Batch 330 / 1132 Sammon Loss : 1498.68798828125\n",
            "Batch 340 / 1132 Sammon Loss : 1310.0771484375\n",
            "Batch 350 / 1132 Sammon Loss : 2167.6767578125\n",
            "Batch 360 / 1132 Sammon Loss : 2347.267578125\n",
            "Batch 370 / 1132 Sammon Loss : 2387.09375\n",
            "Batch 380 / 1132 Sammon Loss : 1943.0634765625\n",
            "Batch 390 / 1132 Sammon Loss : 2353.825927734375\n",
            "Batch 400 / 1132 Sammon Loss : 2804.70947265625\n",
            "Batch 410 / 1132 Sammon Loss : 2040.279541015625\n",
            "Batch 420 / 1132 Sammon Loss : 2048.276123046875\n",
            "Batch 430 / 1132 Sammon Loss : 2198.4423828125\n",
            "Batch 440 / 1132 Sammon Loss : 3051.42333984375\n",
            "Batch 450 / 1132 Sammon Loss : 1937.6085205078125\n",
            "Batch 460 / 1132 Sammon Loss : 2388.27392578125\n",
            "Batch 470 / 1132 Sammon Loss : 1904.9224853515625\n",
            "Batch 480 / 1132 Sammon Loss : 2505.024658203125\n",
            "Batch 490 / 1132 Sammon Loss : 1442.1004638671875\n",
            "Batch 500 / 1132 Sammon Loss : 5514.0048828125\n",
            "Batch 510 / 1132 Sammon Loss : 5359.83984375\n",
            "Batch 520 / 1132 Sammon Loss : 3736.6435546875\n",
            "Batch 530 / 1132 Sammon Loss : 4782.9765625\n",
            "Batch 540 / 1132 Sammon Loss : 1219.73046875\n",
            "Batch 550 / 1132 Sammon Loss : 1361.9471435546875\n",
            "Batch 560 / 1132 Sammon Loss : 1486.158203125\n",
            "Batch 570 / 1132 Sammon Loss : 1464.557373046875\n",
            "Batch 580 / 1132 Sammon Loss : 1669.6495361328125\n",
            "Batch 590 / 1132 Sammon Loss : 1806.486083984375\n",
            "Batch 600 / 1132 Sammon Loss : 1305.314208984375\n",
            "Batch 610 / 1132 Sammon Loss : 1145.2657470703125\n",
            "Batch 620 / 1132 Sammon Loss : 3146.84619140625\n",
            "Batch 630 / 1132 Sammon Loss : 3281.2822265625\n",
            "Batch 640 / 1132 Sammon Loss : 2957.24462890625\n",
            "Batch 650 / 1132 Sammon Loss : 3803.9521484375\n",
            "Batch 660 / 1132 Sammon Loss : 1639.24951171875\n",
            "Batch 670 / 1132 Sammon Loss : 1255.9525146484375\n",
            "Batch 680 / 1132 Sammon Loss : 1786.715087890625\n",
            "Batch 690 / 1132 Sammon Loss : 1220.14697265625\n",
            "Batch 700 / 1132 Sammon Loss : 1626.345947265625\n",
            "Batch 710 / 1132 Sammon Loss : 1174.812255859375\n",
            "Batch 720 / 1132 Sammon Loss : 1819.493408203125\n",
            "Batch 730 / 1132 Sammon Loss : 792.79541015625\n",
            "Batch 740 / 1132 Sammon Loss : 1494.2430419921875\n",
            "Batch 750 / 1132 Sammon Loss : 1163.54736328125\n",
            "Batch 760 / 1132 Sammon Loss : 1630.25048828125\n",
            "Batch 770 / 1132 Sammon Loss : 2419.6279296875\n",
            "Batch 780 / 1132 Sammon Loss : 2103.421875\n",
            "Batch 790 / 1132 Sammon Loss : 1377.699462890625\n",
            "Batch 800 / 1132 Sammon Loss : 2429.743408203125\n",
            "Batch 810 / 1132 Sammon Loss : 1143.398681640625\n",
            "Batch 820 / 1132 Sammon Loss : 1303.32080078125\n",
            "Batch 830 / 1132 Sammon Loss : 2245.152587890625\n",
            "Batch 840 / 1132 Sammon Loss : 1872.717041015625\n",
            "Batch 850 / 1132 Sammon Loss : 3679.35302734375\n",
            "Batch 860 / 1132 Sammon Loss : 3988.392578125\n",
            "Batch 870 / 1132 Sammon Loss : 4381.19921875\n",
            "Batch 880 / 1132 Sammon Loss : 1923.2099609375\n",
            "Batch 890 / 1132 Sammon Loss : 1647.28173828125\n",
            "Batch 900 / 1132 Sammon Loss : 1575.87255859375\n",
            "Batch 910 / 1132 Sammon Loss : 1891.35693359375\n",
            "Batch 920 / 1132 Sammon Loss : 1357.7493896484375\n",
            "Batch 930 / 1132 Sammon Loss : 2873.6953125\n",
            "Batch 940 / 1132 Sammon Loss : 3790.75537109375\n",
            "Batch 950 / 1132 Sammon Loss : 3170.27978515625\n",
            "Batch 960 / 1132 Sammon Loss : 2293.5078125\n",
            "Batch 970 / 1132 Sammon Loss : 1567.89794921875\n",
            "Batch 980 / 1132 Sammon Loss : 1321.909423828125\n",
            "Batch 990 / 1132 Sammon Loss : 1772.2060546875\n",
            "Batch 1000 / 1132 Sammon Loss : 1384.3818359375\n",
            "Batch 1010 / 1132 Sammon Loss : 1061.8603515625\n",
            "Batch 1020 / 1132 Sammon Loss : 2125.2021484375\n",
            "Batch 1030 / 1132 Sammon Loss : 1965.361328125\n",
            "Batch 1040 / 1132 Sammon Loss : 2274.8447265625\n",
            "Batch 1050 / 1132 Sammon Loss : 1596.8798828125\n",
            "Batch 1060 / 1132 Sammon Loss : 1478.4515380859375\n",
            "Batch 1070 / 1132 Sammon Loss : 3716.872314453125\n",
            "Batch 1080 / 1132 Sammon Loss : 2052.48291015625\n",
            "Batch 1090 / 1132 Sammon Loss : 1359.720458984375\n",
            "Batch 1100 / 1132 Sammon Loss : 4394.57421875\n",
            "Batch 1110 / 1132 Sammon Loss : 1476.479248046875\n",
            "Batch 1120 / 1132 Sammon Loss : 1825.775390625\n",
            "Batch 1130 / 1132 Sammon Loss : 1633.012451171875\n",
            "Batch 10 / 505 Regressor Loss : 106018.125\n",
            "Batch 20 / 505 Regressor Loss : 117151.609375\n",
            "Batch 30 / 505 Regressor Loss : 100172.1484375\n",
            "Batch 40 / 505 Regressor Loss : 101119.5234375\n",
            "Batch 50 / 505 Regressor Loss : 131885.125\n",
            "Batch 60 / 505 Regressor Loss : 126395.1484375\n",
            "Batch 70 / 505 Regressor Loss : 125997.421875\n",
            "Batch 80 / 505 Regressor Loss : 128023.8125\n",
            "Batch 90 / 505 Regressor Loss : 131417.78125\n",
            "Batch 100 / 505 Regressor Loss : 106108.078125\n",
            "Batch 110 / 505 Regressor Loss : 124186.2109375\n",
            "Batch 120 / 505 Regressor Loss : 113465.34375\n",
            "Batch 130 / 505 Regressor Loss : 127854.8046875\n",
            "Batch 140 / 505 Regressor Loss : 133873.625\n",
            "Batch 150 / 505 Regressor Loss : 118506.125\n",
            "Batch 160 / 505 Regressor Loss : 94432.859375\n",
            "Batch 170 / 505 Regressor Loss : 103222.8359375\n",
            "Batch 180 / 505 Regressor Loss : 132066.4375\n",
            "Batch 190 / 505 Regressor Loss : 126659.46875\n",
            "Batch 200 / 505 Regressor Loss : 103900.421875\n",
            "Batch 210 / 505 Regressor Loss : 111475.5625\n",
            "Batch 220 / 505 Regressor Loss : 124104.28125\n",
            "Batch 230 / 505 Regressor Loss : 116929.828125\n",
            "Batch 240 / 505 Regressor Loss : 108420.53125\n",
            "Batch 250 / 505 Regressor Loss : 149692.3125\n",
            "Batch 260 / 505 Regressor Loss : 126731.3046875\n",
            "Batch 270 / 505 Regressor Loss : 107404.484375\n",
            "Batch 280 / 505 Regressor Loss : 125100.453125\n",
            "Batch 290 / 505 Regressor Loss : 123917.0625\n",
            "Batch 300 / 505 Regressor Loss : 115780.609375\n",
            "Batch 310 / 505 Regressor Loss : 120015.1484375\n",
            "Batch 320 / 505 Regressor Loss : 146229.5\n",
            "Batch 330 / 505 Regressor Loss : 112941.40625\n",
            "Batch 340 / 505 Regressor Loss : 110091.859375\n",
            "Batch 350 / 505 Regressor Loss : 93335.53125\n",
            "Batch 360 / 505 Regressor Loss : 100590.109375\n",
            "Batch 370 / 505 Regressor Loss : 120224.03125\n",
            "Batch 380 / 505 Regressor Loss : 103924.8125\n",
            "Batch 390 / 505 Regressor Loss : 115830.984375\n",
            "Batch 400 / 505 Regressor Loss : 138969.734375\n",
            "Batch 410 / 505 Regressor Loss : 104876.234375\n",
            "Batch 420 / 505 Regressor Loss : 125763.125\n",
            "Batch 430 / 505 Regressor Loss : 110264.1484375\n",
            "Batch 440 / 505 Regressor Loss : 111714.171875\n",
            "Batch 450 / 505 Regressor Loss : 99641.921875\n",
            "Batch 460 / 505 Regressor Loss : 127475.59375\n",
            "Batch 470 / 505 Regressor Loss : 120011.9609375\n",
            "Batch 480 / 505 Regressor Loss : 120837.96875\n",
            "Batch 490 / 505 Regressor Loss : 120950.5625\n",
            "Batch 500 / 505 Regressor Loss : 118681.375\n",
            "Epoch  75 / 100  Sammon Loss : 1054.831787109375  Regressor Loss : 102589.203125\n",
            "Val Loss : 101267.8046875\n",
            "Batch 10 / 1132 Sammon Loss : 3388.509765625\n",
            "Batch 20 / 1132 Sammon Loss : 3829.203857421875\n",
            "Batch 30 / 1132 Sammon Loss : 3875.72314453125\n",
            "Batch 40 / 1132 Sammon Loss : 4020.2041015625\n",
            "Batch 50 / 1132 Sammon Loss : 4218.740234375\n",
            "Batch 60 / 1132 Sammon Loss : 2127.72900390625\n",
            "Batch 70 / 1132 Sammon Loss : 2161.506103515625\n",
            "Batch 80 / 1132 Sammon Loss : 1864.4998779296875\n",
            "Batch 90 / 1132 Sammon Loss : 1897.293701171875\n",
            "Batch 100 / 1132 Sammon Loss : 1728.3525390625\n",
            "Batch 110 / 1132 Sammon Loss : 1360.65625\n",
            "Batch 120 / 1132 Sammon Loss : 752.5953369140625\n",
            "Batch 130 / 1132 Sammon Loss : 1375.41259765625\n",
            "Batch 140 / 1132 Sammon Loss : 1553.958251953125\n",
            "Batch 150 / 1132 Sammon Loss : 1759.901611328125\n",
            "Batch 160 / 1132 Sammon Loss : 1629.603515625\n",
            "Batch 170 / 1132 Sammon Loss : 3615.136962890625\n",
            "Batch 180 / 1132 Sammon Loss : 4076.80859375\n",
            "Batch 190 / 1132 Sammon Loss : 3095.23046875\n",
            "Batch 200 / 1132 Sammon Loss : 2932.144775390625\n",
            "Batch 210 / 1132 Sammon Loss : 2745.65625\n",
            "Batch 220 / 1132 Sammon Loss : 2091.62548828125\n",
            "Batch 230 / 1132 Sammon Loss : 1874.291259765625\n",
            "Batch 240 / 1132 Sammon Loss : 1253.7972412109375\n",
            "Batch 250 / 1132 Sammon Loss : 1214.673095703125\n",
            "Batch 260 / 1132 Sammon Loss : 1411.712646484375\n",
            "Batch 270 / 1132 Sammon Loss : 1547.1875\n",
            "Batch 280 / 1132 Sammon Loss : 1231.295654296875\n",
            "Batch 290 / 1132 Sammon Loss : 1348.5877685546875\n",
            "Batch 300 / 1132 Sammon Loss : 2647.9580078125\n",
            "Batch 310 / 1132 Sammon Loss : 1321.7518310546875\n",
            "Batch 320 / 1132 Sammon Loss : 1181.9739990234375\n",
            "Batch 330 / 1132 Sammon Loss : 1586.11767578125\n",
            "Batch 340 / 1132 Sammon Loss : 1359.193603515625\n",
            "Batch 350 / 1132 Sammon Loss : 2221.64453125\n",
            "Batch 360 / 1132 Sammon Loss : 2314.00341796875\n",
            "Batch 370 / 1132 Sammon Loss : 2379.22705078125\n",
            "Batch 380 / 1132 Sammon Loss : 1915.261474609375\n",
            "Batch 390 / 1132 Sammon Loss : 2345.4423828125\n",
            "Batch 400 / 1132 Sammon Loss : 2745.256103515625\n",
            "Batch 410 / 1132 Sammon Loss : 2024.2608642578125\n",
            "Batch 420 / 1132 Sammon Loss : 2036.02099609375\n",
            "Batch 430 / 1132 Sammon Loss : 2182.265625\n",
            "Batch 440 / 1132 Sammon Loss : 3034.36376953125\n",
            "Batch 450 / 1132 Sammon Loss : 1965.20654296875\n",
            "Batch 460 / 1132 Sammon Loss : 2409.377685546875\n",
            "Batch 470 / 1132 Sammon Loss : 1935.60498046875\n",
            "Batch 480 / 1132 Sammon Loss : 2536.568115234375\n",
            "Batch 490 / 1132 Sammon Loss : 1510.82861328125\n",
            "Batch 500 / 1132 Sammon Loss : 5539.82177734375\n",
            "Batch 510 / 1132 Sammon Loss : 5409.98974609375\n",
            "Batch 520 / 1132 Sammon Loss : 3746.92529296875\n",
            "Batch 530 / 1132 Sammon Loss : 4824.6689453125\n",
            "Batch 540 / 1132 Sammon Loss : 1296.34521484375\n",
            "Batch 550 / 1132 Sammon Loss : 1429.540283203125\n",
            "Batch 560 / 1132 Sammon Loss : 1565.720703125\n",
            "Batch 570 / 1132 Sammon Loss : 1563.33837890625\n",
            "Batch 580 / 1132 Sammon Loss : 1757.968017578125\n",
            "Batch 590 / 1132 Sammon Loss : 1892.0496826171875\n",
            "Batch 600 / 1132 Sammon Loss : 1391.6669921875\n",
            "Batch 610 / 1132 Sammon Loss : 1249.0830078125\n",
            "Batch 620 / 1132 Sammon Loss : 3199.90380859375\n",
            "Batch 630 / 1132 Sammon Loss : 3374.832763671875\n",
            "Batch 640 / 1132 Sammon Loss : 3071.822021484375\n",
            "Batch 650 / 1132 Sammon Loss : 3853.49462890625\n",
            "Batch 660 / 1132 Sammon Loss : 1731.9202880859375\n",
            "Batch 670 / 1132 Sammon Loss : 1324.277099609375\n",
            "Batch 680 / 1132 Sammon Loss : 1858.644775390625\n",
            "Batch 690 / 1132 Sammon Loss : 1290.3486328125\n",
            "Batch 700 / 1132 Sammon Loss : 1752.4625244140625\n",
            "Batch 710 / 1132 Sammon Loss : 1289.16259765625\n",
            "Batch 720 / 1132 Sammon Loss : 1901.481689453125\n",
            "Batch 730 / 1132 Sammon Loss : 886.4828491210938\n",
            "Batch 740 / 1132 Sammon Loss : 1643.201171875\n",
            "Batch 750 / 1132 Sammon Loss : 1252.215576171875\n",
            "Batch 760 / 1132 Sammon Loss : 1736.488525390625\n",
            "Batch 770 / 1132 Sammon Loss : 2530.1181640625\n",
            "Batch 780 / 1132 Sammon Loss : 2220.638671875\n",
            "Batch 790 / 1132 Sammon Loss : 1497.6224365234375\n",
            "Batch 800 / 1132 Sammon Loss : 2549.080810546875\n",
            "Batch 810 / 1132 Sammon Loss : 1237.00048828125\n",
            "Batch 820 / 1132 Sammon Loss : 1423.9296875\n",
            "Batch 830 / 1132 Sammon Loss : 2332.86474609375\n",
            "Batch 840 / 1132 Sammon Loss : 2012.2152099609375\n",
            "Batch 850 / 1132 Sammon Loss : 3816.067138671875\n",
            "Batch 860 / 1132 Sammon Loss : 4104.8955078125\n",
            "Batch 870 / 1132 Sammon Loss : 4469.3291015625\n",
            "Batch 880 / 1132 Sammon Loss : 2052.6650390625\n",
            "Batch 890 / 1132 Sammon Loss : 1817.7950439453125\n",
            "Batch 900 / 1132 Sammon Loss : 1715.18359375\n",
            "Batch 910 / 1132 Sammon Loss : 1985.82568359375\n",
            "Batch 920 / 1132 Sammon Loss : 1463.135009765625\n",
            "Batch 930 / 1132 Sammon Loss : 3040.559814453125\n",
            "Batch 940 / 1132 Sammon Loss : 3980.262939453125\n",
            "Batch 950 / 1132 Sammon Loss : 3348.25048828125\n",
            "Batch 960 / 1132 Sammon Loss : 2468.62841796875\n",
            "Batch 970 / 1132 Sammon Loss : 1688.39013671875\n",
            "Batch 980 / 1132 Sammon Loss : 1427.079345703125\n",
            "Batch 990 / 1132 Sammon Loss : 1895.90966796875\n",
            "Batch 1000 / 1132 Sammon Loss : 1512.214111328125\n",
            "Batch 1010 / 1132 Sammon Loss : 1130.0673828125\n",
            "Batch 1020 / 1132 Sammon Loss : 2209.65234375\n",
            "Batch 1030 / 1132 Sammon Loss : 2103.337646484375\n",
            "Batch 1040 / 1132 Sammon Loss : 2458.13623046875\n",
            "Batch 1050 / 1132 Sammon Loss : 1713.75390625\n",
            "Batch 1060 / 1132 Sammon Loss : 1649.262451171875\n",
            "Batch 1070 / 1132 Sammon Loss : 3900.9052734375\n",
            "Batch 1080 / 1132 Sammon Loss : 2140.05078125\n",
            "Batch 1090 / 1132 Sammon Loss : 1471.594970703125\n",
            "Batch 1100 / 1132 Sammon Loss : 4573.96533203125\n",
            "Batch 1110 / 1132 Sammon Loss : 1609.769775390625\n",
            "Batch 1120 / 1132 Sammon Loss : 1917.0010986328125\n",
            "Batch 1130 / 1132 Sammon Loss : 1783.453857421875\n",
            "Batch 10 / 505 Regressor Loss : 105804.703125\n",
            "Batch 20 / 505 Regressor Loss : 116934.0859375\n",
            "Batch 30 / 505 Regressor Loss : 99953.640625\n",
            "Batch 40 / 505 Regressor Loss : 100898.21875\n",
            "Batch 50 / 505 Regressor Loss : 131617.625\n",
            "Batch 60 / 505 Regressor Loss : 126151.2421875\n",
            "Batch 70 / 505 Regressor Loss : 125762.28125\n",
            "Batch 80 / 505 Regressor Loss : 127769.1484375\n",
            "Batch 90 / 505 Regressor Loss : 131185.03125\n",
            "Batch 100 / 505 Regressor Loss : 105879.25\n",
            "Batch 110 / 505 Regressor Loss : 123958.296875\n",
            "Batch 120 / 505 Regressor Loss : 113255.375\n",
            "Batch 130 / 505 Regressor Loss : 127626.734375\n",
            "Batch 140 / 505 Regressor Loss : 133630.65625\n",
            "Batch 150 / 505 Regressor Loss : 118279.984375\n",
            "Batch 160 / 505 Regressor Loss : 94224.78125\n",
            "Batch 170 / 505 Regressor Loss : 103006.5\n",
            "Batch 180 / 505 Regressor Loss : 131823.421875\n",
            "Batch 190 / 505 Regressor Loss : 126406.484375\n",
            "Batch 200 / 505 Regressor Loss : 103699.3125\n",
            "Batch 210 / 505 Regressor Loss : 111250.0234375\n",
            "Batch 220 / 505 Regressor Loss : 123870.1875\n",
            "Batch 230 / 505 Regressor Loss : 116708.046875\n",
            "Batch 240 / 505 Regressor Loss : 108212.515625\n",
            "Batch 250 / 505 Regressor Loss : 149431.96875\n",
            "Batch 260 / 505 Regressor Loss : 126486.1171875\n",
            "Batch 270 / 505 Regressor Loss : 107192.859375\n",
            "Batch 280 / 505 Regressor Loss : 124863.359375\n",
            "Batch 290 / 505 Regressor Loss : 123695.90625\n",
            "Batch 300 / 505 Regressor Loss : 115547.609375\n",
            "Batch 310 / 505 Regressor Loss : 119786.0625\n",
            "Batch 320 / 505 Regressor Loss : 145966.578125\n",
            "Batch 330 / 505 Regressor Loss : 112709.4609375\n",
            "Batch 340 / 505 Regressor Loss : 109851.734375\n",
            "Batch 350 / 505 Regressor Loss : 93125.5859375\n",
            "Batch 360 / 505 Regressor Loss : 100393.5625\n",
            "Batch 370 / 505 Regressor Loss : 119986.921875\n",
            "Batch 380 / 505 Regressor Loss : 103713.140625\n",
            "Batch 390 / 505 Regressor Loss : 115606.0859375\n",
            "Batch 400 / 505 Regressor Loss : 138699.4375\n",
            "Batch 410 / 505 Regressor Loss : 104663.0859375\n",
            "Batch 420 / 505 Regressor Loss : 125531.0859375\n",
            "Batch 430 / 505 Regressor Loss : 110042.9921875\n",
            "Batch 440 / 505 Regressor Loss : 111486.5625\n",
            "Batch 450 / 505 Regressor Loss : 99440.5625\n",
            "Batch 460 / 505 Regressor Loss : 127240.875\n",
            "Batch 470 / 505 Regressor Loss : 119798.2734375\n",
            "Batch 480 / 505 Regressor Loss : 120611.65625\n",
            "Batch 490 / 505 Regressor Loss : 120717.640625\n",
            "Batch 500 / 505 Regressor Loss : 118464.859375\n",
            "Epoch  76 / 100  Sammon Loss : 1131.6119384765625  Regressor Loss : 102388.4921875\n",
            "Val Loss : 101014.3671875\n",
            "Batch 10 / 1132 Sammon Loss : 3498.0087890625\n",
            "Batch 20 / 1132 Sammon Loss : 3981.48046875\n",
            "Batch 30 / 1132 Sammon Loss : 4009.69677734375\n",
            "Batch 40 / 1132 Sammon Loss : 4145.2236328125\n",
            "Batch 50 / 1132 Sammon Loss : 4326.0263671875\n",
            "Batch 60 / 1132 Sammon Loss : 2238.734375\n",
            "Batch 70 / 1132 Sammon Loss : 2314.09033203125\n",
            "Batch 80 / 1132 Sammon Loss : 2018.6083984375\n",
            "Batch 90 / 1132 Sammon Loss : 2055.735107421875\n",
            "Batch 100 / 1132 Sammon Loss : 1846.199951171875\n",
            "Batch 110 / 1132 Sammon Loss : 1416.1878662109375\n",
            "Batch 120 / 1132 Sammon Loss : 780.6060791015625\n",
            "Batch 130 / 1132 Sammon Loss : 1448.634033203125\n",
            "Batch 140 / 1132 Sammon Loss : 1557.1015625\n",
            "Batch 150 / 1132 Sammon Loss : 1842.37548828125\n",
            "Batch 160 / 1132 Sammon Loss : 1735.710205078125\n",
            "Batch 170 / 1132 Sammon Loss : 3681.6259765625\n",
            "Batch 180 / 1132 Sammon Loss : 4196.5146484375\n",
            "Batch 190 / 1132 Sammon Loss : 3229.60498046875\n",
            "Batch 200 / 1132 Sammon Loss : 3034.693359375\n",
            "Batch 210 / 1132 Sammon Loss : 2797.322265625\n",
            "Batch 230 / 1132 Sammon Loss : 1913.70654296875\n",
            "Batch 240 / 1132 Sammon Loss : 1302.296142578125\n",
            "Batch 250 / 1132 Sammon Loss : 1243.7398681640625\n",
            "Batch 260 / 1132 Sammon Loss : 1450.649658203125\n",
            "Batch 270 / 1132 Sammon Loss : 1567.3582763671875\n",
            "Batch 280 / 1132 Sammon Loss : 1237.95361328125\n",
            "Batch 290 / 1132 Sammon Loss : 1338.5\n",
            "Batch 300 / 1132 Sammon Loss : 2719.48193359375\n",
            "Batch 310 / 1132 Sammon Loss : 1312.1204833984375\n",
            "Batch 320 / 1132 Sammon Loss : 1151.115478515625\n",
            "Batch 330 / 1132 Sammon Loss : 1533.160400390625\n",
            "Batch 340 / 1132 Sammon Loss : 1330.7728271484375\n",
            "Batch 350 / 1132 Sammon Loss : 2234.59326171875\n",
            "Batch 360 / 1132 Sammon Loss : 2395.3984375\n",
            "Batch 370 / 1132 Sammon Loss : 2433.31640625\n",
            "Batch 380 / 1132 Sammon Loss : 2000.63818359375\n",
            "Batch 390 / 1132 Sammon Loss : 2393.318359375\n",
            "Batch 400 / 1132 Sammon Loss : 2842.62451171875\n",
            "Batch 410 / 1132 Sammon Loss : 2067.658203125\n",
            "Batch 420 / 1132 Sammon Loss : 2080.635498046875\n",
            "Batch 430 / 1132 Sammon Loss : 2257.166259765625\n",
            "Batch 440 / 1132 Sammon Loss : 3102.3037109375\n",
            "Batch 450 / 1132 Sammon Loss : 2051.11767578125\n",
            "Batch 460 / 1132 Sammon Loss : 2463.513671875\n",
            "Batch 470 / 1132 Sammon Loss : 2020.247802734375\n",
            "Batch 480 / 1132 Sammon Loss : 2600.29052734375\n",
            "Batch 490 / 1132 Sammon Loss : 1576.1807861328125\n",
            "Batch 500 / 1132 Sammon Loss : 5583.14013671875\n",
            "Batch 510 / 1132 Sammon Loss : 5409.63427734375\n",
            "Batch 520 / 1132 Sammon Loss : 3813.26171875\n",
            "Batch 530 / 1132 Sammon Loss : 4869.8876953125\n",
            "Batch 540 / 1132 Sammon Loss : 1339.86767578125\n",
            "Batch 550 / 1132 Sammon Loss : 1496.04931640625\n",
            "Batch 560 / 1132 Sammon Loss : 1615.3800048828125\n",
            "Batch 570 / 1132 Sammon Loss : 1583.8565673828125\n",
            "Batch 580 / 1132 Sammon Loss : 1806.177490234375\n",
            "Batch 590 / 1132 Sammon Loss : 1903.5595703125\n",
            "Batch 600 / 1132 Sammon Loss : 1445.0103759765625\n",
            "Batch 610 / 1132 Sammon Loss : 1270.261962890625\n",
            "Batch 620 / 1132 Sammon Loss : 3261.48583984375\n",
            "Batch 630 / 1132 Sammon Loss : 3412.50634765625\n",
            "Batch 640 / 1132 Sammon Loss : 3070.40283203125\n",
            "Batch 650 / 1132 Sammon Loss : 3836.53759765625\n",
            "Batch 660 / 1132 Sammon Loss : 1769.71826171875\n",
            "Batch 670 / 1132 Sammon Loss : 1339.016357421875\n",
            "Batch 680 / 1132 Sammon Loss : 1903.9085693359375\n",
            "Batch 690 / 1132 Sammon Loss : 1315.7822265625\n",
            "Batch 700 / 1132 Sammon Loss : 1781.5\n",
            "Batch 710 / 1132 Sammon Loss : 1292.87890625\n",
            "Batch 720 / 1132 Sammon Loss : 1910.419921875\n",
            "Batch 730 / 1132 Sammon Loss : 946.02783203125\n",
            "Batch 740 / 1132 Sammon Loss : 1673.860595703125\n",
            "Batch 750 / 1132 Sammon Loss : 1300.69677734375\n",
            "Batch 760 / 1132 Sammon Loss : 1734.59423828125\n",
            "Batch 770 / 1132 Sammon Loss : 2548.2880859375\n",
            "Batch 780 / 1132 Sammon Loss : 2245.03271484375\n",
            "Batch 790 / 1132 Sammon Loss : 1496.336181640625\n",
            "Batch 800 / 1132 Sammon Loss : 2544.7109375\n",
            "Batch 810 / 1132 Sammon Loss : 1234.8563232421875\n",
            "Batch 820 / 1132 Sammon Loss : 1393.279541015625\n",
            "Batch 830 / 1132 Sammon Loss : 2334.485595703125\n",
            "Batch 840 / 1132 Sammon Loss : 1981.357421875\n",
            "Batch 850 / 1132 Sammon Loss : 3836.354248046875\n",
            "Batch 860 / 1132 Sammon Loss : 4137.61962890625\n",
            "Batch 870 / 1132 Sammon Loss : 4520.498046875\n",
            "Batch 880 / 1132 Sammon Loss : 2027.69873046875\n",
            "Batch 890 / 1132 Sammon Loss : 1776.9208984375\n",
            "Batch 900 / 1132 Sammon Loss : 1716.660888671875\n",
            "Batch 910 / 1132 Sammon Loss : 1968.6690673828125\n",
            "Batch 920 / 1132 Sammon Loss : 1421.422119140625\n",
            "Batch 930 / 1132 Sammon Loss : 3089.48876953125\n",
            "Batch 940 / 1132 Sammon Loss : 4012.705078125\n",
            "Batch 950 / 1132 Sammon Loss : 3402.670654296875\n",
            "Batch 960 / 1132 Sammon Loss : 2468.90234375\n",
            "Batch 970 / 1132 Sammon Loss : 1720.673095703125\n",
            "Batch 980 / 1132 Sammon Loss : 1414.024658203125\n",
            "Batch 990 / 1132 Sammon Loss : 1879.3302001953125\n",
            "Batch 1000 / 1132 Sammon Loss : 1479.603515625\n",
            "Batch 1010 / 1132 Sammon Loss : 1091.40283203125\n",
            "Batch 1020 / 1132 Sammon Loss : 2205.22607421875\n",
            "Batch 1030 / 1132 Sammon Loss : 2122.105224609375\n",
            "Batch 1040 / 1132 Sammon Loss : 2458.953857421875\n",
            "Batch 1050 / 1132 Sammon Loss : 1690.2218017578125\n",
            "Batch 1060 / 1132 Sammon Loss : 1595.7047119140625\n",
            "Batch 1070 / 1132 Sammon Loss : 3901.60546875\n",
            "Batch 1080 / 1132 Sammon Loss : 2226.472900390625\n",
            "Batch 1090 / 1132 Sammon Loss : 1472.4964599609375\n",
            "Batch 1100 / 1132 Sammon Loss : 4623.40869140625\n",
            "Batch 1110 / 1132 Sammon Loss : 1579.097412109375\n",
            "Batch 1120 / 1132 Sammon Loss : 1903.7513427734375\n",
            "Batch 1130 / 1132 Sammon Loss : 1768.89892578125\n",
            "Batch 10 / 505 Regressor Loss : 105591.6484375\n",
            "Batch 20 / 505 Regressor Loss : 116716.9296875\n",
            "Batch 30 / 505 Regressor Loss : 99735.5234375\n",
            "Batch 40 / 505 Regressor Loss : 100677.3125\n",
            "Batch 50 / 505 Regressor Loss : 131350.5625\n",
            "Batch 60 / 505 Regressor Loss : 125907.75\n",
            "Batch 70 / 505 Regressor Loss : 125527.546875\n",
            "Batch 80 / 505 Regressor Loss : 127514.921875\n",
            "Batch 90 / 505 Regressor Loss : 130952.6484375\n",
            "Batch 100 / 505 Regressor Loss : 105650.8359375\n",
            "Batch 110 / 505 Regressor Loss : 123730.7734375\n",
            "Batch 120 / 505 Regressor Loss : 113045.8046875\n",
            "Batch 130 / 505 Regressor Loss : 127399.046875\n",
            "Batch 140 / 505 Regressor Loss : 133388.109375\n",
            "Batch 150 / 505 Regressor Loss : 118054.234375\n",
            "Batch 160 / 505 Regressor Loss : 94017.0859375\n",
            "Batch 170 / 505 Regressor Loss : 102790.5859375\n",
            "Batch 180 / 505 Regressor Loss : 131580.828125\n",
            "Batch 190 / 505 Regressor Loss : 126153.90625\n",
            "Batch 200 / 505 Regressor Loss : 103498.59375\n",
            "Batch 210 / 505 Regressor Loss : 111024.875\n",
            "Batch 220 / 505 Regressor Loss : 123636.5546875\n",
            "Batch 230 / 505 Regressor Loss : 116486.671875\n",
            "Batch 240 / 505 Regressor Loss : 108004.8671875\n",
            "Batch 250 / 505 Regressor Loss : 149172.03125\n",
            "Batch 260 / 505 Regressor Loss : 126241.375\n",
            "Batch 270 / 505 Regressor Loss : 106981.625\n",
            "Batch 280 / 505 Regressor Loss : 124626.671875\n",
            "Batch 290 / 505 Regressor Loss : 123475.125\n",
            "Batch 300 / 505 Regressor Loss : 115315.0234375\n",
            "Batch 310 / 505 Regressor Loss : 119557.375\n",
            "Batch 320 / 505 Regressor Loss : 145704.09375\n",
            "Batch 330 / 505 Regressor Loss : 112477.921875\n",
            "Batch 340 / 505 Regressor Loss : 109612.0234375\n",
            "Batch 350 / 505 Regressor Loss : 92916.0234375\n",
            "Batch 360 / 505 Regressor Loss : 100197.40625\n",
            "Batch 370 / 505 Regressor Loss : 119750.2109375\n",
            "Batch 380 / 505 Regressor Loss : 103501.875\n",
            "Batch 390 / 505 Regressor Loss : 115381.609375\n",
            "Batch 400 / 505 Regressor Loss : 138429.609375\n",
            "Batch 410 / 505 Regressor Loss : 104450.359375\n",
            "Batch 420 / 505 Regressor Loss : 125299.4375\n",
            "Batch 430 / 505 Regressor Loss : 109822.234375\n",
            "Batch 440 / 505 Regressor Loss : 111259.359375\n",
            "Batch 450 / 505 Regressor Loss : 99239.5859375\n",
            "Batch 460 / 505 Regressor Loss : 127006.5625\n",
            "Batch 470 / 505 Regressor Loss : 119584.984375\n",
            "Batch 480 / 505 Regressor Loss : 120385.75\n",
            "Batch 490 / 505 Regressor Loss : 120485.109375\n",
            "Batch 500 / 505 Regressor Loss : 118248.75\n",
            "Epoch  77 / 100  Sammon Loss : 1091.19580078125  Regressor Loss : 102188.1796875\n",
            "Val Loss : 100761.328125\n",
            "Batch 10 / 1132 Sammon Loss : 3546.03466796875\n",
            "Batch 20 / 1132 Sammon Loss : 4038.6767578125\n",
            "Batch 30 / 1132 Sammon Loss : 4040.86767578125\n",
            "Batch 40 / 1132 Sammon Loss : 4190.892578125\n",
            "Batch 50 / 1132 Sammon Loss : 4350.1865234375\n",
            "Batch 60 / 1132 Sammon Loss : 2219.41015625\n",
            "Batch 70 / 1132 Sammon Loss : 2279.29638671875\n",
            "Batch 80 / 1132 Sammon Loss : 1982.87744140625\n",
            "Batch 90 / 1132 Sammon Loss : 2048.2265625\n",
            "Batch 100 / 1132 Sammon Loss : 1806.1236572265625\n",
            "Batch 110 / 1132 Sammon Loss : 1410.0677490234375\n",
            "Batch 120 / 1132 Sammon Loss : 740.769775390625\n",
            "Batch 130 / 1132 Sammon Loss : 1412.556640625\n",
            "Batch 140 / 1132 Sammon Loss : 1553.4139404296875\n",
            "Batch 150 / 1132 Sammon Loss : 1794.65869140625\n",
            "Batch 160 / 1132 Sammon Loss : 1670.5341796875\n",
            "Batch 170 / 1132 Sammon Loss : 3769.068359375\n",
            "Batch 180 / 1132 Sammon Loss : 4319.3447265625\n",
            "Batch 190 / 1132 Sammon Loss : 3266.741943359375\n",
            "Batch 200 / 1132 Sammon Loss : 3039.364013671875\n",
            "Batch 210 / 1132 Sammon Loss : 2811.890625\n",
            "Batch 220 / 1132 Sammon Loss : 2150.4404296875\n",
            "Batch 230 / 1132 Sammon Loss : 1876.38720703125\n",
            "Batch 240 / 1132 Sammon Loss : 1291.914794921875\n",
            "Batch 250 / 1132 Sammon Loss : 1212.5279541015625\n",
            "Batch 260 / 1132 Sammon Loss : 1423.4937744140625\n",
            "Batch 270 / 1132 Sammon Loss : 1578.912353515625\n",
            "Batch 280 / 1132 Sammon Loss : 1250.4814453125\n",
            "Batch 290 / 1132 Sammon Loss : 1321.24951171875\n",
            "Batch 300 / 1132 Sammon Loss : 2722.046142578125\n",
            "Batch 310 / 1132 Sammon Loss : 1322.5518798828125\n",
            "Batch 320 / 1132 Sammon Loss : 1118.104248046875\n",
            "Batch 330 / 1132 Sammon Loss : 1524.01171875\n",
            "Batch 340 / 1132 Sammon Loss : 1311.6484375\n",
            "Batch 350 / 1132 Sammon Loss : 2225.947998046875\n",
            "Batch 360 / 1132 Sammon Loss : 2468.87353515625\n",
            "Batch 370 / 1132 Sammon Loss : 2479.22607421875\n",
            "Batch 380 / 1132 Sammon Loss : 2014.4036865234375\n",
            "Batch 390 / 1132 Sammon Loss : 2418.7568359375\n",
            "Batch 400 / 1132 Sammon Loss : 2918.5078125\n",
            "Batch 410 / 1132 Sammon Loss : 2094.780517578125\n",
            "Batch 420 / 1132 Sammon Loss : 2099.4658203125\n",
            "Batch 430 / 1132 Sammon Loss : 2313.5537109375\n",
            "Batch 440 / 1132 Sammon Loss : 3116.927734375\n",
            "Batch 450 / 1132 Sammon Loss : 2028.031982421875\n",
            "Batch 460 / 1132 Sammon Loss : 2483.75146484375\n",
            "Batch 470 / 1132 Sammon Loss : 2009.854736328125\n",
            "Batch 480 / 1132 Sammon Loss : 2591.33349609375\n",
            "Batch 490 / 1132 Sammon Loss : 1539.83935546875\n",
            "Batch 500 / 1132 Sammon Loss : 5630.1767578125\n",
            "Batch 510 / 1132 Sammon Loss : 5482.623046875\n",
            "Batch 520 / 1132 Sammon Loss : 3843.12646484375\n",
            "Batch 530 / 1132 Sammon Loss : 4932.3564453125\n",
            "Batch 540 / 1132 Sammon Loss : 1341.535888671875\n",
            "Batch 550 / 1132 Sammon Loss : 1480.745849609375\n",
            "Batch 560 / 1132 Sammon Loss : 1616.73095703125\n",
            "Batch 570 / 1132 Sammon Loss : 1564.34375\n",
            "Batch 580 / 1132 Sammon Loss : 1771.2015380859375\n",
            "Batch 590 / 1132 Sammon Loss : 1888.5894775390625\n",
            "Batch 600 / 1132 Sammon Loss : 1436.9378662109375\n",
            "Batch 610 / 1132 Sammon Loss : 1265.8349609375\n",
            "Batch 620 / 1132 Sammon Loss : 3342.064453125\n",
            "Batch 630 / 1132 Sammon Loss : 3504.326416015625\n",
            "Batch 640 / 1132 Sammon Loss : 3155.448486328125\n",
            "Batch 650 / 1132 Sammon Loss : 3982.93798828125\n",
            "Batch 660 / 1132 Sammon Loss : 1782.206787109375\n",
            "Batch 670 / 1132 Sammon Loss : 1340.4345703125\n",
            "Batch 680 / 1132 Sammon Loss : 1914.584716796875\n",
            "Batch 690 / 1132 Sammon Loss : 1317.0753173828125\n",
            "Batch 700 / 1132 Sammon Loss : 1809.595458984375\n",
            "Batch 710 / 1132 Sammon Loss : 1254.308349609375\n",
            "Batch 720 / 1132 Sammon Loss : 1951.2763671875\n",
            "Batch 730 / 1132 Sammon Loss : 875.3089599609375\n",
            "Batch 740 / 1132 Sammon Loss : 1570.122802734375\n",
            "Batch 750 / 1132 Sammon Loss : 1198.190185546875\n",
            "Batch 760 / 1132 Sammon Loss : 1780.7109375\n",
            "Batch 770 / 1132 Sammon Loss : 2560.30126953125\n",
            "Batch 780 / 1132 Sammon Loss : 2249.9716796875\n",
            "Batch 790 / 1132 Sammon Loss : 1464.8798828125\n",
            "Batch 800 / 1132 Sammon Loss : 2500.341552734375\n",
            "Batch 810 / 1132 Sammon Loss : 1189.4501953125\n",
            "Batch 820 / 1132 Sammon Loss : 1330.549560546875\n",
            "Batch 830 / 1132 Sammon Loss : 2310.1455078125\n",
            "Batch 840 / 1132 Sammon Loss : 1933.8035888671875\n",
            "Batch 850 / 1132 Sammon Loss : 3837.412109375\n",
            "Batch 860 / 1132 Sammon Loss : 4128.826171875\n",
            "Batch 870 / 1132 Sammon Loss : 4500.7705078125\n",
            "Batch 880 / 1132 Sammon Loss : 1992.07666015625\n",
            "Batch 890 / 1132 Sammon Loss : 1720.49853515625\n",
            "Batch 900 / 1132 Sammon Loss : 1654.794921875\n",
            "Batch 910 / 1132 Sammon Loss : 1949.526123046875\n",
            "Batch 920 / 1132 Sammon Loss : 1383.5836181640625\n",
            "Batch 930 / 1132 Sammon Loss : 3065.517333984375\n",
            "Batch 940 / 1132 Sammon Loss : 3944.419189453125\n",
            "Batch 950 / 1132 Sammon Loss : 3321.4501953125\n",
            "Batch 960 / 1132 Sammon Loss : 2437.76708984375\n",
            "Batch 970 / 1132 Sammon Loss : 1676.7208251953125\n",
            "Batch 980 / 1132 Sammon Loss : 1379.229248046875\n",
            "Batch 990 / 1132 Sammon Loss : 1865.6075439453125\n",
            "Batch 1000 / 1132 Sammon Loss : 1481.890380859375\n",
            "Batch 1010 / 1132 Sammon Loss : 1121.521484375\n",
            "Batch 1020 / 1132 Sammon Loss : 2187.1943359375\n",
            "Batch 1030 / 1132 Sammon Loss : 2071.028564453125\n",
            "Batch 1040 / 1132 Sammon Loss : 2406.92041015625\n",
            "Batch 1050 / 1132 Sammon Loss : 1684.814208984375\n",
            "Batch 1060 / 1132 Sammon Loss : 1608.467529296875\n",
            "Batch 1070 / 1132 Sammon Loss : 3849.70556640625\n",
            "Batch 1080 / 1132 Sammon Loss : 2169.40087890625\n",
            "Batch 1090 / 1132 Sammon Loss : 1480.745361328125\n",
            "Batch 1100 / 1132 Sammon Loss : 4617.5439453125\n",
            "Batch 1110 / 1132 Sammon Loss : 1578.7215576171875\n",
            "Batch 1120 / 1132 Sammon Loss : 1877.17578125\n",
            "Batch 1130 / 1132 Sammon Loss : 1712.237060546875\n",
            "Batch 10 / 505 Regressor Loss : 105379.0\n",
            "Batch 20 / 505 Regressor Loss : 116500.203125\n",
            "Batch 30 / 505 Regressor Loss : 99517.8359375\n",
            "Batch 40 / 505 Regressor Loss : 100456.8125\n",
            "Batch 50 / 505 Regressor Loss : 131083.921875\n",
            "Batch 60 / 505 Regressor Loss : 125664.671875\n",
            "Batch 70 / 505 Regressor Loss : 125293.2109375\n",
            "Batch 80 / 505 Regressor Loss : 127261.125\n",
            "Batch 90 / 505 Regressor Loss : 130720.6796875\n",
            "Batch 100 / 505 Regressor Loss : 105422.8125\n",
            "Batch 110 / 505 Regressor Loss : 123503.625\n",
            "Batch 120 / 505 Regressor Loss : 112836.6171875\n",
            "Batch 130 / 505 Regressor Loss : 127171.734375\n",
            "Batch 140 / 505 Regressor Loss : 133145.96875\n",
            "Batch 150 / 505 Regressor Loss : 117828.875\n",
            "Batch 160 / 505 Regressor Loss : 93809.8125\n",
            "Batch 170 / 505 Regressor Loss : 102575.0625\n",
            "Batch 180 / 505 Regressor Loss : 131338.625\n",
            "Batch 190 / 505 Regressor Loss : 125901.765625\n",
            "Batch 200 / 505 Regressor Loss : 103298.265625\n",
            "Batch 210 / 505 Regressor Loss : 110800.1484375\n",
            "Batch 220 / 505 Regressor Loss : 123403.3125\n",
            "Batch 230 / 505 Regressor Loss : 116265.6796875\n",
            "Batch 240 / 505 Regressor Loss : 107797.625\n",
            "Batch 250 / 505 Regressor Loss : 148912.53125\n",
            "Batch 260 / 505 Regressor Loss : 125997.0859375\n",
            "Batch 270 / 505 Regressor Loss : 106770.796875\n",
            "Batch 280 / 505 Regressor Loss : 124390.390625\n",
            "Batch 290 / 505 Regressor Loss : 123254.75\n",
            "Batch 300 / 505 Regressor Loss : 115082.8359375\n",
            "Batch 310 / 505 Regressor Loss : 119329.0859375\n",
            "Batch 320 / 505 Regressor Loss : 145442.046875\n",
            "Batch 330 / 505 Regressor Loss : 112246.7734375\n",
            "Batch 340 / 505 Regressor Loss : 109372.734375\n",
            "Batch 350 / 505 Regressor Loss : 92706.8671875\n",
            "Batch 360 / 505 Regressor Loss : 100001.640625\n",
            "Batch 370 / 505 Regressor Loss : 119513.9375\n",
            "Batch 380 / 505 Regressor Loss : 103291.015625\n",
            "Batch 390 / 505 Regressor Loss : 115157.5\n",
            "Batch 400 / 505 Regressor Loss : 138160.21875\n",
            "Batch 410 / 505 Regressor Loss : 104238.046875\n",
            "Batch 420 / 505 Regressor Loss : 125068.1875\n",
            "Batch 430 / 505 Regressor Loss : 109601.859375\n",
            "Batch 440 / 505 Regressor Loss : 111032.5546875\n",
            "Batch 450 / 505 Regressor Loss : 99039.015625\n",
            "Batch 460 / 505 Regressor Loss : 126772.6484375\n",
            "Batch 470 / 505 Regressor Loss : 119372.046875\n",
            "Batch 480 / 505 Regressor Loss : 120160.234375\n",
            "Batch 490 / 505 Regressor Loss : 120253.0\n",
            "Batch 500 / 505 Regressor Loss : 118033.046875\n",
            "Epoch  78 / 100  Sammon Loss : 1072.76123046875  Regressor Loss : 101988.25\n",
            "Val Loss : 100508.7109375\n",
            "Batch 10 / 1132 Sammon Loss : 3534.302734375\n",
            "Batch 20 / 1132 Sammon Loss : 3990.22802734375\n",
            "Batch 30 / 1132 Sammon Loss : 4008.334716796875\n",
            "Batch 40 / 1132 Sammon Loss : 4167.11962890625\n",
            "Batch 50 / 1132 Sammon Loss : 4302.04345703125\n",
            "Batch 60 / 1132 Sammon Loss : 2195.303466796875\n",
            "Batch 70 / 1132 Sammon Loss : 2256.85595703125\n",
            "Batch 80 / 1132 Sammon Loss : 1938.474365234375\n",
            "Batch 90 / 1132 Sammon Loss : 1999.43994140625\n",
            "Batch 100 / 1132 Sammon Loss : 1800.0888671875\n",
            "Batch 110 / 1132 Sammon Loss : 1410.67041015625\n",
            "Batch 120 / 1132 Sammon Loss : 772.4061279296875\n",
            "Batch 130 / 1132 Sammon Loss : 1413.65673828125\n",
            "Batch 140 / 1132 Sammon Loss : 1572.2091064453125\n",
            "Batch 150 / 1132 Sammon Loss : 1777.6629638671875\n",
            "Batch 160 / 1132 Sammon Loss : 1639.267822265625\n",
            "Batch 170 / 1132 Sammon Loss : 3720.115478515625\n",
            "Batch 180 / 1132 Sammon Loss : 4190.2138671875\n",
            "Batch 190 / 1132 Sammon Loss : 3181.573974609375\n",
            "Batch 200 / 1132 Sammon Loss : 2983.45849609375\n",
            "Batch 210 / 1132 Sammon Loss : 2797.7197265625\n",
            "Batch 220 / 1132 Sammon Loss : 2122.133056640625\n",
            "Batch 230 / 1132 Sammon Loss : 1854.795166015625\n",
            "Batch 240 / 1132 Sammon Loss : 1280.64453125\n",
            "Batch 250 / 1132 Sammon Loss : 1198.264892578125\n",
            "Batch 260 / 1132 Sammon Loss : 1417.6004638671875\n",
            "Batch 270 / 1132 Sammon Loss : 1515.788330078125\n",
            "Batch 280 / 1132 Sammon Loss : 1212.7578125\n",
            "Batch 290 / 1132 Sammon Loss : 1297.51123046875\n",
            "Batch 300 / 1132 Sammon Loss : 2676.24951171875\n",
            "Batch 310 / 1132 Sammon Loss : 1302.9254150390625\n",
            "Batch 320 / 1132 Sammon Loss : 1128.553466796875\n",
            "Batch 330 / 1132 Sammon Loss : 1516.23193359375\n",
            "Batch 340 / 1132 Sammon Loss : 1318.94677734375\n",
            "Batch 350 / 1132 Sammon Loss : 2229.81396484375\n",
            "Batch 360 / 1132 Sammon Loss : 2341.9814453125\n",
            "Batch 370 / 1132 Sammon Loss : 2398.56494140625\n",
            "Batch 380 / 1132 Sammon Loss : 1955.433349609375\n",
            "Batch 390 / 1132 Sammon Loss : 2377.295654296875\n",
            "Batch 400 / 1132 Sammon Loss : 2800.0869140625\n",
            "Batch 410 / 1132 Sammon Loss : 2049.43994140625\n",
            "Batch 420 / 1132 Sammon Loss : 2051.680908203125\n",
            "Batch 430 / 1132 Sammon Loss : 2248.1435546875\n",
            "Batch 440 / 1132 Sammon Loss : 3053.98486328125\n",
            "Batch 450 / 1132 Sammon Loss : 1981.016845703125\n",
            "Batch 460 / 1132 Sammon Loss : 2438.027099609375\n",
            "Batch 470 / 1132 Sammon Loss : 1975.640625\n",
            "Batch 480 / 1132 Sammon Loss : 2555.570556640625\n",
            "Batch 490 / 1132 Sammon Loss : 1523.397705078125\n",
            "Batch 500 / 1132 Sammon Loss : 5590.958984375\n",
            "Batch 510 / 1132 Sammon Loss : 5407.41845703125\n",
            "Batch 520 / 1132 Sammon Loss : 3777.654052734375\n",
            "Batch 530 / 1132 Sammon Loss : 4845.0537109375\n",
            "Batch 540 / 1132 Sammon Loss : 1342.112060546875\n",
            "Batch 550 / 1132 Sammon Loss : 1477.5472412109375\n",
            "Batch 560 / 1132 Sammon Loss : 1583.3717041015625\n",
            "Batch 570 / 1132 Sammon Loss : 1551.61083984375\n",
            "Batch 580 / 1132 Sammon Loss : 1747.923828125\n",
            "Batch 590 / 1132 Sammon Loss : 1870.1046142578125\n",
            "Batch 600 / 1132 Sammon Loss : 1410.25634765625\n",
            "Batch 610 / 1132 Sammon Loss : 1244.46044921875\n",
            "Batch 620 / 1132 Sammon Loss : 3247.011962890625\n",
            "Batch 630 / 1132 Sammon Loss : 3422.12109375\n",
            "Batch 640 / 1132 Sammon Loss : 3080.58837890625\n",
            "Batch 650 / 1132 Sammon Loss : 3887.455078125\n",
            "Batch 660 / 1132 Sammon Loss : 1740.6153564453125\n",
            "Batch 670 / 1132 Sammon Loss : 1318.8426513671875\n",
            "Batch 680 / 1132 Sammon Loss : 1880.707763671875\n",
            "Batch 690 / 1132 Sammon Loss : 1303.03564453125\n",
            "Batch 700 / 1132 Sammon Loss : 1765.708251953125\n",
            "Batch 710 / 1132 Sammon Loss : 1263.271240234375\n",
            "Batch 720 / 1132 Sammon Loss : 1903.6710205078125\n",
            "Batch 730 / 1132 Sammon Loss : 898.9090576171875\n",
            "Batch 740 / 1132 Sammon Loss : 1595.32470703125\n",
            "Batch 750 / 1132 Sammon Loss : 1243.9034423828125\n",
            "Batch 760 / 1132 Sammon Loss : 1706.284912109375\n",
            "Batch 770 / 1132 Sammon Loss : 2511.30615234375\n",
            "Batch 780 / 1132 Sammon Loss : 2223.12744140625\n",
            "Batch 790 / 1132 Sammon Loss : 1447.716552734375\n",
            "Batch 800 / 1132 Sammon Loss : 2512.33642578125\n",
            "Batch 810 / 1132 Sammon Loss : 1208.888427734375\n",
            "Batch 820 / 1132 Sammon Loss : 1370.3359375\n",
            "Batch 830 / 1132 Sammon Loss : 2338.9169921875\n",
            "Batch 840 / 1132 Sammon Loss : 1976.291015625\n",
            "Batch 850 / 1132 Sammon Loss : 3867.4296875\n",
            "Batch 860 / 1132 Sammon Loss : 4144.7138671875\n",
            "Batch 870 / 1132 Sammon Loss : 4508.96044921875\n",
            "Batch 880 / 1132 Sammon Loss : 2003.504638671875\n",
            "Batch 890 / 1132 Sammon Loss : 1774.30126953125\n",
            "Batch 900 / 1132 Sammon Loss : 1720.77392578125\n",
            "Batch 910 / 1132 Sammon Loss : 1951.364013671875\n",
            "Batch 920 / 1132 Sammon Loss : 1452.289794921875\n",
            "Batch 930 / 1132 Sammon Loss : 3114.3818359375\n",
            "Batch 940 / 1132 Sammon Loss : 3992.253173828125\n",
            "Batch 950 / 1132 Sammon Loss : 3385.05517578125\n",
            "Batch 960 / 1132 Sammon Loss : 2452.54541015625\n",
            "Batch 970 / 1132 Sammon Loss : 1712.4493408203125\n",
            "Batch 980 / 1132 Sammon Loss : 1386.396484375\n",
            "Batch 990 / 1132 Sammon Loss : 1897.508544921875\n",
            "Batch 1000 / 1132 Sammon Loss : 1478.421630859375\n",
            "Batch 1010 / 1132 Sammon Loss : 1162.3057861328125\n",
            "Batch 1020 / 1132 Sammon Loss : 2225.910888671875\n",
            "Batch 1030 / 1132 Sammon Loss : 2102.696533203125\n",
            "Batch 1040 / 1132 Sammon Loss : 2442.6474609375\n",
            "Batch 1050 / 1132 Sammon Loss : 1698.342041015625\n",
            "Batch 1060 / 1132 Sammon Loss : 1613.7685546875\n",
            "Batch 1070 / 1132 Sammon Loss : 3852.0771484375\n",
            "Batch 1080 / 1132 Sammon Loss : 2174.677978515625\n",
            "Batch 1090 / 1132 Sammon Loss : 1488.8724365234375\n",
            "Batch 1100 / 1132 Sammon Loss : 4560.17724609375\n",
            "Batch 1110 / 1132 Sammon Loss : 1589.601318359375\n",
            "Batch 1120 / 1132 Sammon Loss : 1910.502197265625\n",
            "Batch 1130 / 1132 Sammon Loss : 1731.2391357421875\n",
            "Batch 10 / 505 Regressor Loss : 105166.734375\n",
            "Batch 20 / 505 Regressor Loss : 116283.8359375\n",
            "Batch 30 / 505 Regressor Loss : 99300.546875\n",
            "Batch 40 / 505 Regressor Loss : 100236.7109375\n",
            "Batch 50 / 505 Regressor Loss : 130817.7109375\n",
            "Batch 60 / 505 Regressor Loss : 125422.0\n",
            "Batch 70 / 505 Regressor Loss : 125059.265625\n",
            "Batch 80 / 505 Regressor Loss : 127007.796875\n",
            "Batch 90 / 505 Regressor Loss : 130489.109375\n",
            "Batch 100 / 505 Regressor Loss : 105195.1875\n",
            "Batch 110 / 505 Regressor Loss : 123276.859375\n",
            "Batch 120 / 505 Regressor Loss : 112627.8125\n",
            "Batch 130 / 505 Regressor Loss : 126944.796875\n",
            "Batch 140 / 505 Regressor Loss : 132904.21875\n",
            "Batch 150 / 505 Regressor Loss : 117603.921875\n",
            "Batch 160 / 505 Regressor Loss : 93602.9375\n",
            "Batch 170 / 505 Regressor Loss : 102359.9375\n",
            "Batch 180 / 505 Regressor Loss : 131096.84375\n",
            "Batch 190 / 505 Regressor Loss : 125650.0234375\n",
            "Batch 200 / 505 Regressor Loss : 103098.296875\n",
            "Batch 210 / 505 Regressor Loss : 110575.828125\n",
            "Batch 220 / 505 Regressor Loss : 123170.4921875\n",
            "Batch 230 / 505 Regressor Loss : 116045.0859375\n",
            "Batch 240 / 505 Regressor Loss : 107590.75\n",
            "Batch 250 / 505 Regressor Loss : 148653.40625\n",
            "Batch 260 / 505 Regressor Loss : 125753.1875\n",
            "Batch 270 / 505 Regressor Loss : 106560.3671875\n",
            "Batch 280 / 505 Regressor Loss : 124154.5234375\n",
            "Batch 290 / 505 Regressor Loss : 123034.7734375\n",
            "Batch 300 / 505 Regressor Loss : 114851.046875\n",
            "Batch 310 / 505 Regressor Loss : 119101.203125\n",
            "Batch 320 / 505 Regressor Loss : 145180.40625\n",
            "Batch 330 / 505 Regressor Loss : 112016.0234375\n",
            "Batch 340 / 505 Regressor Loss : 109133.84375\n",
            "Batch 350 / 505 Regressor Loss : 92498.125\n",
            "Batch 360 / 505 Regressor Loss : 99806.25\n",
            "Batch 370 / 505 Regressor Loss : 119278.0625\n",
            "Batch 380 / 505 Regressor Loss : 103080.546875\n",
            "Batch 390 / 505 Regressor Loss : 114933.796875\n",
            "Batch 400 / 505 Regressor Loss : 137891.25\n",
            "Batch 410 / 505 Regressor Loss : 104026.125\n",
            "Batch 420 / 505 Regressor Loss : 124837.3359375\n",
            "Batch 430 / 505 Regressor Loss : 109381.875\n",
            "Batch 440 / 505 Regressor Loss : 110806.15625\n",
            "Batch 450 / 505 Regressor Loss : 98838.8125\n",
            "Batch 460 / 505 Regressor Loss : 126539.140625\n",
            "Batch 470 / 505 Regressor Loss : 119159.4921875\n",
            "Batch 480 / 505 Regressor Loss : 119935.1171875\n",
            "Batch 490 / 505 Regressor Loss : 120021.28125\n",
            "Batch 500 / 505 Regressor Loss : 117817.734375\n",
            "Epoch  79 / 100  Sammon Loss : 1122.9462890625  Regressor Loss : 101788.7109375\n",
            "Val Loss : 100256.5\n",
            "Batch 10 / 1132 Sammon Loss : 3507.703369140625\n",
            "Batch 20 / 1132 Sammon Loss : 3964.9228515625\n",
            "Batch 30 / 1132 Sammon Loss : 3970.531982421875\n",
            "Batch 40 / 1132 Sammon Loss : 4127.6572265625\n",
            "Batch 50 / 1132 Sammon Loss : 4311.1142578125\n",
            "Batch 60 / 1132 Sammon Loss : 2211.7333984375\n",
            "Batch 70 / 1132 Sammon Loss : 2303.795654296875\n",
            "Batch 80 / 1132 Sammon Loss : 1975.557373046875\n",
            "Batch 90 / 1132 Sammon Loss : 2032.14892578125\n",
            "Batch 100 / 1132 Sammon Loss : 1840.6639404296875\n",
            "Batch 110 / 1132 Sammon Loss : 1425.84033203125\n",
            "Batch 120 / 1132 Sammon Loss : 816.9911499023438\n",
            "Batch 130 / 1132 Sammon Loss : 1472.704833984375\n",
            "Batch 140 / 1132 Sammon Loss : 1575.45703125\n",
            "Batch 150 / 1132 Sammon Loss : 1827.84326171875\n",
            "Batch 160 / 1132 Sammon Loss : 1703.4207763671875\n",
            "Batch 170 / 1132 Sammon Loss : 3696.59423828125\n",
            "Batch 180 / 1132 Sammon Loss : 4180.650390625\n",
            "Batch 190 / 1132 Sammon Loss : 3190.5908203125\n",
            "Batch 200 / 1132 Sammon Loss : 2993.28369140625\n",
            "Batch 210 / 1132 Sammon Loss : 2804.775634765625\n",
            "Batch 220 / 1132 Sammon Loss : 2134.2490234375\n",
            "Batch 230 / 1132 Sammon Loss : 1875.7618408203125\n",
            "Batch 240 / 1132 Sammon Loss : 1288.68359375\n",
            "Batch 250 / 1132 Sammon Loss : 1194.5009765625\n",
            "Batch 260 / 1132 Sammon Loss : 1424.912353515625\n",
            "Batch 270 / 1132 Sammon Loss : 1547.260009765625\n",
            "Batch 280 / 1132 Sammon Loss : 1222.2314453125\n",
            "Batch 290 / 1132 Sammon Loss : 1343.3275146484375\n",
            "Batch 300 / 1132 Sammon Loss : 2700.422119140625\n",
            "Batch 310 / 1132 Sammon Loss : 1328.93505859375\n",
            "Batch 320 / 1132 Sammon Loss : 1164.507568359375\n",
            "Batch 330 / 1132 Sammon Loss : 1555.8558349609375\n",
            "Batch 340 / 1132 Sammon Loss : 1338.449951171875\n",
            "Batch 350 / 1132 Sammon Loss : 2247.72705078125\n",
            "Batch 360 / 1132 Sammon Loss : 2365.197265625\n",
            "Batch 370 / 1132 Sammon Loss : 2412.536376953125\n",
            "Batch 380 / 1132 Sammon Loss : 1995.5509033203125\n",
            "Batch 390 / 1132 Sammon Loss : 2398.379150390625\n",
            "Batch 400 / 1132 Sammon Loss : 2838.70751953125\n",
            "Batch 410 / 1132 Sammon Loss : 2070.31494140625\n",
            "Batch 420 / 1132 Sammon Loss : 2082.369384765625\n",
            "Batch 430 / 1132 Sammon Loss : 2273.77685546875\n",
            "Batch 440 / 1132 Sammon Loss : 3074.46240234375\n",
            "Batch 450 / 1132 Sammon Loss : 2017.72412109375\n",
            "Batch 460 / 1132 Sammon Loss : 2459.431640625\n",
            "Batch 470 / 1132 Sammon Loss : 1998.1884765625\n",
            "Batch 480 / 1132 Sammon Loss : 2576.87255859375\n",
            "Batch 490 / 1132 Sammon Loss : 1529.0899658203125\n",
            "Batch 500 / 1132 Sammon Loss : 5626.7685546875\n",
            "Batch 510 / 1132 Sammon Loss : 5454.798828125\n",
            "Batch 520 / 1132 Sammon Loss : 3813.53515625\n",
            "Batch 530 / 1132 Sammon Loss : 4900.82373046875\n",
            "Batch 540 / 1132 Sammon Loss : 1298.0499267578125\n",
            "Batch 550 / 1132 Sammon Loss : 1450.493896484375\n",
            "Batch 560 / 1132 Sammon Loss : 1616.78125\n",
            "Batch 570 / 1132 Sammon Loss : 1597.0601806640625\n",
            "Batch 580 / 1132 Sammon Loss : 1789.57080078125\n",
            "Batch 590 / 1132 Sammon Loss : 1917.603271484375\n",
            "Batch 600 / 1132 Sammon Loss : 1441.216552734375\n",
            "Batch 610 / 1132 Sammon Loss : 1264.950927734375\n",
            "Batch 620 / 1132 Sammon Loss : 3282.516357421875\n",
            "Batch 630 / 1132 Sammon Loss : 3446.47021484375\n",
            "Batch 640 / 1132 Sammon Loss : 3100.056640625\n",
            "Batch 650 / 1132 Sammon Loss : 3896.43359375\n",
            "Batch 660 / 1132 Sammon Loss : 1797.423095703125\n",
            "Batch 670 / 1132 Sammon Loss : 1342.985107421875\n",
            "Batch 680 / 1132 Sammon Loss : 1903.3570556640625\n",
            "Batch 690 / 1132 Sammon Loss : 1305.99365234375\n",
            "Batch 700 / 1132 Sammon Loss : 1775.00927734375\n",
            "Batch 710 / 1132 Sammon Loss : 1280.7325439453125\n",
            "Batch 720 / 1132 Sammon Loss : 1906.524658203125\n",
            "Batch 730 / 1132 Sammon Loss : 905.6410522460938\n",
            "Batch 740 / 1132 Sammon Loss : 1649.650146484375\n",
            "Batch 750 / 1132 Sammon Loss : 1251.36572265625\n",
            "Batch 760 / 1132 Sammon Loss : 1734.715087890625\n",
            "Batch 770 / 1132 Sammon Loss : 2566.6708984375\n",
            "Batch 780 / 1132 Sammon Loss : 2268.267578125\n",
            "Batch 790 / 1132 Sammon Loss : 1486.640625\n",
            "Batch 800 / 1132 Sammon Loss : 2547.73828125\n",
            "Batch 810 / 1132 Sammon Loss : 1230.536376953125\n",
            "Batch 820 / 1132 Sammon Loss : 1419.801025390625\n",
            "Batch 830 / 1132 Sammon Loss : 2376.66845703125\n",
            "Batch 840 / 1132 Sammon Loss : 2021.3668212890625\n",
            "Batch 850 / 1132 Sammon Loss : 3844.677978515625\n",
            "Batch 860 / 1132 Sammon Loss : 4147.8369140625\n",
            "Batch 870 / 1132 Sammon Loss : 4543.830078125\n",
            "Batch 880 / 1132 Sammon Loss : 2052.20654296875\n",
            "Batch 890 / 1132 Sammon Loss : 1764.046142578125\n",
            "Batch 900 / 1132 Sammon Loss : 1707.2373046875\n",
            "Batch 910 / 1132 Sammon Loss : 1978.461669921875\n",
            "Batch 920 / 1132 Sammon Loss : 1441.3892822265625\n",
            "Batch 930 / 1132 Sammon Loss : 3104.24560546875\n",
            "Batch 940 / 1132 Sammon Loss : 4010.572265625\n",
            "Batch 950 / 1132 Sammon Loss : 3379.4111328125\n",
            "Batch 960 / 1132 Sammon Loss : 2475.3232421875\n",
            "Batch 970 / 1132 Sammon Loss : 1686.1761474609375\n",
            "Batch 980 / 1132 Sammon Loss : 1437.49267578125\n",
            "Batch 990 / 1132 Sammon Loss : 1898.5296630859375\n",
            "Batch 1000 / 1132 Sammon Loss : 1486.8765869140625\n",
            "Batch 1010 / 1132 Sammon Loss : 1113.962158203125\n",
            "Batch 1020 / 1132 Sammon Loss : 2232.583984375\n",
            "Batch 1030 / 1132 Sammon Loss : 2113.86962890625\n",
            "Batch 1040 / 1132 Sammon Loss : 2448.478515625\n",
            "Batch 1050 / 1132 Sammon Loss : 1709.792724609375\n",
            "Batch 1060 / 1132 Sammon Loss : 1614.174072265625\n",
            "Batch 1070 / 1132 Sammon Loss : 3932.347900390625\n",
            "Batch 1080 / 1132 Sammon Loss : 2257.054443359375\n",
            "Batch 1090 / 1132 Sammon Loss : 1480.0435791015625\n",
            "Batch 1100 / 1132 Sammon Loss : 4695.6708984375\n",
            "Batch 1110 / 1132 Sammon Loss : 1630.6434326171875\n",
            "Batch 1120 / 1132 Sammon Loss : 1930.772216796875\n",
            "Batch 1130 / 1132 Sammon Loss : 1793.0174560546875\n",
            "Batch 10 / 505 Regressor Loss : 104954.859375\n",
            "Batch 20 / 505 Regressor Loss : 116067.875\n",
            "Batch 30 / 505 Regressor Loss : 99083.6484375\n",
            "Batch 40 / 505 Regressor Loss : 100016.9921875\n",
            "Batch 50 / 505 Regressor Loss : 130551.921875\n",
            "Batch 60 / 505 Regressor Loss : 125179.734375\n",
            "Batch 70 / 505 Regressor Loss : 124825.7109375\n",
            "Batch 80 / 505 Regressor Loss : 126754.859375\n",
            "Batch 90 / 505 Regressor Loss : 130257.90625\n",
            "Batch 100 / 505 Regressor Loss : 104967.953125\n",
            "Batch 110 / 505 Regressor Loss : 123050.4609375\n",
            "Batch 120 / 505 Regressor Loss : 112419.390625\n",
            "Batch 130 / 505 Regressor Loss : 126718.25\n",
            "Batch 140 / 505 Regressor Loss : 132662.875\n",
            "Batch 150 / 505 Regressor Loss : 117379.359375\n",
            "Batch 160 / 505 Regressor Loss : 93396.453125\n",
            "Batch 170 / 505 Regressor Loss : 102145.2109375\n",
            "Batch 180 / 505 Regressor Loss : 130855.46875\n",
            "Batch 190 / 505 Regressor Loss : 125398.7109375\n",
            "Batch 200 / 505 Regressor Loss : 102898.7109375\n",
            "Batch 210 / 505 Regressor Loss : 110351.8984375\n",
            "Batch 220 / 505 Regressor Loss : 122938.0859375\n",
            "Batch 230 / 505 Regressor Loss : 115824.890625\n",
            "Batch 240 / 505 Regressor Loss : 107384.2734375\n",
            "Batch 250 / 505 Regressor Loss : 148394.71875\n",
            "Batch 260 / 505 Regressor Loss : 125509.75\n",
            "Batch 270 / 505 Regressor Loss : 106350.3359375\n",
            "Batch 280 / 505 Regressor Loss : 123919.0625\n",
            "Batch 290 / 505 Regressor Loss : 122815.171875\n",
            "Batch 300 / 505 Regressor Loss : 114619.65625\n",
            "Batch 310 / 505 Regressor Loss : 118873.7109375\n",
            "Batch 320 / 505 Regressor Loss : 144919.21875\n",
            "Batch 330 / 505 Regressor Loss : 111785.6796875\n",
            "Batch 340 / 505 Regressor Loss : 108895.3671875\n",
            "Batch 350 / 505 Regressor Loss : 92289.7734375\n",
            "Batch 360 / 505 Regressor Loss : 99611.2421875\n",
            "Batch 370 / 505 Regressor Loss : 119042.609375\n",
            "Batch 380 / 505 Regressor Loss : 102870.4921875\n",
            "Batch 390 / 505 Regressor Loss : 114710.484375\n",
            "Batch 400 / 505 Regressor Loss : 137622.78125\n",
            "Batch 410 / 505 Regressor Loss : 103814.640625\n",
            "Batch 420 / 505 Regressor Loss : 124606.875\n",
            "Batch 430 / 505 Regressor Loss : 109162.296875\n",
            "Batch 440 / 505 Regressor Loss : 110580.15625\n",
            "Batch 450 / 505 Regressor Loss : 98639.0234375\n",
            "Batch 460 / 505 Regressor Loss : 126306.03125\n",
            "Batch 470 / 505 Regressor Loss : 118947.3125\n",
            "Batch 480 / 505 Regressor Loss : 119710.3984375\n",
            "Batch 490 / 505 Regressor Loss : 119789.9609375\n",
            "Batch 500 / 505 Regressor Loss : 117602.796875\n",
            "Epoch  80 / 100  Sammon Loss : 1102.7469482421875  Regressor Loss : 101589.546875\n",
            "Val Loss : 100004.7109375\n",
            "Batch 10 / 1132 Sammon Loss : 3616.2041015625\n",
            "Batch 20 / 1132 Sammon Loss : 4068.012451171875\n",
            "Batch 30 / 1132 Sammon Loss : 4079.3330078125\n",
            "Batch 40 / 1132 Sammon Loss : 4259.78759765625\n",
            "Batch 50 / 1132 Sammon Loss : 4428.9609375\n",
            "Batch 60 / 1132 Sammon Loss : 2272.84765625\n",
            "Batch 70 / 1132 Sammon Loss : 2340.037353515625\n",
            "Batch 80 / 1132 Sammon Loss : 2021.726318359375\n",
            "Batch 90 / 1132 Sammon Loss : 2060.74658203125\n",
            "Batch 100 / 1132 Sammon Loss : 1880.884033203125\n",
            "Batch 110 / 1132 Sammon Loss : 1491.5697021484375\n",
            "Batch 120 / 1132 Sammon Loss : 852.3768310546875\n",
            "Batch 130 / 1132 Sammon Loss : 1475.777587890625\n",
            "Batch 140 / 1132 Sammon Loss : 1638.7706298828125\n",
            "Batch 150 / 1132 Sammon Loss : 1844.1180419921875\n",
            "Batch 160 / 1132 Sammon Loss : 1680.177001953125\n",
            "Batch 170 / 1132 Sammon Loss : 3779.83984375\n",
            "Batch 180 / 1132 Sammon Loss : 4230.29638671875\n",
            "Batch 190 / 1132 Sammon Loss : 3217.0078125\n",
            "Batch 200 / 1132 Sammon Loss : 3054.9951171875\n",
            "Batch 210 / 1132 Sammon Loss : 2830.9482421875\n",
            "Batch 220 / 1132 Sammon Loss : 2176.97412109375\n",
            "Batch 230 / 1132 Sammon Loss : 1945.5286865234375\n",
            "Batch 240 / 1132 Sammon Loss : 1355.79052734375\n",
            "Batch 250 / 1132 Sammon Loss : 1241.054443359375\n",
            "Batch 260 / 1132 Sammon Loss : 1465.802978515625\n",
            "Batch 270 / 1132 Sammon Loss : 1557.0955810546875\n",
            "Batch 280 / 1132 Sammon Loss : 1252.226318359375\n",
            "Batch 290 / 1132 Sammon Loss : 1320.395263671875\n",
            "Batch 300 / 1132 Sammon Loss : 2734.429443359375\n",
            "Batch 310 / 1132 Sammon Loss : 1319.020263671875\n",
            "Batch 320 / 1132 Sammon Loss : 1150.11376953125\n",
            "Batch 330 / 1132 Sammon Loss : 1545.577392578125\n",
            "Batch 340 / 1132 Sammon Loss : 1344.1046142578125\n",
            "Batch 350 / 1132 Sammon Loss : 2255.9482421875\n",
            "Batch 360 / 1132 Sammon Loss : 2405.4736328125\n",
            "Batch 370 / 1132 Sammon Loss : 2460.47265625\n",
            "Batch 380 / 1132 Sammon Loss : 2001.4361572265625\n",
            "Batch 390 / 1132 Sammon Loss : 2419.77001953125\n",
            "Batch 400 / 1132 Sammon Loss : 2862.74267578125\n",
            "Batch 410 / 1132 Sammon Loss : 2074.795166015625\n",
            "Batch 420 / 1132 Sammon Loss : 2092.58935546875\n",
            "Batch 430 / 1132 Sammon Loss : 2255.472412109375\n",
            "Batch 440 / 1132 Sammon Loss : 3077.91357421875\n",
            "Batch 450 / 1132 Sammon Loss : 2035.58154296875\n",
            "Batch 460 / 1132 Sammon Loss : 2485.115234375\n",
            "Batch 470 / 1132 Sammon Loss : 2023.515625\n",
            "Batch 480 / 1132 Sammon Loss : 2610.26416015625\n",
            "Batch 490 / 1132 Sammon Loss : 1578.133056640625\n",
            "Batch 500 / 1132 Sammon Loss : 5620.9951171875\n",
            "Batch 510 / 1132 Sammon Loss : 5472.26171875\n",
            "Batch 520 / 1132 Sammon Loss : 3809.61376953125\n",
            "Batch 530 / 1132 Sammon Loss : 4890.43359375\n",
            "Batch 540 / 1132 Sammon Loss : 1378.7041015625\n",
            "Batch 550 / 1132 Sammon Loss : 1509.1148681640625\n",
            "Batch 560 / 1132 Sammon Loss : 1629.3863525390625\n",
            "Batch 570 / 1132 Sammon Loss : 1611.6707763671875\n",
            "Batch 580 / 1132 Sammon Loss : 1785.6380615234375\n",
            "Batch 590 / 1132 Sammon Loss : 1947.8870849609375\n",
            "Batch 600 / 1132 Sammon Loss : 1444.21484375\n",
            "Batch 610 / 1132 Sammon Loss : 1364.225830078125\n",
            "Batch 620 / 1132 Sammon Loss : 3172.23779296875\n",
            "Batch 630 / 1132 Sammon Loss : 3425.033935546875\n",
            "Batch 640 / 1132 Sammon Loss : 3113.103759765625\n",
            "Batch 650 / 1132 Sammon Loss : 3874.28125\n",
            "Batch 660 / 1132 Sammon Loss : 1757.574462890625\n",
            "Batch 670 / 1132 Sammon Loss : 1319.49755859375\n",
            "Batch 680 / 1132 Sammon Loss : 1901.670166015625\n",
            "Batch 690 / 1132 Sammon Loss : 1299.9903564453125\n",
            "Batch 700 / 1132 Sammon Loss : 1791.024169921875\n",
            "Batch 710 / 1132 Sammon Loss : 1307.6712646484375\n",
            "Batch 720 / 1132 Sammon Loss : 1915.154296875\n",
            "Batch 730 / 1132 Sammon Loss : 919.3665771484375\n",
            "Batch 740 / 1132 Sammon Loss : 1661.8076171875\n",
            "Batch 750 / 1132 Sammon Loss : 1303.37158203125\n",
            "Batch 760 / 1132 Sammon Loss : 1771.059814453125\n",
            "Batch 770 / 1132 Sammon Loss : 2558.554931640625\n",
            "Batch 780 / 1132 Sammon Loss : 2274.81982421875\n",
            "Batch 790 / 1132 Sammon Loss : 1522.82421875\n",
            "Batch 800 / 1132 Sammon Loss : 2575.20068359375\n",
            "Batch 810 / 1132 Sammon Loss : 1270.6820068359375\n",
            "Batch 820 / 1132 Sammon Loss : 1441.3050537109375\n",
            "Batch 830 / 1132 Sammon Loss : 2360.287353515625\n",
            "Batch 840 / 1132 Sammon Loss : 2015.0020751953125\n",
            "Batch 850 / 1132 Sammon Loss : 3827.2041015625\n",
            "Batch 860 / 1132 Sammon Loss : 4145.4833984375\n",
            "Batch 870 / 1132 Sammon Loss : 4539.2744140625\n",
            "Batch 880 / 1132 Sammon Loss : 2063.89990234375\n",
            "Batch 890 / 1132 Sammon Loss : 1787.1751708984375\n",
            "Batch 900 / 1132 Sammon Loss : 1724.962890625\n",
            "Batch 910 / 1132 Sammon Loss : 1981.90576171875\n",
            "Batch 920 / 1132 Sammon Loss : 1467.5048828125\n",
            "Batch 930 / 1132 Sammon Loss : 3062.870849609375\n",
            "Batch 940 / 1132 Sammon Loss : 4013.734375\n",
            "Batch 950 / 1132 Sammon Loss : 3384.315673828125\n",
            "Batch 960 / 1132 Sammon Loss : 2464.737060546875\n",
            "Batch 970 / 1132 Sammon Loss : 1697.4786376953125\n",
            "Batch 980 / 1132 Sammon Loss : 1439.1741943359375\n",
            "Batch 990 / 1132 Sammon Loss : 1904.109130859375\n",
            "Batch 1000 / 1132 Sammon Loss : 1493.295654296875\n",
            "Batch 1010 / 1132 Sammon Loss : 1111.4654541015625\n",
            "Batch 1020 / 1132 Sammon Loss : 2227.47509765625\n",
            "Batch 1030 / 1132 Sammon Loss : 2132.822021484375\n",
            "Batch 1040 / 1132 Sammon Loss : 2467.04541015625\n",
            "Batch 1050 / 1132 Sammon Loss : 1699.3485107421875\n",
            "Batch 1060 / 1132 Sammon Loss : 1609.7545166015625\n",
            "Batch 1070 / 1132 Sammon Loss : 3942.68798828125\n",
            "Batch 1080 / 1132 Sammon Loss : 2197.4052734375\n",
            "Batch 1090 / 1132 Sammon Loss : 1475.048095703125\n",
            "Batch 1100 / 1132 Sammon Loss : 4649.49169921875\n",
            "Batch 1110 / 1132 Sammon Loss : 1609.161865234375\n",
            "Batch 1120 / 1132 Sammon Loss : 1916.6551513671875\n",
            "Batch 1130 / 1132 Sammon Loss : 1756.981689453125\n",
            "Batch 10 / 505 Regressor Loss : 104743.375\n",
            "Batch 20 / 505 Regressor Loss : 115852.3125\n",
            "Batch 30 / 505 Regressor Loss : 98867.1484375\n",
            "Batch 40 / 505 Regressor Loss : 99797.671875\n",
            "Batch 50 / 505 Regressor Loss : 130286.578125\n",
            "Batch 60 / 505 Regressor Loss : 124937.8671875\n",
            "Batch 70 / 505 Regressor Loss : 124592.546875\n",
            "Batch 80 / 505 Regressor Loss : 126502.359375\n",
            "Batch 90 / 505 Regressor Loss : 130027.109375\n",
            "Batch 100 / 505 Regressor Loss : 104741.1171875\n",
            "Batch 110 / 505 Regressor Loss : 122824.4375\n",
            "Batch 120 / 505 Regressor Loss : 112211.34375\n",
            "Batch 130 / 505 Regressor Loss : 126492.0625\n",
            "Batch 140 / 505 Regressor Loss : 132421.921875\n",
            "Batch 150 / 505 Regressor Loss : 117155.171875\n",
            "Batch 160 / 505 Regressor Loss : 93190.359375\n",
            "Batch 170 / 505 Regressor Loss : 101930.8984375\n",
            "Batch 180 / 505 Regressor Loss : 130614.5\n",
            "Batch 190 / 505 Regressor Loss : 125147.8125\n",
            "Batch 200 / 505 Regressor Loss : 102699.5\n",
            "Batch 210 / 505 Regressor Loss : 110128.3671875\n",
            "Batch 220 / 505 Regressor Loss : 122706.109375\n",
            "Batch 230 / 505 Regressor Loss : 115605.0859375\n",
            "Batch 240 / 505 Regressor Loss : 107178.171875\n",
            "Batch 250 / 505 Regressor Loss : 148136.421875\n",
            "Batch 260 / 505 Regressor Loss : 125266.75\n",
            "Batch 270 / 505 Regressor Loss : 106140.703125\n",
            "Batch 280 / 505 Regressor Loss : 123684.0\n",
            "Batch 290 / 505 Regressor Loss : 122595.9609375\n",
            "Batch 300 / 505 Regressor Loss : 114388.671875\n",
            "Batch 310 / 505 Regressor Loss : 118646.609375\n",
            "Batch 320 / 505 Regressor Loss : 144658.4375\n",
            "Batch 330 / 505 Regressor Loss : 111555.7109375\n",
            "Batch 340 / 505 Regressor Loss : 108657.296875\n",
            "Batch 350 / 505 Regressor Loss : 92081.796875\n",
            "Batch 360 / 505 Regressor Loss : 99416.609375\n",
            "Batch 370 / 505 Regressor Loss : 118807.546875\n",
            "Batch 380 / 505 Regressor Loss : 102660.8125\n",
            "Batch 390 / 505 Regressor Loss : 114487.546875\n",
            "Batch 400 / 505 Regressor Loss : 137354.71875\n",
            "Batch 410 / 505 Regressor Loss : 103603.546875\n",
            "Batch 420 / 505 Regressor Loss : 124376.8125\n",
            "Batch 430 / 505 Regressor Loss : 108943.0859375\n",
            "Batch 440 / 505 Regressor Loss : 110354.546875\n",
            "Batch 450 / 505 Regressor Loss : 98439.625\n",
            "Batch 460 / 505 Regressor Loss : 126073.3046875\n",
            "Batch 470 / 505 Regressor Loss : 118735.5\n",
            "Batch 480 / 505 Regressor Loss : 119486.046875\n",
            "Batch 490 / 505 Regressor Loss : 119559.046875\n",
            "Batch 500 / 505 Regressor Loss : 117388.2734375\n",
            "Epoch  81 / 100  Sammon Loss : 1098.4049072265625  Regressor Loss : 101390.7734375\n",
            "Val Loss : 99753.328125\n",
            "Batch 10 / 1132 Sammon Loss : 3573.51708984375\n",
            "Batch 20 / 1132 Sammon Loss : 4057.60009765625\n",
            "Batch 30 / 1132 Sammon Loss : 4046.630126953125\n",
            "Batch 40 / 1132 Sammon Loss : 4236.923828125\n",
            "Batch 50 / 1132 Sammon Loss : 4384.91455078125\n",
            "Batch 60 / 1132 Sammon Loss : 2245.02587890625\n",
            "Batch 70 / 1132 Sammon Loss : 2329.40966796875\n",
            "Batch 80 / 1132 Sammon Loss : 2005.2265625\n",
            "Batch 90 / 1132 Sammon Loss : 2032.517333984375\n",
            "Batch 100 / 1132 Sammon Loss : 1842.61376953125\n",
            "Batch 110 / 1132 Sammon Loss : 1426.6180419921875\n",
            "Batch 120 / 1132 Sammon Loss : 791.1500244140625\n",
            "Batch 130 / 1132 Sammon Loss : 1457.182861328125\n",
            "Batch 140 / 1132 Sammon Loss : 1569.8626708984375\n",
            "Batch 150 / 1132 Sammon Loss : 1815.560302734375\n",
            "Batch 160 / 1132 Sammon Loss : 1696.12451171875\n",
            "Batch 170 / 1132 Sammon Loss : 3743.8701171875\n",
            "Batch 180 / 1132 Sammon Loss : 4272.48876953125\n",
            "Batch 190 / 1132 Sammon Loss : 3243.1435546875\n",
            "Batch 200 / 1132 Sammon Loss : 3049.2978515625\n",
            "Batch 210 / 1132 Sammon Loss : 2828.48974609375\n",
            "Batch 220 / 1132 Sammon Loss : 2172.3466796875\n",
            "Batch 230 / 1132 Sammon Loss : 1888.732421875\n",
            "Batch 240 / 1132 Sammon Loss : 1265.300537109375\n",
            "Batch 250 / 1132 Sammon Loss : 1234.11181640625\n",
            "Batch 260 / 1132 Sammon Loss : 1442.5968017578125\n",
            "Batch 270 / 1132 Sammon Loss : 1554.554931640625\n",
            "Batch 280 / 1132 Sammon Loss : 1208.2706298828125\n",
            "Batch 290 / 1132 Sammon Loss : 1306.24072265625\n",
            "Batch 300 / 1132 Sammon Loss : 2745.9638671875\n",
            "Batch 310 / 1132 Sammon Loss : 1319.6282958984375\n",
            "Batch 320 / 1132 Sammon Loss : 1139.2919921875\n",
            "Batch 330 / 1132 Sammon Loss : 1532.9364013671875\n",
            "Batch 340 / 1132 Sammon Loss : 1313.2431640625\n",
            "Batch 350 / 1132 Sammon Loss : 2224.6064453125\n",
            "Batch 360 / 1132 Sammon Loss : 2413.72119140625\n",
            "Batch 370 / 1132 Sammon Loss : 2447.117431640625\n",
            "Batch 380 / 1132 Sammon Loss : 1985.4100341796875\n",
            "Batch 390 / 1132 Sammon Loss : 2383.274658203125\n",
            "Batch 400 / 1132 Sammon Loss : 2908.80224609375\n",
            "Batch 410 / 1132 Sammon Loss : 2076.991943359375\n",
            "Batch 420 / 1132 Sammon Loss : 2086.663818359375\n",
            "Batch 430 / 1132 Sammon Loss : 2264.335205078125\n",
            "Batch 440 / 1132 Sammon Loss : 3086.66064453125\n",
            "Batch 450 / 1132 Sammon Loss : 2034.4022216796875\n",
            "Batch 460 / 1132 Sammon Loss : 2471.3955078125\n",
            "Batch 470 / 1132 Sammon Loss : 1969.37109375\n",
            "Batch 480 / 1132 Sammon Loss : 2586.6513671875\n",
            "Batch 490 / 1132 Sammon Loss : 1530.5244140625\n",
            "Batch 500 / 1132 Sammon Loss : 5601.37744140625\n",
            "Batch 510 / 1132 Sammon Loss : 5454.79638671875\n",
            "Batch 520 / 1132 Sammon Loss : 3806.50244140625\n",
            "Batch 530 / 1132 Sammon Loss : 4877.033203125\n",
            "Batch 540 / 1132 Sammon Loss : 1333.14990234375\n",
            "Batch 550 / 1132 Sammon Loss : 1471.43701171875\n",
            "Batch 560 / 1132 Sammon Loss : 1591.8623046875\n",
            "Batch 570 / 1132 Sammon Loss : 1576.86474609375\n",
            "Batch 580 / 1132 Sammon Loss : 1730.552978515625\n",
            "Batch 590 / 1132 Sammon Loss : 1882.617431640625\n",
            "Batch 600 / 1132 Sammon Loss : 1411.220703125\n",
            "Batch 610 / 1132 Sammon Loss : 1229.5108642578125\n",
            "Batch 620 / 1132 Sammon Loss : 3290.42724609375\n",
            "Batch 630 / 1132 Sammon Loss : 3465.10791015625\n",
            "Batch 640 / 1132 Sammon Loss : 3104.19189453125\n",
            "Batch 650 / 1132 Sammon Loss : 3941.952392578125\n",
            "Batch 660 / 1132 Sammon Loss : 1760.05859375\n",
            "Batch 670 / 1132 Sammon Loss : 1329.66259765625\n",
            "Batch 680 / 1132 Sammon Loss : 1898.5186767578125\n",
            "Batch 690 / 1132 Sammon Loss : 1270.7744140625\n",
            "Batch 700 / 1132 Sammon Loss : 1748.41015625\n",
            "Batch 710 / 1132 Sammon Loss : 1224.6893310546875\n",
            "Batch 720 / 1132 Sammon Loss : 1908.26416015625\n",
            "Batch 730 / 1132 Sammon Loss : 847.1268310546875\n",
            "Batch 740 / 1132 Sammon Loss : 1569.126708984375\n",
            "Batch 750 / 1132 Sammon Loss : 1190.7496337890625\n",
            "Batch 760 / 1132 Sammon Loss : 1740.68896484375\n",
            "Batch 770 / 1132 Sammon Loss : 2556.37109375\n",
            "Batch 780 / 1132 Sammon Loss : 2247.923828125\n",
            "Batch 790 / 1132 Sammon Loss : 1474.28759765625\n",
            "Batch 800 / 1132 Sammon Loss : 2512.223388671875\n",
            "Batch 810 / 1132 Sammon Loss : 1196.5439453125\n",
            "Batch 820 / 1132 Sammon Loss : 1363.10791015625\n",
            "Batch 830 / 1132 Sammon Loss : 2348.38134765625\n",
            "Batch 840 / 1132 Sammon Loss : 1974.1793212890625\n",
            "Batch 850 / 1132 Sammon Loss : 3867.220703125\n",
            "Batch 860 / 1132 Sammon Loss : 4145.751953125\n",
            "Batch 870 / 1132 Sammon Loss : 4542.97314453125\n",
            "Batch 880 / 1132 Sammon Loss : 2010.748291015625\n",
            "Batch 890 / 1132 Sammon Loss : 1756.722900390625\n",
            "Batch 900 / 1132 Sammon Loss : 1682.335205078125\n",
            "Batch 910 / 1132 Sammon Loss : 2002.205810546875\n",
            "Batch 920 / 1132 Sammon Loss : 1468.072021484375\n",
            "Batch 930 / 1132 Sammon Loss : 3136.23095703125\n",
            "Batch 940 / 1132 Sammon Loss : 3999.3603515625\n",
            "Batch 950 / 1132 Sammon Loss : 3382.29443359375\n",
            "Batch 960 / 1132 Sammon Loss : 2494.018798828125\n",
            "Batch 970 / 1132 Sammon Loss : 1694.87890625\n",
            "Batch 980 / 1132 Sammon Loss : 1412.505615234375\n",
            "Batch 990 / 1132 Sammon Loss : 1911.854736328125\n",
            "Batch 1000 / 1132 Sammon Loss : 1480.453125\n",
            "Batch 1010 / 1132 Sammon Loss : 1106.20751953125\n",
            "Batch 1020 / 1132 Sammon Loss : 2221.684814453125\n",
            "Batch 1030 / 1132 Sammon Loss : 2116.076416015625\n",
            "Batch 1040 / 1132 Sammon Loss : 2433.463134765625\n",
            "Batch 1050 / 1132 Sammon Loss : 1719.35986328125\n",
            "Batch 1060 / 1132 Sammon Loss : 1640.968017578125\n",
            "Batch 1070 / 1132 Sammon Loss : 3872.966796875\n",
            "Batch 1080 / 1132 Sammon Loss : 2197.8916015625\n",
            "Batch 1090 / 1132 Sammon Loss : 1541.4033203125\n",
            "Batch 1100 / 1132 Sammon Loss : 4599.458984375\n",
            "Batch 1110 / 1132 Sammon Loss : 1591.2176513671875\n",
            "Batch 1120 / 1132 Sammon Loss : 1906.2794189453125\n",
            "Batch 1130 / 1132 Sammon Loss : 1732.9736328125\n",
            "Batch 10 / 505 Regressor Loss : 104532.2734375\n",
            "Batch 20 / 505 Regressor Loss : 115637.125\n",
            "Batch 30 / 505 Regressor Loss : 98651.0546875\n",
            "Batch 40 / 505 Regressor Loss : 99578.7421875\n",
            "Batch 50 / 505 Regressor Loss : 130021.6484375\n",
            "Batch 60 / 505 Regressor Loss : 124696.421875\n",
            "Batch 70 / 505 Regressor Loss : 124359.7734375\n",
            "Batch 80 / 505 Regressor Loss : 126250.28125\n",
            "Batch 90 / 505 Regressor Loss : 129796.6875\n",
            "Batch 100 / 505 Regressor Loss : 104514.671875\n",
            "Batch 110 / 505 Regressor Loss : 122598.796875\n",
            "Batch 120 / 505 Regressor Loss : 112003.6875\n",
            "Batch 130 / 505 Regressor Loss : 126266.265625\n",
            "Batch 140 / 505 Regressor Loss : 132181.40625\n",
            "Batch 150 / 505 Regressor Loss : 116931.3984375\n",
            "Batch 160 / 505 Regressor Loss : 92984.6796875\n",
            "Batch 170 / 505 Regressor Loss : 101716.9609375\n",
            "Batch 180 / 505 Regressor Loss : 130373.953125\n",
            "Batch 190 / 505 Regressor Loss : 124897.34375\n",
            "Batch 200 / 505 Regressor Loss : 102500.65625\n",
            "Batch 210 / 505 Regressor Loss : 109905.234375\n",
            "Batch 220 / 505 Regressor Loss : 122474.53125\n",
            "Batch 230 / 505 Regressor Loss : 115385.671875\n",
            "Batch 240 / 505 Regressor Loss : 106972.4375\n",
            "Batch 250 / 505 Regressor Loss : 147878.546875\n",
            "Batch 260 / 505 Regressor Loss : 125024.15625\n",
            "Batch 270 / 505 Regressor Loss : 105931.484375\n",
            "Batch 280 / 505 Regressor Loss : 123449.375\n",
            "Batch 290 / 505 Regressor Loss : 122377.1484375\n",
            "Batch 300 / 505 Regressor Loss : 114158.078125\n",
            "Batch 310 / 505 Regressor Loss : 118419.8984375\n",
            "Batch 320 / 505 Regressor Loss : 144398.09375\n",
            "Batch 330 / 505 Regressor Loss : 111326.171875\n",
            "Batch 340 / 505 Regressor Loss : 108419.640625\n",
            "Batch 350 / 505 Regressor Loss : 91874.25\n",
            "Batch 360 / 505 Regressor Loss : 99222.34375\n",
            "Batch 370 / 505 Regressor Loss : 118572.9296875\n",
            "Batch 380 / 505 Regressor Loss : 102451.5546875\n",
            "Batch 390 / 505 Regressor Loss : 114265.0\n",
            "Batch 400 / 505 Regressor Loss : 137087.09375\n",
            "Batch 410 / 505 Regressor Loss : 103392.8671875\n",
            "Batch 420 / 505 Regressor Loss : 124147.140625\n",
            "Batch 430 / 505 Regressor Loss : 108724.2734375\n",
            "Batch 440 / 505 Regressor Loss : 110129.3359375\n",
            "Batch 450 / 505 Regressor Loss : 98240.609375\n",
            "Batch 460 / 505 Regressor Loss : 125840.9921875\n",
            "Batch 470 / 505 Regressor Loss : 118524.0546875\n",
            "Batch 480 / 505 Regressor Loss : 119262.109375\n",
            "Batch 490 / 505 Regressor Loss : 119328.546875\n",
            "Batch 500 / 505 Regressor Loss : 117174.125\n",
            "Epoch  82 / 100  Sammon Loss : 1097.0458984375  Regressor Loss : 101192.3671875\n",
            "Val Loss : 99502.359375\n",
            "Batch 10 / 1132 Sammon Loss : 3538.91015625\n",
            "Batch 20 / 1132 Sammon Loss : 4007.38916015625\n",
            "Batch 30 / 1132 Sammon Loss : 4001.7822265625\n",
            "Batch 40 / 1132 Sammon Loss : 4217.8896484375\n",
            "Batch 50 / 1132 Sammon Loss : 4351.4140625\n",
            "Batch 60 / 1132 Sammon Loss : 2234.76513671875\n",
            "Batch 70 / 1132 Sammon Loss : 2291.873046875\n",
            "Batch 80 / 1132 Sammon Loss : 1996.58544921875\n",
            "Batch 90 / 1132 Sammon Loss : 2017.358154296875\n",
            "Batch 100 / 1132 Sammon Loss : 1822.73291015625\n",
            "Batch 110 / 1132 Sammon Loss : 1437.461181640625\n",
            "Batch 120 / 1132 Sammon Loss : 764.1654052734375\n",
            "Batch 130 / 1132 Sammon Loss : 1429.86279296875\n",
            "Batch 140 / 1132 Sammon Loss : 1600.802734375\n",
            "Batch 150 / 1132 Sammon Loss : 1811.8990478515625\n",
            "Batch 160 / 1132 Sammon Loss : 1684.0972900390625\n",
            "Batch 170 / 1132 Sammon Loss : 3738.4072265625\n",
            "Batch 180 / 1132 Sammon Loss : 4245.802734375\n",
            "Batch 190 / 1132 Sammon Loss : 3235.9306640625\n",
            "Batch 200 / 1132 Sammon Loss : 3022.375\n",
            "Batch 210 / 1132 Sammon Loss : 2836.0859375\n",
            "Batch 220 / 1132 Sammon Loss : 2141.7763671875\n",
            "Batch 230 / 1132 Sammon Loss : 1860.7672119140625\n",
            "Batch 240 / 1132 Sammon Loss : 1265.275390625\n",
            "Batch 250 / 1132 Sammon Loss : 1189.593505859375\n",
            "Batch 260 / 1132 Sammon Loss : 1430.90576171875\n",
            "Batch 270 / 1132 Sammon Loss : 1552.4130859375\n",
            "Batch 280 / 1132 Sammon Loss : 1180.46875\n",
            "Batch 290 / 1132 Sammon Loss : 1287.80224609375\n",
            "Batch 300 / 1132 Sammon Loss : 2706.28271484375\n",
            "Batch 310 / 1132 Sammon Loss : 1299.9791259765625\n",
            "Batch 320 / 1132 Sammon Loss : 1136.672607421875\n",
            "Batch 330 / 1132 Sammon Loss : 1539.23193359375\n",
            "Batch 340 / 1132 Sammon Loss : 1319.2672119140625\n",
            "Batch 350 / 1132 Sammon Loss : 2243.66650390625\n",
            "Batch 360 / 1132 Sammon Loss : 2486.622802734375\n",
            "Batch 370 / 1132 Sammon Loss : 2480.76318359375\n",
            "Batch 380 / 1132 Sammon Loss : 2027.726806640625\n",
            "Batch 390 / 1132 Sammon Loss : 2417.39794921875\n",
            "Batch 400 / 1132 Sammon Loss : 2932.9130859375\n",
            "Batch 410 / 1132 Sammon Loss : 2095.0634765625\n",
            "Batch 420 / 1132 Sammon Loss : 2112.61083984375\n",
            "Batch 430 / 1132 Sammon Loss : 2326.973876953125\n",
            "Batch 440 / 1132 Sammon Loss : 3147.8603515625\n",
            "Batch 450 / 1132 Sammon Loss : 2051.48681640625\n",
            "Batch 460 / 1132 Sammon Loss : 2506.580810546875\n",
            "Batch 470 / 1132 Sammon Loss : 2036.2679443359375\n",
            "Batch 480 / 1132 Sammon Loss : 2645.32470703125\n",
            "Batch 490 / 1132 Sammon Loss : 1554.7236328125\n",
            "Batch 500 / 1132 Sammon Loss : 5708.77978515625\n",
            "Batch 510 / 1132 Sammon Loss : 5522.0009765625\n",
            "Batch 520 / 1132 Sammon Loss : 3862.15576171875\n",
            "Batch 530 / 1132 Sammon Loss : 4933.447265625\n",
            "Batch 540 / 1132 Sammon Loss : 1405.2239990234375\n",
            "Batch 550 / 1132 Sammon Loss : 1554.9715576171875\n",
            "Batch 560 / 1132 Sammon Loss : 1653.8922119140625\n",
            "Batch 570 / 1132 Sammon Loss : 1644.64697265625\n",
            "Batch 580 / 1132 Sammon Loss : 1848.0419921875\n",
            "Batch 590 / 1132 Sammon Loss : 1964.5958251953125\n",
            "Batch 600 / 1132 Sammon Loss : 1479.81298828125\n",
            "Batch 610 / 1132 Sammon Loss : 1354.2979736328125\n",
            "Batch 620 / 1132 Sammon Loss : 3254.64404296875\n",
            "Batch 630 / 1132 Sammon Loss : 3492.798828125\n",
            "Batch 640 / 1132 Sammon Loss : 3148.619140625\n",
            "Batch 650 / 1132 Sammon Loss : 3917.273193359375\n",
            "Batch 660 / 1132 Sammon Loss : 1828.31298828125\n",
            "Batch 670 / 1132 Sammon Loss : 1363.4091796875\n",
            "Batch 680 / 1132 Sammon Loss : 1950.862060546875\n",
            "Batch 690 / 1132 Sammon Loss : 1375.8740234375\n",
            "Batch 700 / 1132 Sammon Loss : 1830.97998046875\n",
            "Batch 710 / 1132 Sammon Loss : 1340.966552734375\n",
            "Batch 720 / 1132 Sammon Loss : 1963.602783203125\n",
            "Batch 730 / 1132 Sammon Loss : 929.9874877929688\n",
            "Batch 740 / 1132 Sammon Loss : 1712.5604248046875\n",
            "Batch 750 / 1132 Sammon Loss : 1350.1611328125\n",
            "Batch 760 / 1132 Sammon Loss : 1819.727294921875\n",
            "Batch 770 / 1132 Sammon Loss : 2621.37109375\n",
            "Batch 780 / 1132 Sammon Loss : 2350.867919921875\n",
            "Batch 790 / 1132 Sammon Loss : 1571.5372314453125\n",
            "Batch 800 / 1132 Sammon Loss : 2611.600341796875\n",
            "Batch 810 / 1132 Sammon Loss : 1321.495849609375\n",
            "Batch 820 / 1132 Sammon Loss : 1482.1083984375\n",
            "Batch 830 / 1132 Sammon Loss : 2436.133056640625\n",
            "Batch 840 / 1132 Sammon Loss : 2071.65869140625\n",
            "Batch 850 / 1132 Sammon Loss : 3905.393310546875\n",
            "Batch 860 / 1132 Sammon Loss : 4227.5244140625\n",
            "Batch 870 / 1132 Sammon Loss : 4622.8125\n",
            "Batch 880 / 1132 Sammon Loss : 2118.255859375\n",
            "Batch 890 / 1132 Sammon Loss : 1815.380126953125\n",
            "Batch 900 / 1132 Sammon Loss : 1775.73486328125\n",
            "Batch 910 / 1132 Sammon Loss : 2031.17724609375\n",
            "Batch 920 / 1132 Sammon Loss : 1488.46875\n",
            "Batch 930 / 1132 Sammon Loss : 3161.26904296875\n",
            "Batch 940 / 1132 Sammon Loss : 4136.75634765625\n",
            "Batch 950 / 1132 Sammon Loss : 3478.5341796875\n",
            "Batch 960 / 1132 Sammon Loss : 2554.76953125\n",
            "Batch 970 / 1132 Sammon Loss : 1762.680908203125\n",
            "Batch 980 / 1132 Sammon Loss : 1475.7568359375\n",
            "Batch 990 / 1132 Sammon Loss : 1958.7313232421875\n",
            "Batch 1000 / 1132 Sammon Loss : 1528.4735107421875\n",
            "Batch 1010 / 1132 Sammon Loss : 1121.1363525390625\n",
            "Batch 1020 / 1132 Sammon Loss : 2287.91162109375\n",
            "Batch 1030 / 1132 Sammon Loss : 2202.93798828125\n",
            "Batch 1040 / 1132 Sammon Loss : 2536.388671875\n",
            "Batch 1050 / 1132 Sammon Loss : 1751.93017578125\n",
            "Batch 1060 / 1132 Sammon Loss : 1638.55712890625\n",
            "Batch 1070 / 1132 Sammon Loss : 4011.48876953125\n",
            "Batch 1080 / 1132 Sammon Loss : 2228.86669921875\n",
            "Batch 1090 / 1132 Sammon Loss : 1505.14990234375\n",
            "Batch 1100 / 1132 Sammon Loss : 4716.6044921875\n",
            "Batch 1110 / 1132 Sammon Loss : 1646.0625\n",
            "Batch 1120 / 1132 Sammon Loss : 1957.211181640625\n",
            "Batch 1130 / 1132 Sammon Loss : 1815.123046875\n",
            "Batch 10 / 505 Regressor Loss : 104321.546875\n",
            "Batch 20 / 505 Regressor Loss : 115422.359375\n",
            "Batch 30 / 505 Regressor Loss : 98435.375\n",
            "Batch 40 / 505 Regressor Loss : 99360.2109375\n",
            "Batch 50 / 505 Regressor Loss : 129757.1484375\n",
            "Batch 60 / 505 Regressor Loss : 124455.375\n",
            "Batch 70 / 505 Regressor Loss : 124127.40625\n",
            "Batch 80 / 505 Regressor Loss : 125998.671875\n",
            "Batch 90 / 505 Regressor Loss : 129566.65625\n",
            "Batch 100 / 505 Regressor Loss : 104288.625\n",
            "Batch 110 / 505 Regressor Loss : 122373.515625\n",
            "Batch 120 / 505 Regressor Loss : 111796.40625\n",
            "Batch 130 / 505 Regressor Loss : 126040.8359375\n",
            "Batch 140 / 505 Regressor Loss : 131941.265625\n",
            "Batch 150 / 505 Regressor Loss : 116708.0\n",
            "Batch 160 / 505 Regressor Loss : 92779.3984375\n",
            "Batch 170 / 505 Regressor Loss : 101503.4296875\n",
            "Batch 180 / 505 Regressor Loss : 130133.8046875\n",
            "Batch 190 / 505 Regressor Loss : 124647.28125\n",
            "Batch 200 / 505 Regressor Loss : 102302.2109375\n",
            "Batch 210 / 505 Regressor Loss : 109682.5\n",
            "Batch 220 / 505 Regressor Loss : 122243.375\n",
            "Batch 230 / 505 Regressor Loss : 115166.65625\n",
            "Batch 240 / 505 Regressor Loss : 106767.1171875\n",
            "Batch 250 / 505 Regressor Loss : 147621.078125\n",
            "Batch 260 / 505 Regressor Loss : 124782.0234375\n",
            "Batch 270 / 505 Regressor Loss : 105722.6484375\n",
            "Batch 280 / 505 Regressor Loss : 123215.15625\n",
            "Batch 290 / 505 Regressor Loss : 122158.734375\n",
            "Batch 300 / 505 Regressor Loss : 113927.875\n",
            "Batch 310 / 505 Regressor Loss : 118193.5859375\n",
            "Batch 320 / 505 Regressor Loss : 144138.171875\n",
            "Batch 330 / 505 Regressor Loss : 111097.0\n",
            "Batch 340 / 505 Regressor Loss : 108182.390625\n",
            "Batch 350 / 505 Regressor Loss : 91667.0859375\n",
            "Batch 360 / 505 Regressor Loss : 99028.484375\n",
            "Batch 370 / 505 Regressor Loss : 118338.7109375\n",
            "Batch 380 / 505 Regressor Loss : 102242.671875\n",
            "Batch 390 / 505 Regressor Loss : 114042.859375\n",
            "Batch 400 / 505 Regressor Loss : 136819.96875\n",
            "Batch 410 / 505 Regressor Loss : 103182.609375\n",
            "Batch 420 / 505 Regressor Loss : 123917.8671875\n",
            "Batch 430 / 505 Regressor Loss : 108505.859375\n",
            "Batch 440 / 505 Regressor Loss : 109904.515625\n",
            "Batch 450 / 505 Regressor Loss : 98042.0\n",
            "Batch 460 / 505 Regressor Loss : 125609.0859375\n",
            "Batch 470 / 505 Regressor Loss : 118312.9921875\n",
            "Batch 480 / 505 Regressor Loss : 119038.5625\n",
            "Batch 490 / 505 Regressor Loss : 119098.4296875\n",
            "Batch 500 / 505 Regressor Loss : 116960.3984375\n",
            "Epoch  83 / 100  Sammon Loss : 1109.1834716796875  Regressor Loss : 100994.3671875\n",
            "Val Loss : 99251.78125\n",
            "Batch 10 / 1132 Sammon Loss : 3633.228515625\n",
            "Batch 20 / 1132 Sammon Loss : 4095.052734375\n",
            "Batch 30 / 1132 Sammon Loss : 4110.3486328125\n",
            "Batch 40 / 1132 Sammon Loss : 4272.61376953125\n",
            "Batch 50 / 1132 Sammon Loss : 4438.685546875\n",
            "Batch 60 / 1132 Sammon Loss : 2282.44677734375\n",
            "Batch 70 / 1132 Sammon Loss : 2347.5439453125\n",
            "Batch 80 / 1132 Sammon Loss : 2046.226318359375\n",
            "Batch 90 / 1132 Sammon Loss : 2075.8466796875\n",
            "Batch 100 / 1132 Sammon Loss : 1882.742919921875\n",
            "Batch 110 / 1132 Sammon Loss : 1476.476806640625\n",
            "Batch 120 / 1132 Sammon Loss : 802.3458251953125\n",
            "Batch 130 / 1132 Sammon Loss : 1497.172607421875\n",
            "Batch 140 / 1132 Sammon Loss : 1620.4688720703125\n",
            "Batch 150 / 1132 Sammon Loss : 1862.785888671875\n",
            "Batch 160 / 1132 Sammon Loss : 1746.38916015625\n",
            "Batch 170 / 1132 Sammon Loss : 3844.96044921875\n",
            "Batch 180 / 1132 Sammon Loss : 4355.4228515625\n",
            "Batch 190 / 1132 Sammon Loss : 3287.63623046875\n",
            "Batch 200 / 1132 Sammon Loss : 3122.24560546875\n",
            "Batch 210 / 1132 Sammon Loss : 2901.7021484375\n",
            "Batch 220 / 1132 Sammon Loss : 2231.668212890625\n",
            "Batch 230 / 1132 Sammon Loss : 2005.008544921875\n",
            "Batch 240 / 1132 Sammon Loss : 1357.1826171875\n",
            "Batch 250 / 1132 Sammon Loss : 1318.2269287109375\n",
            "Batch 260 / 1132 Sammon Loss : 1522.1904296875\n",
            "Batch 270 / 1132 Sammon Loss : 1633.5947265625\n",
            "Batch 280 / 1132 Sammon Loss : 1312.24609375\n",
            "Batch 290 / 1132 Sammon Loss : 1413.137939453125\n",
            "Batch 300 / 1132 Sammon Loss : 2796.07666015625\n",
            "Batch 310 / 1132 Sammon Loss : 1413.0902099609375\n",
            "Batch 320 / 1132 Sammon Loss : 1238.5726318359375\n",
            "Batch 330 / 1132 Sammon Loss : 1604.7744140625\n",
            "Batch 340 / 1132 Sammon Loss : 1445.173583984375\n",
            "Batch 350 / 1132 Sammon Loss : 2324.6328125\n",
            "Batch 360 / 1132 Sammon Loss : 2469.940185546875\n",
            "Batch 370 / 1132 Sammon Loss : 2514.63525390625\n",
            "Batch 380 / 1132 Sammon Loss : 2059.89208984375\n",
            "Batch 390 / 1132 Sammon Loss : 2478.85302734375\n",
            "Batch 400 / 1132 Sammon Loss : 2926.23291015625\n",
            "Batch 410 / 1132 Sammon Loss : 2162.4765625\n",
            "Batch 420 / 1132 Sammon Loss : 2196.672119140625\n",
            "Batch 430 / 1132 Sammon Loss : 2372.086669921875\n",
            "Batch 440 / 1132 Sammon Loss : 3160.5869140625\n",
            "Batch 450 / 1132 Sammon Loss : 2197.5908203125\n",
            "Batch 460 / 1132 Sammon Loss : 2580.0888671875\n",
            "Batch 470 / 1132 Sammon Loss : 2108.65576171875\n",
            "Batch 480 / 1132 Sammon Loss : 2677.57177734375\n",
            "Batch 490 / 1132 Sammon Loss : 1678.486328125\n",
            "Batch 500 / 1132 Sammon Loss : 5695.6005859375\n",
            "Batch 510 / 1132 Sammon Loss : 5558.82763671875\n",
            "Batch 520 / 1132 Sammon Loss : 3944.41455078125\n",
            "Batch 530 / 1132 Sammon Loss : 5005.146484375\n",
            "Batch 540 / 1132 Sammon Loss : 1500.4794921875\n",
            "Batch 550 / 1132 Sammon Loss : 1632.77099609375\n",
            "Batch 560 / 1132 Sammon Loss : 1718.6602783203125\n",
            "Batch 570 / 1132 Sammon Loss : 1724.2314453125\n",
            "Batch 580 / 1132 Sammon Loss : 1872.4993896484375\n",
            "Batch 590 / 1132 Sammon Loss : 1997.844482421875\n",
            "Batch 600 / 1132 Sammon Loss : 1525.1593017578125\n",
            "Batch 610 / 1132 Sammon Loss : 1351.9359130859375\n",
            "Batch 620 / 1132 Sammon Loss : 3412.95166015625\n",
            "Batch 630 / 1132 Sammon Loss : 3610.19140625\n",
            "Batch 640 / 1132 Sammon Loss : 3246.72509765625\n",
            "Batch 650 / 1132 Sammon Loss : 4005.1298828125\n",
            "Batch 660 / 1132 Sammon Loss : 1862.0533447265625\n",
            "Batch 670 / 1132 Sammon Loss : 1467.49755859375\n",
            "Batch 680 / 1132 Sammon Loss : 2027.231201171875\n",
            "Batch 690 / 1132 Sammon Loss : 1391.89404296875\n",
            "Batch 700 / 1132 Sammon Loss : 1857.8551025390625\n",
            "Batch 710 / 1132 Sammon Loss : 1371.6202392578125\n",
            "Batch 720 / 1132 Sammon Loss : 1987.385986328125\n",
            "Batch 730 / 1132 Sammon Loss : 965.419677734375\n",
            "Batch 740 / 1132 Sammon Loss : 1755.2061767578125\n",
            "Batch 750 / 1132 Sammon Loss : 1353.94140625\n",
            "Batch 760 / 1132 Sammon Loss : 1851.308349609375\n",
            "Batch 770 / 1132 Sammon Loss : 2682.46826171875\n",
            "Batch 780 / 1132 Sammon Loss : 2399.52099609375\n",
            "Batch 790 / 1132 Sammon Loss : 1611.194091796875\n",
            "Batch 800 / 1132 Sammon Loss : 2653.68505859375\n",
            "Batch 810 / 1132 Sammon Loss : 1340.356201171875\n",
            "Batch 820 / 1132 Sammon Loss : 1482.9405517578125\n",
            "Batch 830 / 1132 Sammon Loss : 2443.56494140625\n",
            "Batch 840 / 1132 Sammon Loss : 2100.091064453125\n",
            "Batch 850 / 1132 Sammon Loss : 3948.028564453125\n",
            "Batch 860 / 1132 Sammon Loss : 4277.61865234375\n",
            "Batch 870 / 1132 Sammon Loss : 4656.30859375\n",
            "Batch 880 / 1132 Sammon Loss : 2146.464599609375\n",
            "Batch 890 / 1132 Sammon Loss : 1870.1240234375\n",
            "Batch 900 / 1132 Sammon Loss : 1772.0146484375\n",
            "Batch 910 / 1132 Sammon Loss : 2034.441162109375\n",
            "Batch 920 / 1132 Sammon Loss : 1502.9337158203125\n",
            "Batch 930 / 1132 Sammon Loss : 3211.591796875\n",
            "Batch 940 / 1132 Sammon Loss : 4135.05810546875\n",
            "Batch 950 / 1132 Sammon Loss : 3503.658935546875\n",
            "Batch 960 / 1132 Sammon Loss : 2560.2685546875\n",
            "Batch 970 / 1132 Sammon Loss : 1772.699462890625\n",
            "Batch 980 / 1132 Sammon Loss : 1524.3455810546875\n",
            "Batch 990 / 1132 Sammon Loss : 1981.218994140625\n",
            "Batch 1000 / 1132 Sammon Loss : 1560.5384521484375\n",
            "Batch 1010 / 1132 Sammon Loss : 1145.960693359375\n",
            "Batch 1020 / 1132 Sammon Loss : 2276.58203125\n",
            "Batch 1030 / 1132 Sammon Loss : 2201.99462890625\n",
            "Batch 1040 / 1132 Sammon Loss : 2556.072265625\n",
            "Batch 1050 / 1132 Sammon Loss : 1768.01416015625\n",
            "Batch 1060 / 1132 Sammon Loss : 1639.74462890625\n",
            "Batch 1070 / 1132 Sammon Loss : 3990.97119140625\n",
            "Batch 1080 / 1132 Sammon Loss : 2206.20458984375\n",
            "Batch 1090 / 1132 Sammon Loss : 1457.421630859375\n",
            "Batch 1100 / 1132 Sammon Loss : 4642.8603515625\n",
            "Batch 1110 / 1132 Sammon Loss : 1659.411865234375\n",
            "Batch 1120 / 1132 Sammon Loss : 1916.564208984375\n",
            "Batch 1130 / 1132 Sammon Loss : 1773.0469970703125\n",
            "Batch 10 / 505 Regressor Loss : 104111.2109375\n",
            "Batch 20 / 505 Regressor Loss : 115207.953125\n",
            "Batch 30 / 505 Regressor Loss : 98220.09375\n",
            "Batch 40 / 505 Regressor Loss : 99142.0625\n",
            "Batch 50 / 505 Regressor Loss : 129493.0859375\n",
            "Batch 60 / 505 Regressor Loss : 124214.734375\n",
            "Batch 70 / 505 Regressor Loss : 123895.421875\n",
            "Batch 80 / 505 Regressor Loss : 125747.46875\n",
            "Batch 90 / 505 Regressor Loss : 129337.03125\n",
            "Batch 100 / 505 Regressor Loss : 104062.96875\n",
            "Batch 110 / 505 Regressor Loss : 122148.609375\n",
            "Batch 120 / 505 Regressor Loss : 111589.5234375\n",
            "Batch 130 / 505 Regressor Loss : 125815.796875\n",
            "Batch 140 / 505 Regressor Loss : 131701.546875\n",
            "Batch 150 / 505 Regressor Loss : 116485.0234375\n",
            "Batch 160 / 505 Regressor Loss : 92574.5\n",
            "Batch 170 / 505 Regressor Loss : 101290.3046875\n",
            "Batch 180 / 505 Regressor Loss : 129894.0859375\n",
            "Batch 190 / 505 Regressor Loss : 124397.6484375\n",
            "Batch 200 / 505 Regressor Loss : 102104.125\n",
            "Batch 210 / 505 Regressor Loss : 109460.1796875\n",
            "Batch 220 / 505 Regressor Loss : 122012.640625\n",
            "Batch 230 / 505 Regressor Loss : 114948.046875\n",
            "Batch 240 / 505 Regressor Loss : 106562.1484375\n",
            "Batch 250 / 505 Regressor Loss : 147364.03125\n",
            "Batch 260 / 505 Regressor Loss : 124540.3125\n",
            "Batch 270 / 505 Regressor Loss : 105514.21875\n",
            "Batch 280 / 505 Regressor Loss : 122981.3125\n",
            "Batch 290 / 505 Regressor Loss : 121940.703125\n",
            "Batch 300 / 505 Regressor Loss : 113698.09375\n",
            "Batch 310 / 505 Regressor Loss : 117967.671875\n",
            "Batch 320 / 505 Regressor Loss : 143878.671875\n",
            "Batch 330 / 505 Regressor Loss : 110868.2421875\n",
            "Batch 340 / 505 Regressor Loss : 107945.5546875\n",
            "Batch 350 / 505 Regressor Loss : 91460.3125\n",
            "Batch 360 / 505 Regressor Loss : 98834.984375\n",
            "Batch 370 / 505 Regressor Loss : 118104.8984375\n",
            "Batch 380 / 505 Regressor Loss : 102034.2109375\n",
            "Batch 390 / 505 Regressor Loss : 113821.109375\n",
            "Batch 400 / 505 Regressor Loss : 136553.25\n",
            "Batch 410 / 505 Regressor Loss : 102972.75\n",
            "Batch 420 / 505 Regressor Loss : 123688.984375\n",
            "Batch 430 / 505 Regressor Loss : 108287.8359375\n",
            "Batch 440 / 505 Regressor Loss : 109680.1171875\n",
            "Batch 450 / 505 Regressor Loss : 97843.765625\n",
            "Batch 460 / 505 Regressor Loss : 125377.546875\n",
            "Batch 470 / 505 Regressor Loss : 118102.296875\n",
            "Batch 480 / 505 Regressor Loss : 118815.3984375\n",
            "Batch 490 / 505 Regressor Loss : 118868.734375\n",
            "Batch 500 / 505 Regressor Loss : 116747.046875\n",
            "Epoch  84 / 100  Sammon Loss : 1108.3876953125  Regressor Loss : 100796.734375\n",
            "Val Loss : 99001.640625\n",
            "Batch 10 / 1132 Sammon Loss : 3555.79931640625\n",
            "Batch 20 / 1132 Sammon Loss : 4032.4580078125\n",
            "Batch 30 / 1132 Sammon Loss : 4042.9638671875\n",
            "Batch 40 / 1132 Sammon Loss : 4194.24951171875\n",
            "Batch 50 / 1132 Sammon Loss : 4352.6025390625\n",
            "Batch 60 / 1132 Sammon Loss : 2222.16650390625\n",
            "Batch 70 / 1132 Sammon Loss : 2288.291015625\n",
            "Batch 80 / 1132 Sammon Loss : 1995.1380615234375\n",
            "Batch 90 / 1132 Sammon Loss : 2035.5269775390625\n",
            "Batch 100 / 1132 Sammon Loss : 1861.42919921875\n",
            "Batch 110 / 1132 Sammon Loss : 1435.2408447265625\n",
            "Batch 120 / 1132 Sammon Loss : 764.0643920898438\n",
            "Batch 130 / 1132 Sammon Loss : 1432.948974609375\n",
            "Batch 140 / 1132 Sammon Loss : 1578.3028564453125\n",
            "Batch 150 / 1132 Sammon Loss : 1797.9381103515625\n",
            "Batch 160 / 1132 Sammon Loss : 1693.9376220703125\n",
            "Batch 170 / 1132 Sammon Loss : 3718.0185546875\n",
            "Batch 180 / 1132 Sammon Loss : 4231.98095703125\n",
            "Batch 190 / 1132 Sammon Loss : 3230.144287109375\n",
            "Batch 200 / 1132 Sammon Loss : 3008.67041015625\n",
            "Batch 210 / 1132 Sammon Loss : 2815.466064453125\n",
            "Batch 220 / 1132 Sammon Loss : 2143.923828125\n",
            "Batch 230 / 1132 Sammon Loss : 1858.059814453125\n",
            "Batch 240 / 1132 Sammon Loss : 1250.533935546875\n",
            "Batch 250 / 1132 Sammon Loss : 1197.30078125\n",
            "Batch 260 / 1132 Sammon Loss : 1433.142822265625\n",
            "Batch 270 / 1132 Sammon Loss : 1522.8096923828125\n",
            "Batch 280 / 1132 Sammon Loss : 1200.519287109375\n",
            "Batch 290 / 1132 Sammon Loss : 1284.3109130859375\n",
            "Batch 300 / 1132 Sammon Loss : 2690.5625\n",
            "Batch 310 / 1132 Sammon Loss : 1297.1942138671875\n",
            "Batch 320 / 1132 Sammon Loss : 1122.7169189453125\n",
            "Batch 330 / 1132 Sammon Loss : 1535.20361328125\n",
            "Batch 340 / 1132 Sammon Loss : 1300.9664306640625\n",
            "Batch 350 / 1132 Sammon Loss : 2227.1064453125\n",
            "Batch 360 / 1132 Sammon Loss : 2396.2958984375\n",
            "Batch 370 / 1132 Sammon Loss : 2425.151611328125\n",
            "Batch 380 / 1132 Sammon Loss : 1962.53857421875\n",
            "Batch 390 / 1132 Sammon Loss : 2386.1318359375\n",
            "Batch 400 / 1132 Sammon Loss : 2875.6611328125\n",
            "Batch 410 / 1132 Sammon Loss : 2087.4658203125\n",
            "Batch 420 / 1132 Sammon Loss : 2083.531494140625\n",
            "Batch 430 / 1132 Sammon Loss : 2281.58642578125\n",
            "Batch 440 / 1132 Sammon Loss : 3092.70849609375\n",
            "Batch 450 / 1132 Sammon Loss : 1961.76611328125\n",
            "Batch 460 / 1132 Sammon Loss : 2422.994384765625\n",
            "Batch 470 / 1132 Sammon Loss : 1968.692626953125\n",
            "Batch 480 / 1132 Sammon Loss : 2533.84716796875\n",
            "Batch 490 / 1132 Sammon Loss : 1468.4853515625\n",
            "Batch 500 / 1132 Sammon Loss : 5557.99853515625\n",
            "Batch 510 / 1132 Sammon Loss : 5373.7587890625\n",
            "Batch 520 / 1132 Sammon Loss : 3748.375732421875\n",
            "Batch 530 / 1132 Sammon Loss : 4810.271484375\n",
            "Batch 540 / 1132 Sammon Loss : 1253.3995361328125\n",
            "Batch 550 / 1132 Sammon Loss : 1408.239013671875\n",
            "Batch 560 / 1132 Sammon Loss : 1556.9716796875\n",
            "Batch 570 / 1132 Sammon Loss : 1529.509521484375\n",
            "Batch 580 / 1132 Sammon Loss : 1753.713134765625\n",
            "Batch 590 / 1132 Sammon Loss : 1888.697021484375\n",
            "Batch 600 / 1132 Sammon Loss : 1357.0758056640625\n",
            "Batch 610 / 1132 Sammon Loss : 1265.58154296875\n",
            "Batch 620 / 1132 Sammon Loss : 3144.31494140625\n",
            "Batch 630 / 1132 Sammon Loss : 3343.455078125\n",
            "Batch 640 / 1132 Sammon Loss : 3030.829345703125\n",
            "Batch 650 / 1132 Sammon Loss : 3791.772216796875\n",
            "Batch 660 / 1132 Sammon Loss : 1734.618408203125\n",
            "Batch 670 / 1132 Sammon Loss : 1296.04638671875\n",
            "Batch 680 / 1132 Sammon Loss : 1855.043212890625\n",
            "Batch 690 / 1132 Sammon Loss : 1286.423583984375\n",
            "Batch 700 / 1132 Sammon Loss : 1739.572509765625\n",
            "Batch 710 / 1132 Sammon Loss : 1266.85009765625\n",
            "Batch 720 / 1132 Sammon Loss : 1875.30615234375\n",
            "Batch 730 / 1132 Sammon Loss : 913.6375122070312\n",
            "Batch 740 / 1132 Sammon Loss : 1619.75634765625\n",
            "Batch 750 / 1132 Sammon Loss : 1242.37939453125\n",
            "Batch 760 / 1132 Sammon Loss : 1697.814453125\n",
            "Batch 770 / 1132 Sammon Loss : 2495.07763671875\n",
            "Batch 780 / 1132 Sammon Loss : 2182.19970703125\n",
            "Batch 790 / 1132 Sammon Loss : 1453.6346435546875\n",
            "Batch 800 / 1132 Sammon Loss : 2496.01513671875\n",
            "Batch 810 / 1132 Sammon Loss : 1192.6201171875\n",
            "Batch 820 / 1132 Sammon Loss : 1385.513427734375\n",
            "Batch 830 / 1132 Sammon Loss : 2279.299072265625\n",
            "Batch 840 / 1132 Sammon Loss : 1944.922607421875\n",
            "Batch 850 / 1132 Sammon Loss : 3748.611328125\n",
            "Batch 860 / 1132 Sammon Loss : 4051.5439453125\n",
            "Batch 870 / 1132 Sammon Loss : 4447.88525390625\n",
            "Batch 880 / 1132 Sammon Loss : 1973.650634765625\n",
            "Batch 890 / 1132 Sammon Loss : 1720.111572265625\n",
            "Batch 900 / 1132 Sammon Loss : 1659.0255126953125\n",
            "Batch 910 / 1132 Sammon Loss : 1949.8809814453125\n",
            "Batch 920 / 1132 Sammon Loss : 1408.6534423828125\n",
            "Batch 930 / 1132 Sammon Loss : 2960.984375\n",
            "Batch 940 / 1132 Sammon Loss : 3913.42431640625\n",
            "Batch 950 / 1132 Sammon Loss : 3307.13134765625\n",
            "Batch 960 / 1132 Sammon Loss : 2392.99072265625\n",
            "Batch 970 / 1132 Sammon Loss : 1634.23388671875\n",
            "Batch 980 / 1132 Sammon Loss : 1403.4105224609375\n",
            "Batch 990 / 1132 Sammon Loss : 1829.789306640625\n",
            "Batch 1000 / 1132 Sammon Loss : 1434.8289794921875\n",
            "Batch 1010 / 1132 Sammon Loss : 1068.6844482421875\n",
            "Batch 1020 / 1132 Sammon Loss : 2144.41064453125\n",
            "Batch 1030 / 1132 Sammon Loss : 2031.924072265625\n",
            "Batch 1040 / 1132 Sammon Loss : 2383.734375\n",
            "Batch 1050 / 1132 Sammon Loss : 1618.56689453125\n",
            "Batch 1060 / 1132 Sammon Loss : 1523.415771484375\n",
            "Batch 1070 / 1132 Sammon Loss : 3804.736328125\n",
            "Batch 1080 / 1132 Sammon Loss : 2064.396484375\n",
            "Batch 1090 / 1132 Sammon Loss : 1403.46875\n",
            "Batch 1100 / 1132 Sammon Loss : 4443.71728515625\n",
            "Batch 1110 / 1132 Sammon Loss : 1499.96875\n",
            "Batch 1120 / 1132 Sammon Loss : 1820.896728515625\n",
            "Batch 1130 / 1132 Sammon Loss : 1626.754638671875\n",
            "Batch 10 / 505 Regressor Loss : 103901.265625\n",
            "Batch 20 / 505 Regressor Loss : 114993.9375\n",
            "Batch 30 / 505 Regressor Loss : 98005.2109375\n",
            "Batch 40 / 505 Regressor Loss : 98924.3125\n",
            "Batch 50 / 505 Regressor Loss : 129229.453125\n",
            "Batch 60 / 505 Regressor Loss : 123974.5\n",
            "Batch 70 / 505 Regressor Loss : 123663.828125\n",
            "Batch 80 / 505 Regressor Loss : 125496.6875\n",
            "Batch 90 / 505 Regressor Loss : 129107.7734375\n",
            "Batch 100 / 505 Regressor Loss : 103837.7109375\n",
            "Batch 110 / 505 Regressor Loss : 121924.0859375\n",
            "Batch 120 / 505 Regressor Loss : 111383.0\n",
            "Batch 130 / 505 Regressor Loss : 125591.109375\n",
            "Batch 140 / 505 Regressor Loss : 131462.21875\n",
            "Batch 150 / 505 Regressor Loss : 116262.40625\n",
            "Batch 160 / 505 Regressor Loss : 92370.0\n",
            "Batch 170 / 505 Regressor Loss : 101077.5546875\n",
            "Batch 180 / 505 Regressor Loss : 129654.734375\n",
            "Batch 190 / 505 Regressor Loss : 124148.4296875\n",
            "Batch 200 / 505 Regressor Loss : 101906.421875\n",
            "Batch 210 / 505 Regressor Loss : 109238.2421875\n",
            "Batch 220 / 505 Regressor Loss : 121782.3125\n",
            "Batch 230 / 505 Regressor Loss : 114729.8125\n",
            "Batch 240 / 505 Regressor Loss : 106357.5859375\n",
            "Batch 250 / 505 Regressor Loss : 147107.359375\n",
            "Batch 260 / 505 Regressor Loss : 124299.03125\n",
            "Batch 270 / 505 Regressor Loss : 105306.1875\n",
            "Batch 280 / 505 Regressor Loss : 122747.8984375\n",
            "Batch 290 / 505 Regressor Loss : 121723.0546875\n",
            "Batch 300 / 505 Regressor Loss : 113468.6875\n",
            "Batch 310 / 505 Regressor Loss : 117742.1484375\n",
            "Batch 320 / 505 Regressor Loss : 143619.609375\n",
            "Batch 330 / 505 Regressor Loss : 110639.875\n",
            "Batch 340 / 505 Regressor Loss : 107709.1171875\n",
            "Batch 350 / 505 Regressor Loss : 91253.953125\n",
            "Batch 360 / 505 Regressor Loss : 98641.8671875\n",
            "Batch 370 / 505 Regressor Loss : 117871.5\n",
            "Batch 380 / 505 Regressor Loss : 101826.140625\n",
            "Batch 390 / 505 Regressor Loss : 113599.734375\n",
            "Batch 400 / 505 Regressor Loss : 136286.984375\n",
            "Batch 410 / 505 Regressor Loss : 102763.3046875\n",
            "Batch 420 / 505 Regressor Loss : 123460.4921875\n",
            "Batch 430 / 505 Regressor Loss : 108070.1875\n",
            "Batch 440 / 505 Regressor Loss : 109456.0859375\n",
            "Batch 450 / 505 Regressor Loss : 97645.9296875\n",
            "Batch 460 / 505 Regressor Loss : 125146.4296875\n",
            "Batch 470 / 505 Regressor Loss : 117891.984375\n",
            "Batch 480 / 505 Regressor Loss : 118592.625\n",
            "Batch 490 / 505 Regressor Loss : 118639.421875\n",
            "Batch 500 / 505 Regressor Loss : 116534.09375\n",
            "Epoch  85 / 100  Sammon Loss : 1066.1685791015625  Regressor Loss : 100599.484375\n",
            "Val Loss : 98751.8984375\n",
            "Batch 10 / 1132 Sammon Loss : 3401.270751953125\n",
            "Batch 20 / 1132 Sammon Loss : 3850.188232421875\n",
            "Batch 30 / 1132 Sammon Loss : 3870.70947265625\n",
            "Batch 40 / 1132 Sammon Loss : 4010.761474609375\n",
            "Batch 50 / 1132 Sammon Loss : 4198.82763671875\n",
            "Batch 60 / 1132 Sammon Loss : 2132.98974609375\n",
            "Batch 70 / 1132 Sammon Loss : 2176.781982421875\n",
            "Batch 80 / 1132 Sammon Loss : 1857.924560546875\n",
            "Batch 90 / 1132 Sammon Loss : 1882.228515625\n",
            "Batch 100 / 1132 Sammon Loss : 1700.4052734375\n",
            "Batch 110 / 1132 Sammon Loss : 1321.3909912109375\n",
            "Batch 120 / 1132 Sammon Loss : 707.1517333984375\n",
            "Batch 130 / 1132 Sammon Loss : 1337.075927734375\n",
            "Batch 140 / 1132 Sammon Loss : 1504.1605224609375\n",
            "Batch 150 / 1132 Sammon Loss : 1713.57568359375\n",
            "Batch 160 / 1132 Sammon Loss : 1574.9677734375\n",
            "Batch 170 / 1132 Sammon Loss : 3552.537841796875\n",
            "Batch 180 / 1132 Sammon Loss : 3982.545654296875\n",
            "Batch 190 / 1132 Sammon Loss : 3025.6015625\n",
            "Batch 200 / 1132 Sammon Loss : 2816.35302734375\n",
            "Batch 210 / 1132 Sammon Loss : 2662.876953125\n",
            "Batch 220 / 1132 Sammon Loss : 2012.4632568359375\n",
            "Batch 230 / 1132 Sammon Loss : 1779.6595458984375\n",
            "Batch 240 / 1132 Sammon Loss : 1180.6044921875\n",
            "Batch 250 / 1132 Sammon Loss : 1099.0943603515625\n",
            "Batch 260 / 1132 Sammon Loss : 1314.761962890625\n",
            "Batch 270 / 1132 Sammon Loss : 1458.4000244140625\n",
            "Batch 280 / 1132 Sammon Loss : 1186.8349609375\n",
            "Batch 290 / 1132 Sammon Loss : 1270.027587890625\n",
            "Batch 300 / 1132 Sammon Loss : 2584.901611328125\n",
            "Batch 310 / 1132 Sammon Loss : 1258.5205078125\n",
            "Batch 320 / 1132 Sammon Loss : 1071.4329833984375\n",
            "Batch 330 / 1132 Sammon Loss : 1504.1353759765625\n",
            "Batch 340 / 1132 Sammon Loss : 1296.338134765625\n",
            "Batch 350 / 1132 Sammon Loss : 2154.329833984375\n",
            "Batch 360 / 1132 Sammon Loss : 2272.82373046875\n",
            "Batch 370 / 1132 Sammon Loss : 2329.426513671875\n",
            "Batch 380 / 1132 Sammon Loss : 1852.85107421875\n",
            "Batch 390 / 1132 Sammon Loss : 2277.30126953125\n",
            "Batch 400 / 1132 Sammon Loss : 2747.630859375\n",
            "Batch 410 / 1132 Sammon Loss : 2012.00244140625\n",
            "Batch 420 / 1132 Sammon Loss : 1983.6319580078125\n",
            "Batch 430 / 1132 Sammon Loss : 2124.4443359375\n",
            "Batch 440 / 1132 Sammon Loss : 3015.323486328125\n",
            "Batch 450 / 1132 Sammon Loss : 1906.0960693359375\n",
            "Batch 460 / 1132 Sammon Loss : 2340.3427734375\n",
            "Batch 470 / 1132 Sammon Loss : 1887.4013671875\n",
            "Batch 480 / 1132 Sammon Loss : 2456.70263671875\n",
            "Batch 490 / 1132 Sammon Loss : 1428.914794921875\n",
            "Batch 500 / 1132 Sammon Loss : 5447.677734375\n",
            "Batch 510 / 1132 Sammon Loss : 5296.0263671875\n",
            "Batch 520 / 1132 Sammon Loss : 3702.4736328125\n",
            "Batch 530 / 1132 Sammon Loss : 4751.5048828125\n",
            "Batch 540 / 1132 Sammon Loss : 1229.37158203125\n",
            "Batch 550 / 1132 Sammon Loss : 1394.416259765625\n",
            "Batch 560 / 1132 Sammon Loss : 1505.8330078125\n",
            "Batch 570 / 1132 Sammon Loss : 1513.0379638671875\n",
            "Batch 580 / 1132 Sammon Loss : 1716.89697265625\n",
            "Batch 590 / 1132 Sammon Loss : 1830.5831298828125\n",
            "Batch 600 / 1132 Sammon Loss : 1383.1156005859375\n",
            "Batch 610 / 1132 Sammon Loss : 1205.0277099609375\n",
            "Batch 620 / 1132 Sammon Loss : 3103.77392578125\n",
            "Batch 630 / 1132 Sammon Loss : 3259.728759765625\n",
            "Batch 640 / 1132 Sammon Loss : 2970.42529296875\n",
            "Batch 650 / 1132 Sammon Loss : 3771.40478515625\n",
            "Batch 660 / 1132 Sammon Loss : 1638.396728515625\n",
            "Batch 670 / 1132 Sammon Loss : 1340.37646484375\n",
            "Batch 680 / 1132 Sammon Loss : 1827.8248291015625\n",
            "Batch 690 / 1132 Sammon Loss : 1250.290771484375\n",
            "Batch 700 / 1132 Sammon Loss : 1657.84423828125\n",
            "Batch 710 / 1132 Sammon Loss : 1220.4322509765625\n",
            "Batch 720 / 1132 Sammon Loss : 1853.272705078125\n",
            "Batch 730 / 1132 Sammon Loss : 900.6172485351562\n",
            "Batch 740 / 1132 Sammon Loss : 1564.5260009765625\n",
            "Batch 750 / 1132 Sammon Loss : 1237.2271728515625\n",
            "Batch 760 / 1132 Sammon Loss : 1649.0257568359375\n",
            "Batch 770 / 1132 Sammon Loss : 2485.0703125\n",
            "Batch 780 / 1132 Sammon Loss : 2146.220947265625\n",
            "Batch 790 / 1132 Sammon Loss : 1430.5362548828125\n",
            "Batch 800 / 1132 Sammon Loss : 2498.531005859375\n",
            "Batch 810 / 1132 Sammon Loss : 1173.512939453125\n",
            "Batch 820 / 1132 Sammon Loss : 1310.3609619140625\n",
            "Batch 830 / 1132 Sammon Loss : 2320.43798828125\n",
            "Batch 840 / 1132 Sammon Loss : 1933.926025390625\n",
            "Batch 850 / 1132 Sammon Loss : 3748.687255859375\n",
            "Batch 860 / 1132 Sammon Loss : 4046.553466796875\n",
            "Batch 870 / 1132 Sammon Loss : 4459.0263671875\n",
            "Batch 880 / 1132 Sammon Loss : 1927.0301513671875\n",
            "Batch 890 / 1132 Sammon Loss : 1699.027099609375\n",
            "Batch 900 / 1132 Sammon Loss : 1614.11669921875\n",
            "Batch 910 / 1132 Sammon Loss : 1918.5506591796875\n",
            "Batch 920 / 1132 Sammon Loss : 1380.77001953125\n",
            "Batch 930 / 1132 Sammon Loss : 2963.348876953125\n",
            "Batch 940 / 1132 Sammon Loss : 3850.695556640625\n",
            "Batch 950 / 1132 Sammon Loss : 3192.24365234375\n",
            "Batch 960 / 1132 Sammon Loss : 2319.934814453125\n",
            "Batch 970 / 1132 Sammon Loss : 1606.503662109375\n",
            "Batch 980 / 1132 Sammon Loss : 1354.4022216796875\n",
            "Batch 990 / 1132 Sammon Loss : 1824.869384765625\n",
            "Batch 1000 / 1132 Sammon Loss : 1469.27099609375\n",
            "Batch 1010 / 1132 Sammon Loss : 1125.258056640625\n",
            "Batch 1020 / 1132 Sammon Loss : 2159.151123046875\n",
            "Batch 1030 / 1132 Sammon Loss : 2012.220703125\n",
            "Batch 1040 / 1132 Sammon Loss : 2333.9423828125\n",
            "Batch 1050 / 1132 Sammon Loss : 1650.687744140625\n",
            "Batch 1060 / 1132 Sammon Loss : 1544.614990234375\n",
            "Batch 1070 / 1132 Sammon Loss : 3772.72412109375\n",
            "Batch 1080 / 1132 Sammon Loss : 2118.46435546875\n",
            "Batch 1090 / 1132 Sammon Loss : 1445.5235595703125\n",
            "Batch 1100 / 1132 Sammon Loss : 4434.6982421875\n",
            "Batch 1110 / 1132 Sammon Loss : 1539.4873046875\n",
            "Batch 1120 / 1132 Sammon Loss : 1912.44970703125\n",
            "Batch 1130 / 1132 Sammon Loss : 1665.818115234375\n",
            "Batch 10 / 505 Regressor Loss : 103691.7109375\n",
            "Batch 20 / 505 Regressor Loss : 114780.3359375\n",
            "Batch 30 / 505 Regressor Loss : 97790.7109375\n",
            "Batch 40 / 505 Regressor Loss : 98706.9609375\n",
            "Batch 50 / 505 Regressor Loss : 128966.234375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-cd620253e899>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mH1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mH1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mY1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msammon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m56\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m924\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mloss_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPos_Train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9XdPuF1wTAH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "34cb9a52-83cf-4ece-e856-610de038cdc0"
      },
      "source": [
        "pairs[2:9,0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 3, 4, 5, 6, 7, 8])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    }
  ]
}