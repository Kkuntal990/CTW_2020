{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd\nimport pyarrow.parquet as pq\nimport os\nprint(os.listdir(\"../input\"))\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nfrom tensorflow.python.keras.models import Sequential, Model, load_model\nfrom tensorflow.python.keras.layers import Dense\nimport numpy as np\nimport os\nimport h5py\n#from keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.decomposition import PCA","execution_count":1,"outputs":[{"output_type":"stream","text":"['traindata', 'weights-1', 'dsadasd']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find(SNR):\n    #H_R.shape = (512,56,924,5)\n    temp = np.sum(SNR**2, axis=1)\n    #print(temp.shape)\n    idx  = np.argmax(temp, axis=1)\n    return idx\n    \ndef preprocess(H_Re, idx):\n    temp = np.zeros(H_Re.shape[:-1])\n    for i in range(H_Re.shape[0]):\n        temp[i] = H_Re[i,:,:,idx[i]]\n    return temp\n      \n        ","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def get_data(data_file):\n    f = h5py.File(data_file, 'r')\n    H_Re = f['H_Re'][:] #shape (sample size, 56, 924, 5)\n    H_Im = f['H_Im'][:] #shape (sample size, 56, 924, 5)\n    SNR = f['SNR'][:] #shape (sample size, 56, 5)\n    Pos = f['Pos'][:] #shape(sample size, 3)\n    f.close()\n    return H_Re, H_Im, SNR, Pos\n\nCTW_labelled = \"../input/traindata\"\ndata_file = CTW_labelled+\"/file_\"+str(1)+\".hdf5\"\nH_Re, H_Im, SNR, Pos = get_data(data_file)\n\n#print(H_Re[1,1,1,:])\n\nidx = find(SNR)\n\nH_Re = preprocess(H_Re, idx)\nH_Im = preprocess(H_Im, idx)\nprint(idx.shape)\n\n","execution_count":14,"outputs":[{"output_type":"stream","text":"(512, 5)\n(512,)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(2,3):\n    temp = CTW_labelled+\"/file_\"+str(i)+\".hdf5\"\n    tH_Re, tH_Im, tSNR, tPos = get_data(temp)\n    idx = find(tSNR)\n    tH_Re = preprocess(tH_Re, idx)\n    tH_Im = preprocess(tH_Im, idx)\n    H_Re, H_Im, Pos = np.concatenate((H_Re, tH_Re)), np.concatenate((H_Im, tH_Im)), np.concatenate((Pos, tPos))","execution_count":15,"outputs":[{"output_type":"stream","text":"(512, 5)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(H_Re[:,1,1])\n\nsamples = H_Re.shape[0]\nH_Re = H_Re.reshape((samples,-1))\nprint(H_Re.shape)\nH_Im = H_Im.reshape((samples,-1))\n#SNR = SNR.reshape((samples,-1))\n#Pos = Pos.reshape((samples,-1))\n\ndata = np.concatenate((H_Re, H_Im), axis=1)\nprint(data.shape)\n","execution_count":5,"outputs":[{"output_type":"stream","text":"(1024, 51744)\n(1024, 103488)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=24)\no = pca.fit_transform(data[:1000])\ny = pca.explained_variance_ratio_\n#x = np.linspace(1,data[:1000].shape[1], data[:1000].shape[1])\n#plt.plot(x,y)\nprint(o.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data[:int(data.shape[0]/5)].shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n\nmodel = Sequential()\nmodel.add(Dense(compression_1, activation = 'relu', input_shape=(517718,), name = 'compress'))\nmodel.add(Dense(517718, activation='linear'))\nmodel.compile(loss= 'mean_squared_error', optimizer = 'adam')\n#model.loadweights('../input/weights_1/saved_model.pb')\n#print('model summary')\n'''\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    model = tf.keras.Sequential([ tf.keras.layers.Dense(20, activation='relu', input_shape=(103488,), name='compress'),\n                           tf.keras.layers.Dense(103488, activation='linear')])\n    model.compile(loss= 'mean_squared_error', optimizer = 'adam') \n    return model\nprint(data.shape)","execution_count":20,"outputs":[{"output_type":"stream","text":"(1024, 103488)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#compression_1 = 20\nmodel = create_model()\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nearlystopper = EarlyStopping(patience = 3, verbose=1)\ncheckpointer = ModelCheckpoint('Best', verbose=1, save_best_only=True)\nresults = model.fit(data[:1000], data[:1000], validation_split=0.1, batch_size = 10, epochs = 100, callbacks=[earlystopper, checkpointer])\nmodel.save('mymodel.h5')","execution_count":16,"outputs":[{"output_type":"stream","text":"Train on 900 samples, validate on 100 samples\nEpoch 1/100\n870/900 [============================>.] - ETA: 0s - loss: 0.0578\nEpoch 00001: val_loss improved from inf to 0.04778, saving model to Best\n900/900 [==============================] - 3s 4ms/sample - loss: 0.0590 - val_loss: 0.0478\nEpoch 2/100\n880/900 [============================>.] - ETA: 0s - loss: 0.0588\nEpoch 00002: val_loss improved from 0.04778 to 0.04706, saving model to Best\n900/900 [==============================] - 3s 3ms/sample - loss: 0.0586 - val_loss: 0.0471\nEpoch 3/100\n870/900 [============================>.] - ETA: 0s - loss: 0.0592\nEpoch 00003: val_loss improved from 0.04706 to 0.04655, saving model to Best\n900/900 [==============================] - 3s 3ms/sample - loss: 0.0580 - val_loss: 0.0466\nEpoch 4/100\n870/900 [============================>.] - ETA: 0s - loss: 0.0622\nEpoch 00004: val_loss did not improve from 0.04655\n900/900 [==============================] - 2s 2ms/sample - loss: 0.0630 - val_loss: 0.0524\nEpoch 5/100\n890/900 [============================>.] - ETA: 0s - loss: 0.0605\nEpoch 00005: val_loss did not improve from 0.04655\n900/900 [==============================] - 2s 3ms/sample - loss: 0.0608 - val_loss: 0.0485\nEpoch 6/100\n890/900 [============================>.] - ETA: 0s - loss: 0.0581\nEpoch 00006: val_loss did not improve from 0.04655\n900/900 [==============================] - 2s 3ms/sample - loss: 0.0598 - val_loss: 0.0485\nEpoch 00006: early stopping\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data[1001])\nprint(np.sum(((model.predict(data[1001:])[0])-data[1001])**2))","execution_count":25,"outputs":[{"output_type":"stream","text":"[ 0.          0.          0.         ... -0.01215363 -0.01028442\n -0.00774765]\n[-0.01216986 -0.01874961 -0.02362507 ...  0.01901577  0.01566293\n  0.01015284]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = model.fit(data[int(data.shape[0]/5):2*int(data.shape[0]/5)], data[int(data.shape[0]/5):2*int(data.shape[0]/5)], validation_split=0.1, batch_size = 10, epochs = 100, callbacks=[earlystopper, checkpointer])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = model.fit(data[2*int(data.shape[0]/5):3*int(data.shape[0]/5)], data[2*int(data.shape[0]/5):3*int(data.shape[0]/5)], validation_split=0.1, batch_size = 10, epochs = 100, callbacks=[earlystopper, checkpointer])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c_model = create_model()\n#print(os.listdir(\"../output\"))\n\nc_model = load_model('../input/dsadasd/Best')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_decompressed = c_model.predict(data[0:6])\nprint(np.sum((X_decompressed - data[0:6])**2)/517784*6)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}